{
  
    
        "post0": {
            "title": "Efficient estimation of predictive models using high-frequency high-dimensional data",
            "content": "My Master’s thesis, submitted to the Institute for Statistics and Econometrics of the Christian-Albrechts-Universität zu Kiel, Germany, proposes a method to increase the data efficiency of neural networks for asset return prediction. . Abstract . In this thesis, the data efficiency of linear and nonlinear regression models of asset return panel data is enhanced by accounting for cross-sectional correlations and longitudinal volatility clusters of residuals. The procedure is motivated by the infeasible generalized least squares estimator. In an extension, a generalized least squares loss function is proposed to efficiently fit nonlinear relationships via deep neural networks. Feasibility is achieved by estimating the unobserved covariance matrix of residuals with a nonparametrically eigenvalue-regularized ensembled pairwise integrated covariance (NER EPIC) matrix estimator applied to high-frequency returns in high dimensions. Monte Carlo evidence confirms efficiency gains for linear and nonlinear conditional expectation models in finite- samples. A study of historical stock market data for the 100 largest US-based stocks shows substantially improved portfolio return characteristics of general- ized models compared to their standard counterparts. A trading strategy based on the predictions of a neural network, minimizing the proposed generalized ob- jective function, generates an out-of-sample information ratio of 2.59. Compared to a model with the same hyperparameters but minimizing the conventional MSE loss function, this represents an improvement of close to 150%. . Get the thesis here. .",
            "url": "https://jpwoeltjen.github.io/researchBlog/efficiency/2020/01/14/test-markdown-post.html",
            "relUrl": "/efficiency/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Natural Language Processing for Finance",
            "content": "import pandas as pd import numpy as np from matplotlib import pyplot as plt from statsmodels.tsa.stattools import adfuller import seaborn as sns import statsmodels.api as sm from tqdm import tqdm, tnrange, tqdm_notebook from sklearn.metrics.pairwise import cosine_similarity from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer from sklearn import decomposition from IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = &quot;all&quot; %load_ext autoreload %autoreload 2 %config InlineBackend.figure_format = &#39;retina&#39; . Clustering stocks according to Item1 (Business) . Loading documents . #collapse import os import re dr = &#39;/Users/jan/Desktop/SEC-EDGAR-text-copy/data/batch_0002/&#39; data = [] tickers = [] for batch in os.listdir(dr): if batch == &#39;.DS_Store&#39;: continue for file in os.listdir(os.path.join(dr, batch)): if &#39;Item1_excerpt.txt&#39; in file : print(file) ticker = file.split(&#39;_&#39;)[0] if ticker in tickers: print(&#39;Skipping&#39;, ticker, &#39;Reason: ticker duplicated&#39;) continue with open(os.path.join(dr, batch, file)) as f: d = f.read() d = d.replace(&#39; n&#39;,&#39; &#39;) d = re.sub(&quot; d&quot;, &quot;&quot;, d) if len(d) &lt; 2000: print(&#39;Skipping&#39;, ticker, &#39;Reason: len too small&#39;) continue data.append(&#39; n&#39; + d + &#39; n&#39;) tickers.append(ticker) . . SCG_0000091882_10K_20190228_Item1_excerpt.txt WLTW_0001140536_10K_20190227_Item1_excerpt.txt HRL_0000048465_10K_20181207_Item1_excerpt.txt FDX_0001048911_10K_20190716_Item1_excerpt.txt WYNN_0001174922_10K_20190228_Item1_excerpt.txt NDAQ_0001120193_10K_20190222_Item1_excerpt.txt AEE_0000018654_10K_20190226_Item1_excerpt.txt MMC_0000062709_10K_20190221_Item1_excerpt.txt XLNX_0000743988_10K_20190510_Item1_excerpt.txt PNR_0000077360_10K_20190219_Item1_excerpt.txt GD_0000040533_10K_20190213_Item1_excerpt.txt HD_0000354950_10K_20190328_Item1_excerpt.txt LOW_0000060667_10K_20190402_Item1_excerpt.txt OMC_0000029989_10K_20190212_Item1_excerpt.txt BRKB_0001067983_10K_20190225_Item1_excerpt.txt BAX_0000010456_10K_20190221_Item1_excerpt.txt AAL_0000004515_10K_20190225_Item1_excerpt.txt TXT_0000217346_10K_20190214_Item1_excerpt.txt KR_0000056873_10K_20190402_Item1_excerpt.txt CCI_0001051470_10K_20190225_Item1_excerpt.txt MTB_0000036270_10K_20190220_Item1_excerpt.txt ALL_0000899051_10K_20190215_Item1_excerpt.txt TXN_0000097476_10K_20190222_Item1_excerpt.txt PSA_0001393311_10K_20190227_Item1_excerpt.txt UAL_0000100517_10K_20190228_Item1_excerpt.txt CAT_0000018230_10K_20190214_Item1_excerpt.txt AME_0001037868_10K_20190221_Item1_excerpt.txt SHW_0000089800_10K_20190222_Item1_excerpt.txt HOLX_0000859737_10K_20181120_Item1_excerpt.txt WBA_0001618921_10K_20181011_Item1_excerpt.txt LMT_0000936468_10K_20190208_Item1_excerpt.txt HBI_0001359841_10K_20190211_Item1_excerpt.txt JEC_0000052988_10K_20181121_Item1_excerpt.txt RF_0001281761_10K_20190222_Item1_excerpt.txt HON_0000773840_10K_20190208_Item1_excerpt.txt FL_0000850209_10K_20190402_Item1_excerpt.txt NWL_0000814453_10K_20190304_Item1_excerpt.txt AXP_0000004962_10K_20190213_Item1_excerpt.txt CSX_0000277948_10K_20190206_Item1_excerpt.txt JWN_0000072333_10K_20190318_Item1_excerpt.txt PCG_0000075488_10K_20190228_Item1_excerpt.txt FLIR_0000354908_10K_20190228_Item1_excerpt.txt BK_0001390777_10K_20190227_Item1_excerpt.txt CINF_0000020286_10K_20190222_Item1_excerpt.txt ITW_0000049826_10K_20190215_Item1_excerpt.txt ARNC_0000004281_10K_20190221_Item1_excerpt.txt WYN_0001361658_10K_20190226_Item1_excerpt.txt JNPR_0001043604_10K_20190222_Item1_excerpt.txt ROK_0001024478_10K_20181109_Item1_excerpt.txt BBT_0000092230_10K_20190226_Item1_excerpt.txt WM_0000823768_10K_20190214_Item1_excerpt.txt AVY_0000008818_10K_20190227_Item1_excerpt.txt HAS_0000046080_10K_20190226_Item1_excerpt.txt IPG_0000051644_10K_20190225_Item1_excerpt.txt KMB_0000055785_10K_20190207_Item1_excerpt.txt PKI_0000031791_10K_20190226_Item1_excerpt.txt TDG_0001260221_10K_20181109_Item1_excerpt.txt GS_0000886982_10K_20190226_Item1_excerpt.txt ZBH_0001136869_10K_20190226_Item1_excerpt.txt SPG_0001022344_10K_20190222_Item1_excerpt.txt PEP_0000077476_10K_20190215_Item1_excerpt.txt FITB_0000035527_10K_20190301_Item1_excerpt.txt CSCO_0000858877_10K_20190905_Item1_excerpt.txt MYL_0001623613_10K_20190227_Item1_excerpt.txt EQR_0000906107_10K_20190221_Item1_excerpt.txt TSN_0000100493_10K_20181113_Item1_excerpt.txt DHR_0000313616_10K_20190221_Item1_excerpt.txt LH_0000920148_10K_20190228_Item1_excerpt.txt GT_0000042582_10K_20190208_Item1_excerpt.txt VNO_0000899689_10K_20190211_Item1_excerpt.txt IR_0001466258_10K_20190212_Item1_excerpt.txt ORCL_0001341439_10K_20190621_Item1_excerpt.txt MAT_0000063276_10K_20190222_Item1_excerpt.txt LLL_0001039101_10K_20190221_Item1_excerpt.txt UNP_0000100885_10K_20190208_Item1_excerpt.txt KIM_0000879101_10K_20190215_Item1_excerpt.txt CL_0000021665_10K_20190221_Item1_excerpt.txt AMAT_0000006951_10K_20181213_Item1_excerpt.txt SEE_0001012100_10K_20190219_Item1_excerpt.txt NLSN_0001492633_10K_20190228_Item1_excerpt.txt MKC_0000063754_10K_20190125_Item1_excerpt.txt AWK_0001410636_10K_20190219_Item1_excerpt.txt CHD_0000313927_10K_20190221_Item1_excerpt.txt LVLT_0000794323_10K_20190319_Item1_excerpt.txt AON_0000315293_10K_20190219_Item1_excerpt.txt TMK_0000320335_10K_20190301_Item1_excerpt.txt PVH_0000078239_10K_20190329_Item1_excerpt.txt PPG_0000079879_10K_20190221_Item1_excerpt.txt JNJ_0000200406_10K_20190220_Item1_excerpt.txt DIS_0001001039_10K_20181121_Item1_excerpt.txt MRK_0000310158_10K_20190227_Item1_excerpt.txt FAST_0000815556_10K_20190206_Item1_excerpt.txt DUK_0000017797_10K_20190228_Item1_excerpt.txt ADS_0001101215_10K_20190226_Item1_excerpt.txt TGNA_0000039899_10K_20190301_Item1_excerpt.txt UNH_0000731766_10K_20190212_Item1_excerpt.txt KO_0000021344_10K_20190221_Item1_excerpt.txt CMG_0001058090_10K_20190208_Item1_excerpt.txt MSFT_0000789019_10K_20190801_Item1_excerpt.txt NOV_0001021860_10K_20190214_Item1_excerpt.txt VMC_0001396009_10K_20190226_Item1_excerpt.txt DHI_0000882184_10K_20181116_Item1_excerpt.txt AVB_0000915912_10K_20190222_Item1_excerpt.txt HPE_0001645590_10K_20181212_Item1_excerpt.txt NI_0001111711_10K_20190220_Item1_excerpt.txt GPN_0001123360_10K_20190221_Item1_excerpt.txt PGR_0000080661_10K_20190227_Item1_excerpt.txt ABBV_0001551152_10K_20190227_Item1_excerpt.txt ALK_0000766421_10K_20190215_Item1_excerpt.txt EXPE_0001324424_10K_20190208_Item1_excerpt.txt JBHT_0000728535_10K_20190222_Item1_excerpt.txt PCAR_0000075362_10K_20190221_Item1_excerpt.txt EQIX_0001101239_10K_20190222_Item1_excerpt.txt TRIP_0001526520_10K_20190222_Item1_excerpt.txt HP_0000046765_10K_20181116_Item1_excerpt.txt XRX_0000108772_10K_20190225_Item1_excerpt.txt AMG_0001004434_10K_20190222_Item1_excerpt.txt SBUX_0000829224_10K_20181116_Item1_excerpt.txt MNST_0000865752_10K_20190228_Item1_excerpt.txt FB_0001326801_10K_20190131_Item1_excerpt.txt NFLX_0001065280_10K_20190129_Item1_excerpt.txt NEM_0001164727_10K_20190221_Item1_excerpt.txt ADP_0000008670_10K_20190809_Item1_excerpt.txt GILD_0000882095_10K_20190226_Item1_excerpt.txt DG_0000029534_10K_20190322_Item1_excerpt.txt CELG_0000816284_10K_20190226_Item1_excerpt.txt DOW_0000029915_10K_20190211_Item1_excerpt.txt ULTA_0001403568_10K_20190402_Item1_excerpt.txt MAA_0000912595_10K_20190221_Item1_excerpt.txt ADBE_0000796343_10K_20190125_Item1_excerpt.txt QRVO_0001604778_10K_20190517_Item1_excerpt.txt TEL_0001385157_10K_20181113_Item1_excerpt.txt DD_0000030554_10K_20190211_Item1_excerpt.txt CNC_0001071739_10K_20190219_Item1_excerpt.txt CHK_0000895126_10K_20190227_Item1_excerpt.txt SO_0000003153_10K_20190220_Item1_excerpt.txt SLB_0000087347_10K_20190123_Item1_excerpt.txt NVDA_0001045810_10K_20190221_Item1_excerpt.txt CXO_0001358071_10K_20190220_Item1_excerpt.txt ALXN_0000899866_10K_20190206_Item1_excerpt.txt ILMN_0001110803_10K_20190212_Item1_excerpt.txt SCHW_0000316709_10K_20190222_Item1_excerpt.txt IVZ_0000914208_10K_20190222_Item1_excerpt.txt BLK_0001364742_10K_20190228_Item1_excerpt.txt PXD_0001038357_10K_20190226_Item1_excerpt.txt MAR_0001048286_10K_20190301_Item1_excerpt.txt AJG_0000354190_10K_20190208_Item1_excerpt.txt CVX_0000093410_10K_20190222_Item1_excerpt.txt EQT_0000033213_10K_20190214_Item1_excerpt.txt COL_0001137411_10K_20181126_Item1_excerpt.txt AKAM_0001086222_10K_20190228_Item1_excerpt.txt GOOGL_0001652044_10K_20190205_Item1_excerpt.txt CTSH_0001058290_10K_20190219_Item1_excerpt.txt PYPL_0001633917_10K_20190207_Item1_excerpt.txt MA_0001141391_10K_20190213_Item1_excerpt.txt OKE_0001039684_10K_20190226_Item1_excerpt.txt IBM_0000051143_10K_20190226_Item1_excerpt.txt V_0001403161_10K_20181116_Item1_excerpt.txt CERN_0000804753_10K_20190208_Item1_excerpt.txt AMZN_0001018724_10K_20190201_Item1_excerpt.txt NAVI_0001593538_10K_20190226_Item1_excerpt.txt MLM_0000916076_10K_20190225_Item1_excerpt.txt IRM_0001020569_10K_20190214_Item1_excerpt.txt IDXX_0000874716_10K_20190215_Item1_excerpt.txt VRTX_0000875320_10K_20190213_Item1_excerpt.txt EOG_0000821189_10K_20190226_Item1_excerpt.txt XOM_0000034088_10K_20190227_Item1_excerpt.txt BHI_0000808362_10K_20190219_Item1_excerpt.txt EW_0001099800_10K_20190215_Item1_excerpt.txt MRO_0000101778_10K_20190221_Item1_excerpt.txt ISRG_0001035267_10K_20190204_Item1_excerpt.txt CRM_0001108524_10K_20190308_Item1_excerpt.txt PCLN_0001075531_10K_20190227_Item1_excerpt.txt NUE_0000073309_10K_20190228_Item1_excerpt.txt D_0000103682_10K_20190228_Item1_excerpt.txt SWK_0000093556_10K_20190226_Item1_excerpt.txt UDR_0000074208_10K_20190219_Item1_excerpt.txt RCL_0000884887_10K_20190222_Item1_excerpt.txt DLTR_0000935703_10K_20190327_Item1_excerpt.txt ORLY_0000898173_10K_20190227_Item1_excerpt.txt LB_0000701985_10K_20190322_Item1_excerpt.txt MTD_0001037646_10K_20190208_Item1_excerpt.txt MCO_0001059556_10K_20190225_Item1_excerpt.txt IFF_0000051253_10K_20190226_Item1_excerpt.txt ROST_0000745732_10K_20190402_Item1_excerpt.txt SNA_0000091440_10K_20190214_Item1_excerpt.txt EXPD_0000746515_10K_20190222_Item1_excerpt.txt BIIB_0000875045_10K_20190206_Item1_excerpt.txt FMC_0000037785_10K_20190228_Item1_excerpt.txt BXP_0001037540_10K_20190228_Item1_excerpt.txt CF_0001324404_10K_20190222_Item1_excerpt.txt KMX_0001170010_10K_20190419_Item1_excerpt.txt EL_0001001250_10K_20190823_Item1_excerpt.txt AMT_0001053507_10K_20190227_Item1_excerpt.txt FFIV_0001048695_10K_20181121_Item1_excerpt.txt PAYX_0000723531_10K_20190724_Item1_excerpt.txt ABC_0001140859_10K_20181120_Item1_excerpt.txt KSU_0000054480_10K_20190125_Item1_excerpt.txt CME_0001156375_10K_20190228_Item1_excerpt.txt COO_0000711404_10K_20181221_Item1_excerpt.txt ACN_0001467373_10K_20181024_Item1_excerpt.txt TSCO_0000916365_10K_20190221_Item1_excerpt.txt TROW_0001113169_10K_20190213_Item1_excerpt.txt UTX_0000101829_10K_20190207_Item1_excerpt.txt EA_0000712515_10K_20190524_Item1_excerpt.txt MOS_0001285785_10K_20190313_Item1_excerpt.txt APH_0000820313_10K_20190213_Item1_excerpt.txt R_0000085961_10K_20190221_Item1_excerpt.txt ZTS_0001555280_10K_20190214_Item1_excerpt.txt WAT_0001000697_10K_20190226_Item1_excerpt.txt MCK_0000927653_10K_20190515_Item1_excerpt.txt ANTM_0001156039_10K_20190220_Item1_excerpt.txt DISCK_0001437107_10K_20190301_Item1_excerpt.txt FBHS_0001519751_10K_20190225_Item1_excerpt.txt TJX_0000109198_10K_20190403_Item1_excerpt.txt FISV_0000798354_10K_20190221_Item1_excerpt.txt ESS_0000920522_10K_20190221_Item1_excerpt.txt AGN_0001578845_10K_20190215_Item1_excerpt.txt DE_0000315189_10K_20181217_Item1_excerpt.txt CHRW_0001043277_10K_20190225_Item1_excerpt.txt TSS_0000721683_10K_20190221_Item1_excerpt.txt ZION_0000109380_10K_20190226_Item1_excerpt.txt BSX_0000885725_10K_20190219_Item1_excerpt.txt NTRS_0000073124_10K_20190226_Item1_excerpt.txt DLR_0001297996_10K_20190225_Item1_excerpt.txt PBCT_0001378946_10K_20190301_Item1_excerpt.txt WAT_0001000697_10KA_20190301_Item1_excerpt.txt Skipping WAT Reason: ticker duplicated STZ_0000016918_10K_20190423_Item1_excerpt.txt SYK_0000310764_10K_20190207_Item1_excerpt.txt CMA_0000028412_10K_20190212_Item1_excerpt.txt COST_0000909832_10K_20181026_Item1_excerpt.txt ROP_0000882835_10K_20190225_Item1_excerpt.txt HBAN_0000049196_10K_20190215_Item1_excerpt.txt ABT_0000001800_10K_20190222_Item1_excerpt.txt VRSK_0001442145_10K_20190219_Item1_excerpt.txt USB_0000036104_10K_20190222_Item1_excerpt.txt SJM_0000091419_10K_20190617_Item1_excerpt.txt MHK_0000851968_10K_20190228_Item1_excerpt.txt NEE_0000037634_10K_20190215_Item1_excerpt.txt LKQ_0001065696_10K_20190301_Item1_excerpt.txt HSIC_0001000228_10K_20190220_Item1_excerpt.txt HUM_0000049071_10K_20190221_Item1_excerpt.txt CFG_0000759944_10K_20190221_Item1_excerpt.txt LUV_0000092380_10K_20190205_Item1_excerpt.txt UHS_0000352915_10K_20190227_Item1_excerpt.txt TMO_0000097745_10K_20190227_Item1_excerpt.txt CHTR_0001091667_10K_20190131_Item1_excerpt.txt LUK_0000096223_10KT_20190129_Item1_excerpt.txt CCL_0000815097_10K_20190128_Item1_excerpt.txt SWKS_0000004127_10K_20181115_Item1_excerpt.txt BFB_0000014693_10K_20190613_Item1_excerpt.txt ALB_0000915913_10K_20190227_Item1_excerpt.txt QCOM_0000804328_10K_20181107_Item1_excerpt.txt MUR_0000717423_10K_20190227_Item1_excerpt.txt CBG_0001138118_10K_20190301_Item1_excerpt.txt FRT_0000034903_10K_20190213_Item1_excerpt.txt MAC_0000912242_10K_20190225_Item1_excerpt.txt CMCSA_0000902739_10K_20190131_Item1_excerpt.txt DLPH_0001521332_10K_20190204_Item1_excerpt.txt EFX_0000033185_10K_20190221_Item1_excerpt.txt AZO_0000866787_10K_20181024_Item1_excerpt.txt BMY_0000014272_10K_20190225_Item1_excerpt.txt PM_0001413329_10K_20190207_Item1_excerpt.txt SPGI_0000064040_10K_20190213_Item1_excerpt.txt ATVI_0000718877_10K_20190228_Item1_excerpt.txt VAR_0000203527_10K_20181126_Item1_excerpt.txt NOC_0001133421_10K_20190131_Item1_excerpt.txt RHI_0000315213_10K_20190215_Item1_excerpt.txt CB_0000896159_10K_20190228_Item1_excerpt.txt DFS_0001393612_10K_20190220_Item1_excerpt.txt O_0000726728_10K_20190222_Item1_excerpt.txt HCA_0000860730_10K_20190221_Item1_excerpt.txt STT_0000093751_10K_20190221_Item1_excerpt.txt TIF_0000098246_10K_20190322_Item1_excerpt.txt BDX_0000010795_10K_20181121_Item1_excerpt.txt UPS_0001090727_10K_20190221_Item1_excerpt.txt NSC_0000702165_10K_20190208_Item1_excerpt.txt ECL_0000031462_10K_20190301_Item1_excerpt.txt VFC_0000103379_10K_20190524_Item1_excerpt.txt WFC_0000072971_10K_20190227_Item1_excerpt.txt O_0000726728_10KA_20190301_Item1_excerpt.txt Skipping O Reason: ticker duplicated MAS_0000062996_10K_20190207_Item1_excerpt.txt URBN_0000912615_10K_20190401_Item1_excerpt.txt ICE_0001571949_10K_20190207_Item1_excerpt.txt A_0001090872_10K_20181220_Item1_excerpt.txt BWA_0000908255_10KA_20180928_Item1_excerpt.txt SLG_0001040971_10K_20190227_Item1_excerpt.txt BWA_0000908255_10K_20190219_Item1_excerpt.txt Skipping BWA Reason: ticker duplicated FIS_0001136893_10K_20190221_Item1_excerpt.txt PNC_0000713676_10K_20190301_Item1_excerpt.txt PFG_0001126328_10K_20190213_Item1_excerpt.txt LNT_0000052485_10K_20190222_Item1_excerpt.txt LEG_0000058492_10K_20190227_Item1_excerpt.txt VRSN_0001014473_10K_20190215_Item1_excerpt.txt GWW_0000277135_10K_20190228_Item1_excerpt.txt ALLE_0001579241_10K_20190219_Item1_excerpt.txt HOG_0000793952_10K_20190228_Item1_excerpt.txt PFE_0000078003_10K_20190228_Item1_excerpt.txt DGX_0001022079_10K_20190221_Item1_excerpt.txt TAP_0000024545_10K_20190212_Item1_excerpt.txt WU_0001365135_10K_20190221_Item1_excerpt.txt GIS_0000040704_10K_20190628_Item1_excerpt.txt EMR_0000032604_10K_20181119_Item1_excerpt.txt KHC_0001637459_10K_20190607_Item1_excerpt.txt ADM_0000007084_10K_20190219_Item1_excerpt.txt EXC_0000008192_10K_20190208_Item1_excerpt.txt MU_0000723125_10K_20181015_Item1_excerpt.txt TDC_0000816761_10K_20190226_Item1_excerpt.txt WMT_0000104169_10K_20190328_Item1_excerpt.txt WMB_0000107263_10K_20190221_Item1_excerpt.txt CAG_0000023217_10K_20190719_Item1_excerpt.txt SIG_0000832988_10K_20190403_Item1_excerpt.txt MMM_0000066740_10K_20190207_Item1_excerpt.txt CNDT_0001677703_10K_20190228_Item1_excerpt.txt ES_0000013372_10K_20190226_Item1_excerpt.txt IP_0000051434_10K_20190220_Item1_excerpt.txt AMGN_0000318154_10K_20190213_Item1_excerpt.txt HPQ_0000047217_10K_20181213_Item1_excerpt.txt ED_0000023632_10K_20190221_Item1_excerpt.txt CTL_0000018926_10K_20190311_Item1_excerpt.txt GLW_0000024741_10K_20190212_Item1_excerpt.txt AAPL_0000320193_10K_20181105_Item1_excerpt.txt NRG_0001013871_10K_20190228_Item1_excerpt.txt AIV_0000922864_10K_20190220_Item1_excerpt.txt HCP_0000765880_10K_20190214_Item1_excerpt.txt NWSA_0001564708_10K_20190813_Item1_excerpt.txt K_0000055067_10K_20190225_Item1_excerpt.txt CNP_0000048732_10K_20190228_Item1_excerpt.txt T_0000732717_10K_20190220_Item1_excerpt.txt LNC_0000059558_10K_20190220_Item1_excerpt.txt KLAC_0000319201_10K_20190816_Item1_excerpt.txt BEN_0000038777_10K_20181109_Item1_excerpt.txt FTR_0000020520_10K_20190301_Item1_excerpt.txt XEL_0000072903_10K_20190222_Item1_excerpt.txt AAP_0001158449_10K_20190219_Item1_excerpt.txt SRCL_0000861878_10K_20190228_Item1_excerpt.txt SYMC_0000849399_10K_20181026_Item1_excerpt.txt WEC_0000783325_10K_20190226_Item1_excerpt.txt SWN_0000007332_10K_20190228_Item1_excerpt.txt AEP_0000004904_10K_20190221_Item1_excerpt.txt NTAP_0001002047_10K_20190618_Item1_excerpt.txt BBBY_0000886158_10K_20190430_Item1_excerpt.txt TDC_0000816761_10KA_20190301_Item1_excerpt.txt Skipping TDC Reason: ticker duplicated M_0000794367_10K_20190403_Item1_excerpt.txt GRMN_0001121788_10K_20190220_Item1_excerpt.txt TRV_0000086312_10K_20190214_Item1_excerpt.txt RL_0001037038_10K_20190516_Item1_excerpt.txt ENDP_0001593034_10K_20190228_Item1_excerpt.txt PBI_0000078814_10K_20190220_Item1_excerpt.txt L_0000060086_10K_20190213_Item1_excerpt.txt UNM_0000005513_10K_20190219_Item1_excerpt.txt GPS_0000039911_10K_20190319_Item1_excerpt.txt STX_0001137789_10K_20190802_Item1_excerpt.txt CPB_0000016732_10K_20180927_Item1_excerpt.txt EXR_0001289490_10K_20190226_Item1_excerpt.txt PLD_0001045609_10K_20190213_Item1_excerpt.txt LLY_0000059478_10K_20190219_Item1_excerpt.txt MET_0001099219_10K_20190222_Item1_excerpt.txt MPC_0001510295_10K_20190228_Item1_excerpt.txt CTAS_0000723254_10K_20190726_Item1_excerpt.txt AYI_0001144215_10K_20181025_Item1_excerpt.txt FLR_0001124198_10K_20190221_Item1_excerpt.txt ADI_0000006281_10K_20181127_Item1_excerpt.txt VIAB_0001339947_10K_20181116_Item1_excerpt.txt PPL_0000055387_10K_20190214_Item1_excerpt.txt BLL_0000009389_10K_20190222_Item1_excerpt.txt HST_0001061937_10K_20190226_Item1_excerpt.txt VZ_0000732712_10K_20190215_Item1_excerpt.txt TGT_0000027419_10K_20190313_Item1_excerpt.txt JCI_0000833444_10K_20181120_Item1_excerpt.txt CTXS_0000877890_10K_20190215_Item1_excerpt.txt KORS_0001530721_10K_20190529_Item1_excerpt.txt KSS_0000885639_10K_20190322_Item1_excerpt.txt GPC_0000040987_10K_20190225_Item1_excerpt.txt FSLR_0001274494_10K_20190222_Item1_excerpt.txt HRB_0000012659_10K_20190614_Item1_excerpt.txt MSI_0000068505_10K_20190215_Item1_excerpt.txt RIG_0001451505_10K_20190219_Item1_excerpt.txt GM_0001467858_10K_20190206_Item1_excerpt.txt NWS_0001564708_10K_20190813_Item1_excerpt.txt ETN_0001551182_10K_20190227_Item1_excerpt.txt PRU_0001137774_10K_20190215_Item1_excerpt.txt FLS_0000030625_10K_20190220_Item1_excerpt.txt CMS_0000201533_10K_20190205_Item1_excerpt.txt EMN_0000915389_10K_20190227_Item1_excerpt.txt AIG_0000005272_10K_20190215_Item1_excerpt.txt DPS_0001418135_10K_20190228_Item1_excerpt.txt FE_0001031296_10K_20190219_Item1_excerpt.txt SYMC_0000849399_10K_20190524_Item1_excerpt.txt Skipping SYMC Reason: ticker duplicated F_0000037996_10K_20190221_Item1_excerpt.txt BBY_0000764478_10K_20190328_Item1_excerpt.txt PEG_0000081033_10K_20190228_Item1_excerpt.txt . Vectorize . vectorizer = CountVectorizer(stop_words=&#39;english&#39;) X = vectorizer.fit_transform(data).todense() vocab = np.array(vectorizer.get_feature_names()) vectorizer_tfidf = TfidfVectorizer(stop_words=&#39;english&#39;, max_df = 1.,) X_tfidf = vectorizer_tfidf.fit_transform(data) . Dendrogram . from scipy.cluster.hierarchy import ward, dendrogram raw_cosine = pd.DataFrame(cosine_similarity(X_tfidf,X_tfidf)) linkage_matrix = ward(raw_cosine) fig, ax = plt.subplots(figsize=(20, 50)) # set size ax = dendrogram(linkage_matrix, orientation=&quot;right&quot;, labels=tickers); plt.tick_params( axis= &#39;x&#39;, which=&#39;both&#39;, bottom=&#39;off&#39;, top=&#39;off&#39;, labelbottom=&#39;off&#39; ) plt.savefig(&#39;ward_clusters.png&#39;, dpi=200) #save figure as ward_clusters . Singular Value Decomposition . u, s, v = decomposition.randomized_svd(X_tfidf, 20) . def show_topics(a, num_top_words): top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]] topic_words = ([top_words(t) for t in a]) return [&#39; &#39;.join(t) for t in topic_words] . show_topics(v, 10) . [&#39;products company services business customers including financial solutions information product&#39;, &#39;bank banking capital insurance reserve frb federal institution fdic financial&#39;, &#39;gas natural energy company oil electric ferc drilling utility transmission&#39;, &#39;gas fda clinical drug health products natural pharmaceutical patients care&#39;, &#39;stores merchandise company store brands bank products retail apparel retailers&#39;, &#39;insurance apartment properties tenants estate reit communities care health real&#39;, &#39;president vice mr stores served gas care health services merchandise&#39;, &#39;insurance president company vice mr reinsurance clients served chief officer&#39;, &#39;president apartment vice properties mr tenants estate reit communities real&#39;, &#39;company television programming content cable advertising fcc news video broadcast&#39;, &#39;aircraft airlines company care air health carriers medicare airline aviation&#39;, &#39;company clients solutions drilling oil services care merchandise gas health&#39;, &#39;drilling oil reserves wells insurance production proved rigs exploration rig&#39;, &#39;care health medicare company insurance customers medical wireless bank cms&#39;, &#39;care brands food beverage health products segment consumer packaging brand&#39;, &#39;cloud travel hotel security hotels airlines expedia data reservation guests&#39;, &#39;rail freight transportation railroad carriers customers network csx railroads intermodal&#39;, &#39;vehicle vehicles parts automotive travel rail gm cruise transportation carmax&#39;, &#39;payment card payments vehicle visa merchants credit vehicles paypal debit&#39;, &#39;apartment communities aimco residents redevelopment homes community multifamily partnership resident&#39;] . Latent Dirirchlet Allocation . import pyLDAvis import pyLDAvis.sklearn pyLDAvis.enable_notebook() from sklearn.decomposition import LatentDirichletAllocation . lda_tf = LatentDirichletAllocation(n_components=10) lda_tf.fit(X) . LatentDirichletAllocation(batch_size=128, doc_topic_prior=None, evaluate_every=-1, learning_decay=0.7, learning_method=&#39;batch&#39;, learning_offset=10.0, max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001, n_components=10, n_jobs=None, perp_tol=0.1, random_state=None, topic_word_prior=None, total_samples=1000000.0, verbose=0) . /Users/jan/anaconda/lib/python3.6/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version of pandas will change to not sort by default. To accept the future behavior, pass &#39;sort=False&#39;. To retain the current behavior and silence the warning, pass &#39;sort=True&#39;. return pd.concat([default_term_info] + list(topic_dfs)) . Entropy heatmap . from scipy.stats import entropy def jensen_shannon(query, matrix): &quot;&quot;&quot; This function implements a Jensen-Shannon similarity between the input query (an LDA topic distribution for a document) and the entire corpus of topic distributions. It returns an array of length M where M is the number of documents in the corpus &quot;&quot;&quot; # lets keep with the p,q notation above p = query[None,:].T # take transpose q = matrix.T # transpose matrix m = 0.5*(p + q) return np.sqrt(0.5*(entropy(p,m) + entropy(q,m))) . lda_matrix = lda_tf.transform(X) js_df = pd.DataFrame() for i, ticker in enumerate(tickers): js_df[ticker] = jensen_shannon(lda_matrix[i], lda_matrix) js_df.index=tickers . js_df[&#39;AMT&#39;].sort_values() . AMT 0.000000 CCI 0.060150 IRM 0.176408 DLR 0.225501 WLTW 0.265463 ... LLY 0.832111 MRK 0.832179 AMGN 0.832195 VRTX 0.832203 BIIB 0.832239 Name: AMT, Length: 387, dtype: float64 . AMT is very similar to CCI but not to other stocks. This makes sense. I leave it to the reader compare their business descriptions. . &lt;Figure size 2160x1440 with 0 Axes&gt; . /Users/jan/anaconda/lib/python3.6/site-packages/seaborn/matrix.py:143: DeprecationWarning: elementwise comparison failed; this will raise an error in the future. if xticklabels == []: /Users/jan/anaconda/lib/python3.6/site-packages/seaborn/matrix.py:151: DeprecationWarning: elementwise comparison failed; this will raise an error in the future. if yticklabels == []: . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c30eaee48&gt; . Clustering stocks according to Item1A (Risk factors) . Loading risk factor documents . #collapse import os import re dr = &#39;/Users/jan/Desktop/SEC-EDGAR-text-copy/data/batch_0002/&#39; data = [] tickers = [] for batch in os.listdir(dr): if batch == &#39;.DS_Store&#39;: continue for file in os.listdir(os.path.join(dr, batch)): if &#39;Item1A_excerpt.txt&#39; in file : print(file) ticker = file.split(&#39;_&#39;)[0] if ticker in tickers: print(&#39;Skipping&#39;, ticker, &#39;Reason: ticker duplicated&#39;) continue with open(os.path.join(dr, batch, file)) as f: d = f.read() d = d.replace(&#39; n&#39;,&#39; &#39;) d = re.sub(&quot; d&quot;, &quot;&quot;, d) if len(d) &lt; 2000: print(&#39;Skipping&#39;, ticker, &#39;Reason: len too small&#39;) continue data.append(&#39; n&#39; + d + &#39; n&#39;) tickers.append(ticker) . . &lt;&gt;:19: DeprecationWarning: invalid escape sequence d &lt;&gt;:19: DeprecationWarning: invalid escape sequence d &lt;&gt;:19: DeprecationWarning: invalid escape sequence d &lt;ipython-input-15-1c00bd898fb3&gt;:19: DeprecationWarning: invalid escape sequence d d = re.sub(&#34; d&#34;, &#34;&#34;, d) . LH_0000920148_10K_20190228_Item1A_excerpt.txt FDX_0001048911_10K_20190716_Item1A_excerpt.txt Skipping FDX Reason: len too small SCG_0000091882_10K_20190228_Item1A_excerpt.txt AAL_0000004515_10K_20190225_Item1A_excerpt.txt EQR_0000906107_10K_20190221_Item1A_excerpt.txt DUK_0000017797_10K_20190228_Item1A_excerpt.txt AME_0001037868_10K_20190221_Item1A_excerpt.txt NLSN_0001492633_10K_20190228_Item1A_excerpt.txt ROK_0001024478_10K_20181109_Item1A_excerpt.txt AXP_0000004962_10K_20190213_Item1A_excerpt.txt CSX_0000277948_10K_20190206_Item1A_excerpt.txt FLIR_0000354908_10K_20190228_Item1A_excerpt.txt VNO_0000899689_10K_20190211_Item1A_excerpt.txt CHD_0000313927_10K_20190221_Item1A_excerpt.txt KIM_0000879101_10K_20190215_Item1A_excerpt.txt PSA_0001393311_10K_20190227_Item1A_excerpt.txt CCI_0001051470_10K_20190225_Item1A_excerpt.txt TXN_0000097476_10K_20190222_Item1A_excerpt.txt ORCL_0001341439_10K_20190621_Item1A_excerpt.txt GS_0000886982_10K_20190226_Item1A_excerpt.txt HD_0000354950_10K_20190328_Item1A_excerpt.txt ARNC_0000004281_10K_20190221_Item1A_excerpt.txt TXT_0000217346_10K_20190214_Item1A_excerpt.txt MRK_0000310158_10K_20190227_Item1A_excerpt.txt DIS_0001001039_10K_20181121_Item1A_excerpt.txt PKI_0000031791_10K_20190226_Item1A_excerpt.txt GD_0000040533_10K_20190213_Item1A_excerpt.txt OMC_0000029989_10K_20190212_Item1A_excerpt.txt MTB_0000036270_10K_20190220_Item1A_excerpt.txt LVLT_0000794323_10K_20190319_Item1A_excerpt.txt HOLX_0000859737_10K_20181120_Item1A_excerpt.txt ALL_0000899051_10K_20190215_Item1A_excerpt.txt MKC_0000063754_10K_20190125_Item1A_excerpt.txt CSCO_0000858877_10K_20190905_Item1A_excerpt.txt ETR_0000007323_10K_20190226_Item1A_excerpt.txt NWL_0000814453_10K_20190304_Item1A_excerpt.txt BAX_0000010456_10K_20190221_Item1A_excerpt.txt FAST_0000815556_10K_20190206_Item1A_excerpt.txt HBI_0001359841_10K_20190211_Item1A_excerpt.txt WLTW_0001140536_10K_20190227_Item1A_excerpt.txt BBT_0000092230_10K_20190226_Item1A_excerpt.txt HON_0000773840_10K_20190208_Item1A_excerpt.txt IR_0001466258_10K_20190212_Item1A_excerpt.txt PCG_0000075488_10K_20190228_Item1A_excerpt.txt NDAQ_0001120193_10K_20190222_Item1A_excerpt.txt BRKB_0001067983_10K_20190225_Item1A_excerpt.txt LOW_0000060667_10K_20190402_Item1A_excerpt.txt SHW_0000089800_10K_20190222_Item1A_excerpt.txt ZBH_0001136869_10K_20190226_Item1A_excerpt.txt WYNN_0001174922_10K_20190228_Item1A_excerpt.txt RF_0001281761_10K_20190222_Item1A_excerpt.txt HAS_0000046080_10K_20190226_Item1A_excerpt.txt TMK_0000320335_10K_20190301_Item1A_excerpt.txt KR_0000056873_10K_20190402_Item1A_excerpt.txt PEP_0000077476_10K_20190215_Item1A_excerpt.txt CINF_0000020286_10K_20190222_Item1A_excerpt.txt JNPR_0001043604_10K_20190222_Item1A_excerpt.txt FITB_0000035527_10K_20190301_Item1A_excerpt.txt AEE_0000018654_10K_20190226_Item1A_excerpt.txt AON_0000315293_10K_20190219_Item1A_excerpt.txt IPG_0000051644_10K_20190225_Item1A_excerpt.txt AVY_0000008818_10K_20190227_Item1A_excerpt.txt JEC_0000052988_10K_20181121_Item1A_excerpt.txt SPG_0001022344_10K_20190222_Item1A_excerpt.txt BK_0001390777_10K_20190227_Item1A_excerpt.txt Skipping BK Reason: len too small TSN_0000100493_10K_20181113_Item1A_excerpt.txt CAT_0000018230_10K_20190214_Item1A_excerpt.txt DHR_0000313616_10K_20190221_Item1A_excerpt.txt MMC_0000062709_10K_20190221_Item1A_excerpt.txt KMB_0000055785_10K_20190207_Item1A_excerpt.txt PNR_0000077360_10K_20190219_Item1A_excerpt.txt UAL_0000100517_10K_20190228_Item1A_excerpt.txt FL_0000850209_10K_20190402_Item1A_excerpt.txt PPG_0000079879_10K_20190221_Item1A_excerpt.txt AWK_0001410636_10K_20190219_Item1A_excerpt.txt WBA_0001618921_10K_20181011_Item1A_excerpt.txt UNP_0000100885_10K_20190208_Item1A_excerpt.txt XLNX_0000743988_10K_20190510_Item1A_excerpt.txt JWN_0000072333_10K_20190318_Item1A_excerpt.txt GT_0000042582_10K_20190208_Item1A_excerpt.txt JNJ_0000200406_10K_20190220_Item1A_excerpt.txt ITW_0000049826_10K_20190215_Item1A_excerpt.txt WYN_0001361658_10K_20190226_Item1A_excerpt.txt LYB_0001489393_10K_20190221_Item1A_excerpt.txt LMT_0000936468_10K_20190208_Item1A_excerpt.txt MYL_0001623613_10K_20190227_Item1A_excerpt.txt CL_0000021665_10K_20190221_Item1A_excerpt.txt TDG_0001260221_10K_20181109_Item1A_excerpt.txt PVH_0000078239_10K_20190329_Item1A_excerpt.txt SEE_0001012100_10K_20190219_Item1A_excerpt.txt LLL_0001039101_10K_20190221_Item1A_excerpt.txt WM_0000823768_10K_20190214_Item1A_excerpt.txt HRL_0000048465_10K_20181207_Item1A_excerpt.txt AMAT_0000006951_10K_20181213_Item1A_excerpt.txt MAT_0000063276_10K_20190222_Item1A_excerpt.txt GOOGL_0001652044_10K_20190205_Item1A_excerpt.txt PXD_0001038357_10K_20190226_Item1A_excerpt.txt VMC_0001396009_10K_20190226_Item1A_excerpt.txt CHK_0000895126_10K_20190227_Item1A_excerpt.txt OXY_0000797468_10K_20190221_Item1A_excerpt.txt UNH_0000731766_10K_20190212_Item1A_excerpt.txt DOW_0000029915_10K_20190211_Item1A_excerpt.txt AKAM_0001086222_10K_20190228_Item1A_excerpt.txt DD_0000030554_10K_20190211_Item1A_excerpt.txt HPE_0001645590_10K_20181212_Item1A_excerpt.txt TGNA_0000039899_10K_20190301_Item1A_excerpt.txt CNC_0001071739_10K_20190219_Item1A_excerpt.txt FB_0001326801_10K_20190131_Item1A_excerpt.txt SCHW_0000316709_10K_20190222_Item1A_excerpt.txt JBHT_0000728535_10K_20190222_Item1A_excerpt.txt GPN_0001123360_10K_20190221_Item1A_excerpt.txt APA_0000006769_10K_20190301_Item1A_excerpt.txt SO_0000003153_10K_20190220_Item1A_excerpt.txt CRM_0001108524_10K_20190308_Item1A_excerpt.txt DHI_0000882184_10K_20181116_Item1A_excerpt.txt ADBE_0000796343_10K_20190125_Item1A_excerpt.txt SBUX_0000829224_10K_20181116_Item1A_excerpt.txt MSFT_0000789019_10K_20190801_Item1A_excerpt.txt ADS_0001101215_10K_20190226_Item1A_excerpt.txt NKE_0000320187_10K_20190723_Item1A_excerpt.txt ADP_0000008670_10K_20190809_Item1A_excerpt.txt SLB_0000087347_10K_20190123_Item1A_excerpt.txt MA_0001141391_10K_20190213_Item1A_excerpt.txt ALXN_0000899866_10K_20190206_Item1A_excerpt.txt XRX_0000108772_10K_20190225_Item1A_excerpt.txt EW_0001099800_10K_20190215_Item1A_excerpt.txt ALK_0000766421_10K_20190215_Item1A_excerpt.txt IDXX_0000874716_10K_20190215_Item1A_excerpt.txt TRIP_0001526520_10K_20190222_Item1A_excerpt.txt BLK_0001364742_10K_20190228_Item1A_excerpt.txt RRC_0000315852_10K_20190225_Item1A_excerpt.txt AMZN_0001018724_10K_20190201_Item1A_excerpt.txt CELG_0000816284_10K_20190226_Item1A_excerpt.txt PCLN_0001075531_10K_20190227_Item1A_excerpt.txt EOG_0000821189_10K_20190226_Item1A_excerpt.txt BHI_0000808362_10K_20190219_Item1A_excerpt.txt V_0001403161_10K_20181116_Item1A_excerpt.txt MAR_0001048286_10K_20190301_Item1A_excerpt.txt CXO_0001358071_10K_20190220_Item1A_excerpt.txt XEC_0001168054_10K_20190220_Item1A_excerpt.txt NI_0001111711_10K_20190220_Item1A_excerpt.txt XOM_0000034088_10K_20190227_Item1A_excerpt.txt QRVO_0001604778_10K_20190517_Item1A_excerpt.txt COL_0001137411_10K_20181126_Item1A_excerpt.txt HES_0000004447_10K_20190221_Item1A_excerpt.txt REGN_0000872589_10K_20190207_Item1A_excerpt.txt ILMN_0001110803_10K_20190212_Item1A_excerpt.txt EXPE_0001324424_10K_20190208_Item1A_excerpt.txt TEL_0001385157_10K_20181113_Item1A_excerpt.txt MRO_0000101778_10K_20190221_Item1A_excerpt.txt ABBV_0001551152_10K_20190227_Item1A_excerpt.txt AJG_0000354190_10K_20190208_Item1A_excerpt.txt IRM_0001020569_10K_20190214_Item1A_excerpt.txt NEM_0001164727_10K_20190221_Item1A_excerpt.txt COG_0000858470_10K_20190226_Item1A_excerpt.txt NFLX_0001065280_10K_20190129_Item1A_excerpt.txt EQIX_0001101239_10K_20190222_Item1A_excerpt.txt CTSH_0001058290_10K_20190219_Item1A_excerpt.txt IBM_0000051143_10K_20190226_Item1A_excerpt.txt Skipping IBM Reason: len too small ISRG_0001035267_10K_20190204_Item1A_excerpt.txt PCAR_0000075362_10K_20190221_Item1A_excerpt.txt NOV_0001021860_10K_20190214_Item1A_excerpt.txt NVDA_0001045810_10K_20190221_Item1A_excerpt.txt APC_0000773910_10K_20190214_Item1A_excerpt.txt NAVI_0001593538_10K_20190226_Item1A_excerpt.txt GILD_0000882095_10K_20190226_Item1A_excerpt.txt MLM_0000916076_10K_20190225_Item1A_excerpt.txt EQT_0000033213_10K_20190214_Item1A_excerpt.txt PYPL_0001633917_10K_20190207_Item1A_excerpt.txt VRTX_0000875320_10K_20190213_Item1A_excerpt.txt CVX_0000093410_10K_20190222_Item1A_excerpt.txt DG_0000029534_10K_20190322_Item1A_excerpt.txt HP_0000046765_10K_20181116_Item1A_excerpt.txt AMG_0001004434_10K_20190222_Item1A_excerpt.txt IVZ_0000914208_10K_20190222_Item1A_excerpt.txt CERN_0000804753_10K_20190208_Item1A_excerpt.txt MAA_0000912595_10K_20190221_Item1A_excerpt.txt AVB_0000915912_10K_20190222_Item1A_excerpt.txt KO_0000021344_10K_20190221_Item1A_excerpt.txt MNST_0000865752_10K_20190228_Item1A_excerpt.txt OKE_0001039684_10K_20190226_Item1A_excerpt.txt FCX_0000831259_10K_20190215_Item1A_excerpt.txt ULTA_0001403568_10K_20190402_Item1A_excerpt.txt CMG_0001058090_10K_20190208_Item1A_excerpt.txt PGR_0000080661_10K_20190227_Item1A_excerpt.txt TSCO_0000916365_10K_20190221_Item1A_excerpt.txt MHK_0000851968_10K_20190228_Item1A_excerpt.txt WHR_0000106640_10K_20190212_Item1A_excerpt.txt ESS_0000920522_10K_20190221_Item1A_excerpt.txt CMA_0000028412_10K_20190212_Item1A_excerpt.txt ABC_0001140859_10K_20181120_Item1A_excerpt.txt CFG_0000759944_10K_20190221_Item1A_excerpt.txt DE_0000315189_10K_20181217_Item1A_excerpt.txt SYK_0000310764_10K_20190207_Item1A_excerpt.txt HUM_0000049071_10K_20190221_Item1A_excerpt.txt TSS_0000721683_10K_20190221_Item1A_excerpt.txt NEE_0000037634_10K_20190215_Item1A_excerpt.txt CCL_0000815097_10K_20190128_Item1A_excerpt.txt EA_0000712515_10K_20190524_Item1A_excerpt.txt ABT_0000001800_10K_20190222_Item1A_excerpt.txt MUR_0000717423_10K_20190227_Item1A_excerpt.txt CBG_0001138118_10K_20190301_Item1A_excerpt.txt LB_0000701985_10K_20190322_Item1A_excerpt.txt AMT_0001053507_10K_20190227_Item1A_excerpt.txt BFB_0000014693_10K_20190613_Item1A_excerpt.txt BXP_0001037540_10K_20190228_Item1A_excerpt.txt SNA_0000091440_10K_20190214_Item1A_excerpt.txt WAT_0001000697_10K_20190226_Item1A_excerpt.txt SWKS_0000004127_10K_20181115_Item1A_excerpt.txt ZTS_0001555280_10K_20190214_Item1A_excerpt.txt VAR_0000203527_10K_20181126_Item1A_excerpt.txt ACN_0001467373_10K_20181024_Item1A_excerpt.txt NUE_0000073309_10K_20190228_Item1A_excerpt.txt BSX_0000885725_10K_20190219_Item1A_excerpt.txt ATVI_0000718877_10K_20190228_Item1A_excerpt.txt SWK_0000093556_10K_20190226_Item1A_excerpt.txt STZ_0000016918_10K_20190423_Item1A_excerpt.txt EL_0001001250_10K_20190823_Item1A_excerpt.txt HBAN_0000049196_10K_20190215_Item1A_excerpt.txt NTRS_0000073124_10K_20190226_Item1A_excerpt.txt PBCT_0001378946_10K_20190301_Item1A_excerpt.txt BIIB_0000875045_10K_20190206_Item1A_excerpt.txt MTD_0001037646_10K_20190208_Item1A_excerpt.txt ROST_0000745732_10K_20190402_Item1A_excerpt.txt MCO_0001059556_10K_20190225_Item1A_excerpt.txt SJM_0000091419_10K_20190617_Item1A_excerpt.txt CHTR_0001091667_10K_20190131_Item1A_excerpt.txt PM_0001413329_10K_20190207_Item1A_excerpt.txt DISCK_0001437107_10K_20190301_Item1A_excerpt.txt TROW_0001113169_10K_20190213_Item1A_excerpt.txt R_0000085961_10K_20190221_Item1A_excerpt.txt ORLY_0000898173_10K_20190227_Item1A_excerpt.txt ROP_0000882835_10K_20190225_Item1A_excerpt.txt FRT_0000034903_10K_20190213_Item1A_excerpt.txt ZION_0000109380_10K_20190226_Item1A_excerpt.txt MOS_0001285785_10K_20190313_Item1A_excerpt.txt CHRW_0001043277_10K_20190225_Item1A_excerpt.txt WAT_0001000697_10KA_20190301_Item1A_excerpt.txt Skipping WAT Reason: ticker duplicated FFIV_0001048695_10K_20181121_Item1A_excerpt.txt CME_0001156375_10K_20190228_Item1A_excerpt.txt TMO_0000097745_10K_20190227_Item1A_excerpt.txt ALB_0000915913_10K_20190227_Item1A_excerpt.txt LUK_0000096223_10KT_20190129_Item1A_excerpt.txt COO_0000711404_10K_20181221_Item1A_excerpt.txt NOC_0001133421_10K_20190131_Item1A_excerpt.txt EXPD_0000746515_10K_20190222_Item1A_excerpt.txt BMY_0000014272_10K_20190225_Item1A_excerpt.txt UTX_0000101829_10K_20190207_Item1A_excerpt.txt IFF_0000051253_10K_20190226_Item1A_excerpt.txt FBHS_0001519751_10K_20190225_Item1A_excerpt.txt DLTR_0000935703_10K_20190327_Item1A_excerpt.txt SPGI_0000064040_10K_20190213_Item1A_excerpt.txt KMX_0001170010_10K_20190419_Item1A_excerpt.txt AGN_0001578845_10K_20190215_Item1A_excerpt.txt VRSK_0001442145_10K_20190219_Item1A_excerpt.txt UDR_0000074208_10K_20190219_Item1A_excerpt.txt FISV_0000798354_10K_20190221_Item1A_excerpt.txt COST_0000909832_10K_20181026_Item1A_excerpt.txt CMCSA_0000902739_10K_20190131_Item1A_excerpt.txt TJX_0000109198_10K_20190403_Item1A_excerpt.txt LKQ_0001065696_10K_20190301_Item1A_excerpt.txt EFX_0000033185_10K_20190221_Item1A_excerpt.txt AZO_0000866787_10K_20181024_Item1A_excerpt.txt UHS_0000352915_10K_20190227_Item1A_excerpt.txt APH_0000820313_10K_20190213_Item1A_excerpt.txt COP_0001163165_10K_20190219_Item1A_excerpt.txt DLR_0001297996_10K_20190225_Item1A_excerpt.txt D_0000103682_10K_20190228_Item1A_excerpt.txt ANTM_0001156039_10K_20190220_Item1A_excerpt.txt CVS_0000064803_10K_20190228_Item1A_excerpt.txt KSU_0000054480_10K_20190125_Item1A_excerpt.txt HSIC_0001000228_10K_20190220_Item1A_excerpt.txt CF_0001324404_10K_20190222_Item1A_excerpt.txt QCOM_0000804328_10K_20181107_Item1A_excerpt.txt MCK_0000927653_10K_20190515_Item1A_excerpt.txt USB_0000036104_10K_20190222_Item1A_excerpt.txt Skipping USB Reason: len too small MAC_0000912242_10K_20190225_Item1A_excerpt.txt RCL_0000884887_10K_20190222_Item1A_excerpt.txt PAYX_0000723531_10K_20190724_Item1A_excerpt.txt LUV_0000092380_10K_20190205_Item1A_excerpt.txt FMC_0000037785_10K_20190228_Item1A_excerpt.txt DLPH_0001521332_10K_20190204_Item1A_excerpt.txt GWW_0000277135_10K_20190228_Item1A_excerpt.txt BWA_0000908255_10K_20190219_Item1A_excerpt.txt STT_0000093751_10K_20190221_Item1A_excerpt.txt TIF_0000098246_10K_20190322_Item1A_excerpt.txt O_0000726728_10K_20190222_Item1A_excerpt.txt A_0001090872_10K_20181220_Item1A_excerpt.txt MAS_0000062996_10K_20190207_Item1A_excerpt.txt UPS_0001090727_10K_20190221_Item1A_excerpt.txt NSC_0000702165_10K_20190208_Item1A_excerpt.txt BDX_0000010795_10K_20181121_Item1A_excerpt.txt SLG_0001040971_10K_20190227_Item1A_excerpt.txt LNT_0000052485_10K_20190222_Item1A_excerpt.txt CB_0000896159_10K_20190228_Item1A_excerpt.txt DFS_0001393612_10K_20190220_Item1A_excerpt.txt URBN_0000912615_10K_20190401_Item1A_excerpt.txt ICE_0001571949_10K_20190207_Item1A_excerpt.txt HCA_0000860730_10K_20190221_Item1A_excerpt.txt VFC_0000103379_10K_20190524_Item1A_excerpt.txt ECL_0000031462_10K_20190301_Item1A_excerpt.txt ALLE_0001579241_10K_20190219_Item1A_excerpt.txt VRSN_0001014473_10K_20190215_Item1A_excerpt.txt O_0000726728_10KA_20190301_Item1A_excerpt.txt Skipping O Reason: ticker duplicated RHI_0000315213_10K_20190215_Item1A_excerpt.txt PFG_0001126328_10K_20190213_Item1A_excerpt.txt LEG_0000058492_10K_20190227_Item1A_excerpt.txt BWA_0000908255_10KA_20180928_Item1A_excerpt.txt Skipping BWA Reason: ticker duplicated FIS_0001136893_10K_20190221_Item1A_excerpt.txt WFC_0000072971_10K_20190227_Item1A_excerpt.txt Skipping WFC Reason: len too small HOG_0000793952_10K_20190228_Item1A_excerpt.txt PNC_0000713676_10K_20190301_Item1A_excerpt.txt ED_0000023632_10K_20190221_Item1A_excerpt.txt AAPL_0000320193_10K_20181105_Item1A_excerpt.txt HRB_0000012659_10K_20190614_Item1A_excerpt.txt GIS_0000040704_10K_20190628_Item1A_excerpt.txt KMI_0001506307_10K_20190208_Item1A_excerpt.txt VZ_0000732712_10K_20190215_Item1A_excerpt.txt AIG_0000005272_10K_20190215_Item1A_excerpt.txt L_0000060086_10K_20190213_Item1A_excerpt.txt BBBY_0000886158_10K_20190430_Item1A_excerpt.txt PRU_0001137774_10K_20190215_Item1A_excerpt.txt HCP_0000765880_10K_20190214_Item1A_excerpt.txt FLR_0001124198_10K_20190221_Item1A_excerpt.txt ES_0000013372_10K_20190226_Item1A_excerpt.txt K_0000055067_10K_20190225_Item1A_excerpt.txt VIAB_0001339947_10K_20181116_Item1A_excerpt.txt EMR_0000032604_10K_20181119_Item1A_excerpt.txt SRCL_0000861878_10K_20190228_Item1A_excerpt.txt CAG_0000023217_10K_20190719_Item1A_excerpt.txt UNM_0000005513_10K_20190219_Item1A_excerpt.txt WMT_0000104169_10K_20190328_Item1A_excerpt.txt CTXS_0000877890_10K_20190215_Item1A_excerpt.txt HPQ_0000047217_10K_20181213_Item1A_excerpt.txt ADI_0000006281_10K_20181127_Item1A_excerpt.txt FTR_0000020520_10K_20190301_Item1A_excerpt.txt AEP_0000004904_10K_20190221_Item1A_excerpt.txt NRG_0001013871_10K_20190228_Item1A_excerpt.txt JCI_0000833444_10K_20181120_Item1A_excerpt.txt BLL_0000009389_10K_20190222_Item1A_excerpt.txt SYMC_0000849399_10K_20190524_Item1A_excerpt.txt CPB_0000016732_10K_20180927_Item1A_excerpt.txt MET_0001099219_10K_20190222_Item1A_excerpt.txt EXR_0001289490_10K_20190226_Item1A_excerpt.txt NWSA_0001564708_10K_20190813_Item1A_excerpt.txt FSLR_0001274494_10K_20190222_Item1A_excerpt.txt SIG_0000832988_10K_20190403_Item1A_excerpt.txt RIG_0001451505_10K_20190219_Item1A_excerpt.txt TGT_0000027419_10K_20190313_Item1A_excerpt.txt GPC_0000040987_10K_20190225_Item1A_excerpt.txt KHC_0001637459_10K_20190607_Item1A_excerpt.txt M_0000794367_10K_20190403_Item1A_excerpt.txt MU_0000723125_10K_20181015_Item1A_excerpt.txt PEG_0000081033_10K_20190228_Item1A_excerpt.txt RL_0001037038_10K_20190516_Item1A_excerpt.txt MMM_0000066740_10K_20190207_Item1A_excerpt.txt DPS_0001418135_10K_20190228_Item1A_excerpt.txt FLS_0000030625_10K_20190220_Item1A_excerpt.txt BBY_0000764478_10K_20190328_Item1A_excerpt.txt PBI_0000078814_10K_20190220_Item1A_excerpt.txt TDC_0000816761_10KA_20190301_Item1A_excerpt.txt CNDT_0001677703_10K_20190228_Item1A_excerpt.txt MPC_0001510295_10K_20190228_Item1A_excerpt.txt SYMC_0000849399_10K_20181026_Item1A_excerpt.txt Skipping SYMC Reason: ticker duplicated KORS_0001530721_10K_20190529_Item1A_excerpt.txt ETN_0001551182_10K_20190227_Item1A_excerpt.txt STX_0001137789_10K_20190802_Item1A_excerpt.txt EMN_0000915389_10K_20190227_Item1A_excerpt.txt CMS_0000201533_10K_20190205_Item1A_excerpt.txt TAP_0000024545_10K_20190212_Item1A_excerpt.txt FE_0001031296_10K_20190219_Item1A_excerpt.txt CTAS_0000723254_10K_20190726_Item1A_excerpt.txt AYI_0001144215_10K_20181025_Item1A_excerpt.txt T_0000732717_10K_20190220_Item1A_excerpt.txt F_0000037996_10K_20190221_Item1A_excerpt.txt WMB_0000107263_10K_20190221_Item1A_excerpt.txt AIV_0000922864_10K_20190220_Item1A_excerpt.txt NWS_0001564708_10K_20190813_Item1A_excerpt.txt PLD_0001045609_10K_20190213_Item1A_excerpt.txt TRV_0000086312_10K_20190214_Item1A_excerpt.txt DGX_0001022079_10K_20190221_Item1A_excerpt.txt GM_0001467858_10K_20190206_Item1A_excerpt.txt LNC_0000059558_10K_20190220_Item1A_excerpt.txt AAP_0001158449_10K_20190219_Item1A_excerpt.txt NTAP_0001002047_10K_20190618_Item1A_excerpt.txt ADM_0000007084_10K_20190219_Item1A_excerpt.txt PPL_0000055387_10K_20190214_Item1A_excerpt.txt XEL_0000072903_10K_20190222_Item1A_excerpt.txt KLAC_0000319201_10K_20190816_Item1A_excerpt.txt DTE_0000028385_10K_20190207_Item1A_excerpt.txt KSS_0000885639_10K_20190322_Item1A_excerpt.txt PFE_0000078003_10K_20190228_Item1A_excerpt.txt MO_0000764180_10K_20190226_Item1A_excerpt.txt AMGN_0000318154_10K_20190213_Item1A_excerpt.txt PSX_0001534701_10K_20190222_Item1A_excerpt.txt IP_0000051434_10K_20190220_Item1A_excerpt.txt HST_0001061937_10K_20190226_Item1A_excerpt.txt EXC_0000008192_10K_20190208_Item1A_excerpt.txt LLY_0000059478_10K_20190219_Item1A_excerpt.txt CTL_0000018926_10K_20190311_Item1A_excerpt.txt BEN_0000038777_10K_20181109_Item1A_excerpt.txt WU_0001365135_10K_20190221_Item1A_excerpt.txt MSI_0000068505_10K_20190215_Item1A_excerpt.txt ENDP_0001593034_10K_20190228_Item1A_excerpt.txt GPS_0000039911_10K_20190319_Item1A_excerpt.txt TDC_0000816761_10K_20190226_Item1A_excerpt.txt Skipping TDC Reason: ticker duplicated CNP_0000048732_10K_20190228_Item1A_excerpt.txt SWN_0000007332_10K_20190228_Item1A_excerpt.txt GRMN_0001121788_10K_20190220_Item1A_excerpt.txt GLW_0000024741_10K_20190212_Item1A_excerpt.txt WEC_0000783325_10K_20190226_Item1A_excerpt.txt . Vectorize . vectorizer = CountVectorizer(stop_words=&#39;english&#39;) X = vectorizer.fit_transform(data).todense() vocab = np.array(vectorizer.get_feature_names()) vectorizer_tfidf = TfidfVectorizer(stop_words=&#39;english&#39;, max_df = 1.,) X_tfidf = vectorizer_tfidf.fit_transform(data) . Dendrogram . from scipy.cluster.hierarchy import ward, dendrogram raw_cosine = pd.DataFrame(cosine_similarity(X_tfidf,X_tfidf)) linkage_matrix = ward(raw_cosine) fig, ax = plt.subplots(figsize=(20, 50)) ax = dendrogram(linkage_matrix, orientation=&quot;right&quot;, labels=tickers); plt.tick_params( axis= &#39;x&#39;, which=&#39;both&#39;, bottom=&#39;off&#39;, top=&#39;off&#39;, labelbottom=&#39;off&#39; ) plt.savefig(&#39;ward_clusters_rf.png&#39;, dpi=200) . Singular Value Decomposition . u, s, v = decomposition.randomized_svd(X_tfidf, 40) . show_topics(v, 5) . [&#39;business operations financial products results&#39;, &#39;gas oil drilling natural crude&#39;, &#39;company products product operates manufacturing&#39;, &#39;reit tenants clients company estate&#39;, &#39;reit products tenants clinical properties&#39;, &#39;merchandise stores sales customers consumer&#39;, &#39;services oil content solutions products&#39;, &#39;merchandise stores medicare care oil&#39;, &#39;products reinsurance financial oil risk&#39;, &#39;clients client operations reit government&#39;, &#39;clients registrants energy electric utility&#39;, &#39;content programming advertising brands clients&#39;, &#39;aircraft airline airlines travel fuel&#39;, &#39;bank financial merchants card merchant&#39;, &#39;programming vehicles vehicle merchandise clinical&#39;, &#39;rail railroads tenants ptc fuel&#39;, &#39;contracts government merger utc contract&#39;, &#39;vehicles vehicle automotive registrants business&#39;, &#39;solutions business results drug clinical&#39;, &#39;aircraft products airline airlines condition&#39;, &#39;rail merchants registrants card merger&#39;, &#39;rail bank stock shares mr&#39;, &#39;registrants duke dte exelon drilling&#39;, &#39;ships cruise stock merger guests&#39;, &#39;crude utc business vehicles rail&#39;, &#39;registrants apartment aimco adversely maa&#39;, &#39;tobacco dow business cruise tenants&#39;, &#39;drilling cruise ships guests reit&#39;, &#39;tobacco solutions tenants altria bank&#39;, &#39;tobacco altria adult apartment reit&#39;, &#39;tobacco hotels altria hotel services&#39;, &#39;merger dow dupont utc drilling&#39;, &#39;hotels reit hotel mr customers&#39;, &#39;clearing mining pharmaceutical trading generic&#39;, &#39;business dow games dupont services&#39;, &#39;mr vice eastman utc president&#39;, &#39;wireless cms mining semiconductor crude&#39;, &#39;hotels grainger solutions hotel dow&#39;, &#39;vice mr eastman mining water&#39;, &#39;blackrock dow comerica aum dupont&#39;] . Latent Dirirchlet Allocation . import pyLDAvis import pyLDAvis.sklearn pyLDAvis.enable_notebook() from sklearn.decomposition import LatentDirichletAllocation . lda_tf = LatentDirichletAllocation(n_components=10) lda_tf.fit(X) . LatentDirichletAllocation(batch_size=128, doc_topic_prior=None, evaluate_every=-1, learning_decay=0.7, learning_method=&#39;batch&#39;, learning_offset=10.0, max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001, n_components=10, n_jobs=None, perp_tol=0.1, random_state=None, topic_word_prior=None, total_samples=1000000.0, verbose=0) . /Users/jan/anaconda/lib/python3.6/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version of pandas will change to not sort by default. To accept the future behavior, pass &#39;sort=False&#39;. To retain the current behavior and silence the warning, pass &#39;sort=True&#39;. return pd.concat([default_term_info] + list(topic_dfs)) . Entropy heatmap . lda_matrix = lda_tf.transform(X) js_df = pd.DataFrame() for i, ticker in enumerate(tickers): js_df[ticker] = jensen_shannon(lda_matrix[i], lda_matrix) js_df.index=tickers . js_df[&#39;AMT&#39;].sort_values() . AMT 0.000000 DLR 0.140596 IRM 0.174905 EQIX 0.240909 CCI 0.249484 ... AMGN 0.829769 ALXN 0.831655 TGT 0.831767 CMG 0.832220 VRTX 0.832243 Name: AMT, Length: 401, dtype: float64 . &lt;Figure size 2160x1440 with 0 Axes&gt; . /Users/jan/anaconda/lib/python3.6/site-packages/seaborn/matrix.py:143: DeprecationWarning: elementwise comparison failed; this will raise an error in the future. if xticklabels == []: /Users/jan/anaconda/lib/python3.6/site-packages/seaborn/matrix.py:151: DeprecationWarning: elementwise comparison failed; this will raise an error in the future. if yticklabels == []: . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c25e68c50&gt; . Conclusion . NLP of documents can help clustering stocks. Clusters can be used in statistical arbitrage strategies. Exposure to risk factors is a valuable input to portfolio construction methods. It can be used either directly or as input to a shrinkage target for covariance matrix estimation regularization. .",
            "url": "https://jpwoeltjen.github.io/researchBlog/nlp/clustering/svd/lda/2019/12/20/nlp_10-k.html",
            "relUrl": "/nlp/clustering/svd/lda/2019/12/20/nlp_10-k.html",
            "date": " • Dec 20, 2019"
        }
        
    
  
    
        ,"post2": {
            "title": "Nonstationary Spreads",
            "content": "import pandas as pd import numpy as np from matplotlib import pyplot as plt from statsmodels.tsa.stattools import adfuller import seaborn as sns import statsmodels.api as sm from tqdm import tqdm, tnrange, tqdm_notebook import warnings warnings.filterwarnings(&quot;ignore&quot;) from IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = &quot;all&quot; %load_ext autoreload %autoreload 2 %config InlineBackend.figure_format = &#39;retina&#39; . Derivation of the Spread Dynamics . Imagine an asset $A$ that can be replicated, imperfectly, by some carefully chosen group of other assets. Let us call this group the synthetic hedge and denote it by $H$. Since the asset and its synthetic hedge share exposure to underlying processes, we can potentially eliminate a large part of the $A$&#39;s variance by subtracting off the synthetic hedge. We are left with exposure to the spread, i.e., $$Y_t= log left( frac{A_t}{A_0} right)- log left( frac{H_t}{H_0} right).$$ Now, analysis can be based on this spread. As an example, we might hypothesize that $Y_t$ is mean reverting. In order to optimally forecast $Y_t$ we have to think about how to model it. Modeling by a simple mean reverting process implies that $A$ and $H$ won&#39;t diverge too far from one another. Data of real world spreads contradicts this assumption. In fact, spreads tend to look close to geometric Brownian motion. If this were actually the case, there is no hope for forecasting future returns. However, it is possible that the spread has elements of both. . Hypothetical Dynamics (1) . Let us define a stochastic process that is common to both $H$ and $A$ $$C_t = C_{t-1} + epsilon_t,$$ where $ epsilon_t sim mathcal{N} left( mu, sigma^{2}_1 right)$. In words, $C_t$ is a geometric random walk. On top of that there is another random walk, independent of $C_t$, that is specific to $A$. Let us denote this process by $S_t$ and define it by $$S_t = S_{t-1} + omega_t,$$ where $ omega_t sim mathcal{N} left(0, sigma^{2}_ omega right)$. This process can be thought of as returns attributable to asset specific factors such as innovation, leadership, or lawsuits for a stock as an example. Next we define a mean reverting process, $$ R_t = phi R_{t-1} + u_t,$$ where $u_t sim mathcal{N} left(0, sigma^{2}_2 right).$ $R_t$ is an autoregressive process of order 1. This process can be thought of as temporary dislocations due to noise traders, order imbalances or a failure of timely information dissemination. . Let&#39;s imagine that $A_t$ develops as $$A_t = exp big(C_t + S_t + R_t big).$$ . $H_t$ is a function of the common process only and hence modeled by $$H_t = exp big(C_t big)$$ . Suppose that $ sigma^{2}_1 &gt;&gt; sigma^{2}_2$. In that case, there is too little signal in $A_t$ to observe it directly. Through hedging we can create exposure to the spread, though. To extract the spread we buy $A_t$ and short $H_t$. The log-return of this configuration from now $(t=0)$ to time $t$ is $$ log frac{A_t}{A_0} - log frac{H_t}{H_0} = (C_t + S_t + R_t) - (C_0 + S_0 + R_0) - (C_t-C_0) = R_t + S_t - R_0 - S_0.$$ I.e. the spread is a random walk around which an AR(1) process fluctuates. Since the random walk dominates asymptotically, this process is nonstationary. In other words, spread is modeled as a random walk observed with autocorrelated noise. These dynamics can be described by an ARIMA(1, 1, 1) model, which can be estimated via maximum likelihood estimation. . Hypothetical Dynamics (2) . One may also argue that the idiosyncratic process $S_t$ is an almost-nonstationary autoregressive process instead of a random walk. This process is modeled by a local-to-unity autoregressive root, i.e., $ phi$ is not 1 but close to it. Granger and &amp; Morris (1976) show that AR($p$) $+$ AR($q$) $=$ ARMA($p+q$, $max(p,q)$). Hence, the spread in our example would be modeled by an ARIMA(2, 0, 1). . Hypothetical Dynamics (3) . Furthermore, $H_t$ may also be a function of a specific random walk, which is defined by $$S_t^H = S_{t-1}^H + omega_t^H,$$ where $ omega_t^H sim mathcal{N} left(0, sigma^{2}_{ omega^H} right)$ and $ omega_t^H$ is independent of $ omega_t$. In addition $H_t$ may have a mean-reverting element, $R_t^H$, independent of all other processes considered so far. Then the spread is a random walk observed with ARMA(2,1) noise. Since any random walk with ARMA(p,q) noise is an ARIMA process, this spread can be modeled by an ARIMA model. . Simulation . def generate_garch_11_ts(n, sigma_sq_0, mu, alpha, beta, omega): &quot;&quot;&quot; generate GARCH log returns &quot;&quot;&quot; nu = np.random.normal(0,1,n) r = np.zeros(n) epsilon = np.zeros(n) sigma_sq = np.zeros(n) sigma_sq[0] = sigma_sq_0 if min(alpha,beta)&lt;0: raise ValueError(&#39;alpha, beta need to be non-negative&#39;) if omega &lt;=0: raise ValueError(&#39;omega needs to be positive&#39;) if alpha+beta&gt;=1: print(&#39;alpha+beta&gt;=1, variance not defined --&gt; time series will not be weakly stationary&#39;) for i in range(n): if i &gt;0: sigma_sq[i] = omega + alpha * epsilon[i-1]**2 + beta * sigma_sq[i-1] epsilon[i] = (sigma_sq[i]**0.5) * nu[i] r[i] = mu + epsilon[i] return r . def ornstein_uhlenbeck(theta, sigma, size, mu=0, x0=0): x = np.zeros(size) x[0] = x0 for i in range(1,size): dx = theta*(mu-x[i-1]) + sigma*np.random.normal(0,1) x[i] = dx + x[i-1] return x . Simulating an Asset and its Synthetic Hedge According to Hypothetical Dynamics (1) . epsilon = np.random.normal(0.0001,0.02,1000) # Gaussian specific innovations omega = np.random.normal(0, 0.01, epsilon.shape[0]) # Student t specific innovations # omega = 0.01*np.random.standard_t(2, epsilon.shape[0]) # GARCH specific innovations # omega = generate_garch_11_ts(epsilon.shape[0], sigma_sq_0=0, mu=0, alpha=0.1, beta=0.89, omega=0.0001) H = np.exp(np.cumsum(epsilon)) R = ornstein_uhlenbeck(0.5, 0.02, epsilon.shape[0]+1, mu=0, x0=0) S = omega.cumsum() # all equivalent calculations # A = np.exp(np.cumsum(epsilon + omega + np.diff(R))) # A = np.exp(np.cumsum(epsilon) + S + R[1:]) # A = np.exp(np.log(H) + S + R[1:]) A = H*np.exp(S + R[1:]) prices = pd.DataFrame(data={&#39;H&#39;:H, &#39;A&#39;:A}) prices.index = pd.date_range(start=&#39;1/1/2000&#39;, periods=len(prices), freq=&#39;B&#39;) plt.plot(prices) plt.show() . [&lt;matplotlib.lines.Line2D at 0x1c1ddbf550&gt;, &lt;matplotlib.lines.Line2D at 0x1c1e0504a8&gt;] . Y = np.log(prices.A) - np.log(prices.A.iloc[0]) - (np.log(prices.H) - np.log(prices.H.iloc[0])) Y.plot() plt.title(&#39;SPREAD&#39;) plt.show() Y[&#39;2000&#39;].plot() plt.title(&#39;SPREAD&#39;) plt.show() adfuller(Y) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c1e8f37f0&gt; . Text(0.5, 1.0, &#39;SPREAD&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c1e88fb38&gt; . Text(0.5, 1.0, &#39;SPREAD&#39;) . (-1.4982954917413052, 0.5343543656261089, 9, 990, {&#39;1%&#39;: -3.436972562223603, &#39;5%&#39;: -2.864463856182476, &#39;10%&#39;: -2.5683268054280175}, -4544.9966903021395) . Simulating an Asset and its Synthetic Hedge According to Hypothetical Dynamics (3) . n_obs = 2000 # Common Gaussain returns epsilon = np.random.normal(0.0,0.006, n_obs) # Gaussian specific innovations omega = np.random.normal(0, 0.01, epsilon.shape[0]) omega_H = np.random.normal(0, 0.01, epsilon.shape[0]) S = omega.cumsum() S_H = omega_H.cumsum() # Mean reverting process R = ornstein_uhlenbeck(0.1, 0.03, epsilon.shape[0]+1, mu=0, x0=0) R_H = ornstein_uhlenbeck(0.3, 0.01, epsilon.shape[0]+1, mu=0, x0=0) H = np.exp(np.cumsum(epsilon + omega_H + np.diff(R_H))) A = np.exp(np.cumsum(epsilon + omega + np.diff(R))) prices = pd.DataFrame(data={&#39;H&#39;:H, &#39;A&#39;:A}) prices.index = pd.date_range(start=&#39;1/1/2000&#39;, periods=len(prices), freq=&#39;B&#39;) plt.plot(prices) plt.show() . [&lt;matplotlib.lines.Line2D at 0x1c1edd8320&gt;, &lt;matplotlib.lines.Line2D at 0x1c1ee22c18&gt;] . Y = np.log(prices.A) - np.log(prices.A.iloc[0]) - (np.log(prices.H) - np.log(prices.H.iloc[0])) Y.plot() plt.title(&#39;SPREAD&#39;) plt.show() Y[&#39;2000&#39;].plot() plt.title(&#39;SPREAD&#39;) plt.show() adfuller(Y) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c1dd705c0&gt; . Text(0.5, 1.0, &#39;SPREAD&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c1e185a90&gt; . Text(0.5, 1.0, &#39;SPREAD&#39;) . (-3.2720065115288004, 0.01617710650081564, 13, 1986, {&#39;1%&#39;: -3.4336469649065298, &#39;5%&#39;: -2.862996415949189, &#39;10%&#39;: -2.5675453345421984}, -7685.910848870561) . SARIMAX Model Selection . x = Y train_pct = 0.5 train = int(train_pct*x.shape[0]) # introduce NaNs to observe behavior # x.loc[np.random.randint(1,len(x[:train]), len(x[:train]))] = np.nan # x.loc[np.random.randint(len(x[:train])+1, len(x), len(x[train:]))] = np.nan p_list = [] q_list = [] d_list = [] is_list = [] oos_list = [] is_aic_list = [] is_bic_list = [] params_list = [] for p in range(1, 2): for q in range(0, 3): for d in (1,): if q==p==0: continue p_list.append(p) q_list.append(q) d_list.append(d) order=(p, d, q) print(order) # IS fit mod = sm.tsa.statespace.SARIMAX(x[:train], order=order, # seasonal_order=(0, 0, 0, 0), # enforce_stationarity=False, # enforce_invertibility=False ) fit_results = mod.fit() print(fit_results.summary()) params_list.append(fit_results.params) _ = fit_results.plot_diagnostics(figsize=(15, 12)) plt.show() # IS prediction pred = fit_results.get_prediction(dynamic=False) trad_ret = (pred.predicted_mean-x.shift(1))*x.diff() trad_ret.cumsum().plot() plt.title(&#39;IS&#39;) plt.show() ir = trad_ret.mean()/(trad_ret.std()/np.sqrt(250)) is_list.append(ir) is_bic_list.append(fit_results.bic) is_aic_list.append(fit_results.aic) print(&#39;IR&#39;,ir) # OOS prediction mod = sm.tsa.statespace.SARIMAX(x[train:], order=order, # seasonal_order=(0, 0, 0, 0), # enforce_stationarity=False, # enforce_invertibility=False ) oos_results = mod.filter(fit_results.params)# this is needed to ingest data but doesn&#39;t change my test data pred = oos_results.get_prediction(dynamic=False,start=100) pred_ci = pred.conf_int() trad_ret = (pred.predicted_mean-x.shift(1))*x.diff() trad_ret.cumsum().plot() plt.title(&#39;OOS&#39;) plt.show() ir = trad_ret.mean()/(trad_ret.std()/np.sqrt(250)) oos_list.append(ir) print(&#39;IR&#39;,ir) (pred.predicted_mean.diff()-x.diff()).plot() plt.title(&#39;log-return prediction error&#39;) plt.show() # Null model: predict next returns as opposite signed last return trad_ret = (-x.diff().shift(1))*x.diff() trad_ret.cumsum().plot() plt.title(&#39;NULL OOS&#39;) plt.show() ir = trad_ret.mean()/(trad_ret.std()/np.sqrt(250)) print(&#39;IR&#39;,ir) df = pd.DataFrame({&#39;p&#39;:p_list, &#39;d&#39;:d_list, &#39;q&#39;:q_list, &#39;is&#39;:is_list, &#39;oos&#39;: oos_list, &#39;is_aic&#39;: is_aic_list, &#39;is_bic&#39;: is_bic_list, &#39;params&#39;:params_list }) df_sorted = df.sort_values(by=&#39;is_aic&#39;) df_sorted . (1, 1, 0) Statespace Model Results ============================================================================== Dep. Variable: y No. Observations: 1000 Model: SARIMAX(1, 1, 0) Log Likelihood 1896.681 Date: Wed, 04 Dec 2019 AIC -3789.362 Time: 20:58:47 BIC -3779.549 Sample: 01-03-2000 HQIC -3785.632 - 10-31-2003 Covariance Type: opg ============================================================================== coef std err z P&gt;|z| [0.025 0.975] ar.L1 -0.0759 0.032 -2.371 0.018 -0.139 -0.013 sigma2 0.0013 5.75e-05 22.822 0.000 0.001 0.001 =================================================================================== Ljung-Box (Q): 64.10 Jarque-Bera (JB): 1.29 Prob(Q): 0.01 Prob(JB): 0.53 Heteroskedasticity (H): 0.98 Skew: -0.08 Prob(H) (two-sided): 0.89 Kurtosis: 3.09 =================================================================================== Warnings: [1] Covariance matrix calculated using the outer product of gradients (complex-step). . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c2117b438&gt; . Text(0.5, 1.0, &#39;IS&#39;) . IR 1.2087901260674203 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c217fc978&gt; . Text(0.5, 1.0, &#39;OOS&#39;) . IR 0.8832811305644565 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c1ea5f9e8&gt; . Text(0.5, 1.0, &#39;log-return prediction error&#39;) . (1, 1, 1) Statespace Model Results ============================================================================== Dep. Variable: y No. Observations: 1000 Model: SARIMAX(1, 1, 1) Log Likelihood 1909.725 Date: Wed, 04 Dec 2019 AIC -3813.449 Time: 20:58:50 BIC -3798.729 Sample: 01-03-2000 HQIC -3807.854 - 10-31-2003 Covariance Type: opg ============================================================================== coef std err z P&gt;|z| [0.025 0.975] ar.L1 0.8567 0.034 25.546 0.000 0.791 0.922 ma.L1 -0.9497 0.020 -46.461 0.000 -0.990 -0.910 sigma2 0.0013 5.51e-05 23.215 0.000 0.001 0.001 =================================================================================== Ljung-Box (Q): 48.93 Jarque-Bera (JB): 3.04 Prob(Q): 0.16 Prob(JB): 0.22 Heteroskedasticity (H): 0.97 Skew: -0.09 Prob(H) (two-sided): 0.77 Kurtosis: 3.20 =================================================================================== Warnings: [1] Covariance matrix calculated using the outer product of gradients (complex-step). . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c1ee6f860&gt; . Text(0.5, 1.0, &#39;IS&#39;) . IR 2.5498711963729357 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c220fd908&gt; . Text(0.5, 1.0, &#39;OOS&#39;) . IR 2.461818296872518 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c233fc128&gt; . Text(0.5, 1.0, &#39;log-return prediction error&#39;) . (1, 1, 2) Statespace Model Results ============================================================================== Dep. Variable: y No. Observations: 1000 Model: SARIMAX(1, 1, 2) Log Likelihood 1909.836 Date: Wed, 04 Dec 2019 AIC -3811.672 Time: 20:58:54 BIC -3792.045 Sample: 01-03-2000 HQIC -3804.212 - 10-31-2003 Covariance Type: opg ============================================================================== coef std err z P&gt;|z| [0.025 0.975] ar.L1 0.8654 0.038 22.549 0.000 0.790 0.941 ma.L1 -0.9708 0.051 -19.045 0.000 -1.071 -0.871 ma.L2 0.0175 0.038 0.456 0.648 -0.058 0.093 sigma2 0.0013 5.51e-05 23.186 0.000 0.001 0.001 =================================================================================== Ljung-Box (Q): 48.96 Jarque-Bera (JB): 2.98 Prob(Q): 0.16 Prob(JB): 0.23 Heteroskedasticity (H): 0.97 Skew: -0.09 Prob(H) (two-sided): 0.78 Kurtosis: 3.20 =================================================================================== Warnings: [1] Covariance matrix calculated using the outer product of gradients (complex-step). . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c236a28d0&gt; . Text(0.5, 1.0, &#39;IS&#39;) . IR 2.553346430105229 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c1ed6b940&gt; . Text(0.5, 1.0, &#39;OOS&#39;) . IR 2.465331272615362 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c236e8a58&gt; . Text(0.5, 1.0, &#39;log-return prediction error&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c1e8c5080&gt; . Text(0.5, 1.0, &#39;NULL OOS&#39;) . IR 1.0447678647997458 . p d q is oos is_aic is_bic params . 1 1 | 1 | 1 | 2.549871 | 2.461818 | -3813.449449 | -3798.729185 | ar.L1 0.856690 ma.L1 -0.949701 sigma2 ... | . 2 1 | 1 | 2 | 2.553346 | 2.465331 | -3811.672253 | -3792.045234 | ar.L1 0.865388 ma.L1 -0.970781 ma.L2 ... | . 0 1 | 1 | 0 | 1.208790 | 0.883281 | -3789.362084 | -3779.548575 | ar.L1 -0.075854 sigma2 0.001313 dtype: f... | . Adaptive Out-of-sample Forecasting . x = Y x.index = range(len(x)) train_pct = 0.5 train = int(train_pct*x.shape[0]) ACTION_LAG = 0 N_PERIODS = 1 order = (1,1,1) ADAPTIVE = False ####################################################################################### #########################################TRAIN######################################### ####################################################################################### modl = sm.tsa.statespace.SARIMAX(x[:train], order=order) modl_fit = modl.fit() print(modl_fit.summary()) ####################################################################################### #########################################TEST######################################### ####################################################################################### x_test = x[train:] x_train = x[:train] pred = None for update_index in tqdm_notebook(range(0, len(x_test)-1)): next_x = x_test.iloc[update_index: update_index+1] x_train = x_train.append(next_x) modl = sm.tsa.statespace.SARIMAX(x_train, order=order) if ADAPTIVE: # this trains modl further each time a new data point arrives using prior parameters as # starting values # TODO how to initilize properly, choose &#39;learning rate&#39;? modl_fit = modl.fit(start_params=modl_fit.params , maxiter=5) else: # this filters the series without training further modl_fit = modl.filter(modl_fit.params) _pred_multi = modl_fit.forecast(N_PERIODS) # select the forecast furthest out in the future and put it in prediction series at # the timestamp of the next period _pred = pd.Series(data=_pred_multi.iloc[-1], index=[x_test.index[update_index+1]]) if pred is None: pred = _pred else: pred = pred.append(_pred) trad_ret = (pred-x.shift(1))*x.diff().shift(-ACTION_LAG) trad_ret.cumsum().plot() plt.title(&#39;OOS&#39;) plt.show() ir = trad_ret.mean()/(trad_ret.std()/np.sqrt(250)) print(&#39;OOS ir&#39;, ir) . Statespace Model Results ============================================================================== Dep. Variable: y No. Observations: 1000 Model: SARIMAX(1, 1, 1) Log Likelihood 1909.725 Date: Wed, 04 Dec 2019 AIC -3813.449 Time: 20:59:21 BIC -3798.729 Sample: 0 HQIC -3807.854 - 1000 Covariance Type: opg ============================================================================== coef std err z P&gt;|z| [0.025 0.975] ar.L1 0.8567 0.034 25.546 0.000 0.791 0.922 ma.L1 -0.9497 0.020 -46.461 0.000 -0.990 -0.910 sigma2 0.0013 5.51e-05 23.215 0.000 0.001 0.001 =================================================================================== Ljung-Box (Q): 48.93 Jarque-Bera (JB): 3.04 Prob(Q): 0.16 Prob(JB): 0.22 Heteroskedasticity (H): 0.97 Skew: -0.09 Prob(H) (two-sided): 0.77 Kurtosis: 3.20 =================================================================================== Warnings: [1] Covariance matrix calculated using the outer product of gradients (complex-step). . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c21191128&gt; . Text(0.5, 1.0, &#39;OOS&#39;) . OOS ir 2.3429464523910712 . Pitfalls of Irregularly Observed Multivariate Time Series . Regardless of the order that will eventually be used, we have to think carefully about which prices are observable and on which prices we can act. A bar is composed of several ticks observed at increasing time stamps. The last tick within the bar is observable (at the very least) at the closing time of the bar. We cannot assume a fill at this price in backtesting though. This fact is especially important when considering multiple instruments since the last tick of a bar may appear at different times for different instruments. If one were assuming fills at the close price, one would incur look-ahead bias or one would magically go back in time and trade at a past price. It is helpful to visualize these prices by a timeline as is done in Figure 1. . At $t=0$ we observe the closing price of the zeroth bar $A_0$ which is the traded price $p_0^A$ some point in time before -- or directly at -- $t_0$. The same holds for $H_0$, where the last traded price $p_0^H$ occurred slightly after $p_0^A$. $Y_0$ can be computed from $A_0$ and $H_0$ and predictions for $Y_1$ and beyond based on it. There is some lag due to data transmission and computation time. So, the first price we may assume a fill at -- before slippage -- is $p_1^A$ and $p_1^H$. Alternatively, if one only has the bar data without the ticks, one may use $A_1$ and $H_1$ assuming that the close prices occurred not within the computation lag. Then, a one-step forecast doesn&#39;t make sense anymore. In any case, do not forward (or backward) fill prices. This is also a problem for estimation. If prices are forward filled, parameters will be biased in favor of mean-reversion. If there is no traded price for the asset and the hedge at (almost) the same time, record a NaN and don&#39;t forward fill. In the state space form, ARIMA can handle the NaNs and no imputation is necessary. Since, there will always be some discrepancy between traded prices of the asset and its hedge, a small bias is unavoidable without having continuous bid/ask quotes. If the holding period is sufficiently long, the hope is that this bias is negligible. . Proof by Simulation that ffill() Price Intoduces Bias . n_obs = 10000 # Common Gaussain returns epsilon = np.random.normal(0.0,0.01, n_obs) # Gaussian specific innovations omega = np.random.normal(0, 0.01, epsilon.shape[0]) omega_H = np.random.normal(0, 0.01, epsilon.shape[0]) S = omega.cumsum() S_H = omega_H.cumsum() # Mean reverting process ################################################# # Setting the mean reversion speed to zero implies # a random walk and thus profits shouldn&#39;t be possible R = ornstein_uhlenbeck(0.0, 0.0, epsilon.shape[0]+1, mu=0, x0=0) R_H = ornstein_uhlenbeck(0.0, 0.0, epsilon.shape[0]+1, mu=0, x0=0) ################################################# H = np.exp(np.cumsum(epsilon + omega_H + np.diff(R_H))) A = np.exp(np.cumsum(epsilon + omega + np.diff(R))) na_prices = pd.DataFrame(data={&#39;H&#39;:H, &#39;A&#39;:A}) na_prices.loc[np.random.randint(1,len(na_prices), int(0.9*len(na_prices))), &#39;A&#39;] = np.nan na_prices.index = pd.date_range(start=&#39;1/1/2000&#39;, periods=len(na_prices), freq=&#39;B&#39;) plt.plot(na_prices) plt.title(&#39;Prices (raw)&#39;) plt.show() #############CULPRIT############################# ################################################# na_prices.ffill(inplace=True) plt.plot(na_prices) plt.title(&#39;Prices (ffill)&#39;) plt.show() ################################################# ################################################# Y = np.log(na_prices.A) - np.log(na_prices.A.iloc[0]) - (np.log(na_prices.H) - np.log(na_prices.H.iloc[0])) x = Y train_pct = 0.5 train = int(train_pct*x.shape[0]) order = (1, 1, 1) # IS fit mod = sm.tsa.statespace.SARIMAX(x[:train], order=order) fit_results = mod.fit() print(fit_results.summary()) _ = fit_results.plot_diagnostics(figsize=(15, 12)) plt.show() # IS prediction pred = fit_results.get_prediction(dynamic=False) trad_ret = (pred.predicted_mean-x.shift(1))*x.diff() trad_ret.cumsum().plot() plt.title(&#39;IS&#39;) plt.show() ir = trad_ret.mean()/(trad_ret.std()/np.sqrt(250)) print(&#39;IR&#39;,ir) # OOS prediction mod = sm.tsa.statespace.SARIMAX(x[train:], order=order) oos_results = mod.filter(fit_results.params)# this is needed to ingest data but doesn&#39;t change my test data pred = oos_results.get_prediction(dynamic=False,start=100) pred_ci = pred.conf_int() trad_ret = (pred.predicted_mean-x.shift(1))*x.diff() trad_ret.cumsum().plot() plt.title(&#39;OOS&#39;) plt.show() ir = trad_ret.mean()/(trad_ret.std()/np.sqrt(250)) print(&#39;IR&#39;,ir) (pred.predicted_mean.diff()-x.diff()).plot() plt.title(&#39;log-return prediction error&#39;) plt.show() . [&lt;matplotlib.lines.Line2D at 0x1c1e864828&gt;, &lt;matplotlib.lines.Line2D at 0x1c238eb8d0&gt;] . Text(0.5, 1.0, &#39;Prices (raw)&#39;) . [&lt;matplotlib.lines.Line2D at 0x1c1e8462e8&gt;, &lt;matplotlib.lines.Line2D at 0x1c1ed85550&gt;] . Text(0.5, 1.0, &#39;Prices (ffill)&#39;) . Statespace Model Results ============================================================================== Dep. Variable: y No. Observations: 5000 Model: SARIMAX(1, 1, 1) Log Likelihood 13003.408 Date: Wed, 04 Dec 2019 AIC -26000.816 Time: 21:00:50 BIC -25981.265 Sample: 01-03-2000 HQIC -25993.964 - 03-01-2019 Covariance Type: opg ============================================================================== coef std err z P&gt;|z| [0.025 0.975] ar.L1 0.5226 0.100 5.211 0.000 0.326 0.719 ma.L1 -0.6070 0.093 -6.522 0.000 -0.789 -0.425 sigma2 0.0003 4.82e-06 66.777 0.000 0.000 0.000 =================================================================================== Ljung-Box (Q): 33.45 Jarque-Bera (JB): 754.20 Prob(Q): 0.76 Prob(JB): 0.00 Heteroskedasticity (H): 0.99 Skew: -0.03 Prob(H) (two-sided): 0.87 Kurtosis: 4.90 =================================================================================== Warnings: [1] Covariance matrix calculated using the outer product of gradients (complex-step). . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c1ecbdc18&gt; . Text(0.5, 1.0, &#39;IS&#39;) . IR 1.6302357948054453 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c1edf2860&gt; . Text(0.5, 1.0, &#39;OOS&#39;) . IR 1.700099668801834 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c216982e8&gt; . Text(0.5, 1.0, &#39;log-return prediction error&#39;) . The positve profits, where no profits should be possible, show that the ffill introduced a bias. . Parametric Tests of Local Mean Reversion . The autocorrelation function (ACF) of an ARMA(1,1) process is given by $$ rho( tau)= frac{ gamma( tau)}{ gamma(0)}= phi^{ tau-1} frac{( phi+ theta)(1+ phi theta)}{1+2 theta phi+ theta^{2}}, quad tau geq 1.$$ . def arma11_acf(tau, phi, theta): return phi**(tau-1)*((phi+theta)*(1+phi*theta)/(1+2*theta*phi+theta**2)) df = pd.DataFrame(index=range(1,21)) for phi in (-0.8,0.8,0.9): for theta in (-0.89, -0.99): df[f&#39;$ phi$={phi} &#39; +r&#39;$ theta$=&#39; +str(theta)] = df.index.to_series().apply(arma11_acf, args=(phi,theta)) df.plot() plt.title(&#39;ACF ARMA(1,1)&#39;) plt.show() df . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c20c39978&gt; . Text(0.5, 1.0, &#39;ACF ARMA(1,1)&#39;) . $ phi$=-0.8 $ theta$=-0.89 $ phi$=-0.8 $ theta$=-0.99 $ phi$=0.8 $ theta$=-0.89 $ phi$=0.8 $ theta$=-0.99 $ phi$=0.9 $ theta$=-0.89 $ phi$=0.9 $ theta$=-0.99 . 1 -0.899624 | -0.899997 | -0.070416 | -0.099773 | 0.010468 | -0.049520 | . 2 0.719699 | 0.719998 | -0.056333 | -0.079818 | 0.009421 | -0.044568 | . 3 -0.575759 | -0.575998 | -0.045066 | -0.063855 | 0.008479 | -0.040112 | . 4 0.460607 | 0.460799 | -0.036053 | -0.051084 | 0.007631 | -0.036100 | . 5 -0.368486 | -0.368639 | -0.028842 | -0.040867 | 0.006868 | -0.032490 | . 6 0.294789 | 0.294911 | -0.023074 | -0.032694 | 0.006181 | -0.029241 | . 7 -0.235831 | -0.235929 | -0.018459 | -0.026155 | 0.005563 | -0.026317 | . 8 0.188665 | 0.188743 | -0.014767 | -0.020924 | 0.005007 | -0.023685 | . 9 -0.150932 | -0.150994 | -0.011814 | -0.016739 | 0.004506 | -0.021317 | . 10 0.120745 | 0.120796 | -0.009451 | -0.013391 | 0.004056 | -0.019185 | . 11 -0.096596 | -0.096636 | -0.007561 | -0.010713 | 0.003650 | -0.017267 | . 12 0.077277 | 0.077309 | -0.006049 | -0.008570 | 0.003285 | -0.015540 | . 13 -0.061822 | -0.061847 | -0.004839 | -0.006856 | 0.002957 | -0.013986 | . 14 0.049457 | 0.049478 | -0.003871 | -0.005485 | 0.002661 | -0.012587 | . 15 -0.039566 | -0.039582 | -0.003097 | -0.004388 | 0.002395 | -0.011329 | . 16 0.031653 | 0.031666 | -0.002478 | -0.003510 | 0.002155 | -0.010196 | . 17 -0.025322 | -0.025333 | -0.001982 | -0.002808 | 0.001940 | -0.009176 | . 18 0.020258 | 0.020266 | -0.001586 | -0.002247 | 0.001746 | -0.008259 | . 19 -0.016206 | -0.016213 | -0.001268 | -0.001797 | 0.001571 | -0.007433 | . 20 0.012965 | 0.012970 | -0.001015 | -0.001438 | 0.001414 | -0.006689 | . Likelihood ratio test . The thinking behind this test is that if the likelihood of the data is significantly greater under the ARIMA(1,1,1) than under a random walk ARIMA(0,1,0) we can conclude that there is local mean reversion. . from scipy.stats.distributions import chi2 def get_lrt_pvalue(specific_rw_distribution=&#39;normal&#39;): p_list = [] for i in tqdm_notebook(range(100)): # Common Gaussian shocks epsilon = np.random.normal(0.0001,0.02,1000) # Gaussian or GARCH specific shocks if specific_rw_distribution == &#39;normal&#39;: omega = np.random.normal(0, 0.01, epsilon.shape[0]) elif specific_rw_distribution == &#39;garch&#39;: omega = generate_garch_11_ts(epsilon.shape[0], 0.001, 0, 0.4, 0.49, 0.00001) else: raise ValueError(&#39;only `normal` or `garch` specific_rw_distribution supported.&#39;) # Mean reverting &#39;mispricing&#39; R = ornstein_uhlenbeck(0.1, 0.02, epsilon.shape[0]+1, mu=0, x0=0) S = omega.cumsum() C = epsilon.cumsum() H = np.exp(C) A = H*np.exp(S + R[1:]) prices = pd.DataFrame(data={&#39;H&#39;:H, &#39;A&#39;:A}) prices.index = pd.date_range(start=&#39;1/1/2000&#39;, periods=len(prices), freq=&#39;B&#39;) y = (np.log(prices.A) - np.log(prices.A.iloc[0]) - (np.log(prices.H) - np.log(prices.H.iloc[0]))) mod = sm.tsa.statespace.SARIMAX(y, order=(1,1,1)) fit_results = mod.fit() null_mod = sm.tsa.statespace.SARIMAX(y, order=(0,1,0)) null_fit_results = null_mod.fit() lr = 2*(fit_results.llf-null_fit_results.llf) p = chi2.sf(lr, 2) p_list.append(p) return p_list . p_list = get_lrt_pvalue(specific_rw_distribution=&#39;normal&#39;) p_series = pd.Series(p_list) print(&#39; null rejected:&#39;, (p_series&lt;0.05).sum()/len(p_series)) . null rejected: 0.63 . p_list = get_lrt_pvalue(specific_rw_distribution=&#39;garch&#39;) p_series = pd.Series(p_list) print(&#39; null rejected:&#39;, (p_series&lt;0.05).sum()/len(p_series)) . null rejected: 0.55 . from scipy.stats.distributions import chi2 def get_lrt_pvalue_rw(specific_rw_distribution=&#39;normal&#39;): p_list = [] for i in tqdm_notebook(range(100)): # Gaussian or GARCH specific shocks if specific_rw_distribution == &#39;normal&#39;: RW = pd.Series(np.random.normal(0.0001,0.02,1000)).cumsum() elif specific_rw_distribution == &#39;garch&#39;: RW = pd.Series(generate_garch_11_ts(1000, 0.0001, 0, 0.4, 0.49, 0.00001)).cumsum() else: raise ValueError(&#39;only `normal` or `garch` specific_rw_distribution supported.&#39;) mod = sm.tsa.statespace.SARIMAX(RW, order=(1,1,1)) fit_results = mod.fit() null_mod = sm.tsa.statespace.SARIMAX(RW, order=(0,1,0)) null_fit_results = null_mod.fit() lr = 2*(fit_results.llf-null_fit_results.llf) p = chi2.sf(lr, 2) p_list.append(p) return p_list . p_list = get_lrt_pvalue_rw(specific_rw_distribution=&#39;normal&#39;) p_series = pd.Series(p_list) print(&#39; null rejected:&#39;, (p_series&lt;0.05).sum()/len(p_series)) . null rejected: 0.01 . p_list = get_lrt_pvalue_rw(specific_rw_distribution=&#39;garch&#39;) p_series = pd.Series(p_list) print(&#39;null rejected:&#39;, (p_series&lt;0.05).sum()/len(p_series)) . null rejected: 0.23 . The likelihood ratio test works nicely if returns follow a Gaussian distribution. For heavy tailed distributions the null is overrejected significantly. . ARMA coefficient test with heteroskedasticity robust standard errors . t_ar_list = [] t_ar_ma_list = [] for i in tqdm_notebook(range(1000)): n_obs = 1000 #Specific shocks omega = generate_garch_11_ts(n_obs, 0.001, 0, 0.4, 0.49, 0.00001) S = omega.cumsum() # Mean reverting &#39;mispricing&#39; R = ornstein_uhlenbeck(0.1, 0.02, n_obs, mu=0, x0=0) y = pd.Series(S + R) mod = sm.tsa.statespace.SARIMAX(y, order=(1,1,1)) fit_results = mod.fit() t_ar = fit_results.params[&#39;ar.L1&#39;]/np.sqrt(fit_results.cov_params_robust[&#39;ar.L1&#39;][&#39;ar.L1&#39;]) t_ar_ma = (fit_results.params[&#39;ar.L1&#39;]+fit_results.params[&#39;ma.L1&#39;])/np.sqrt( fit_results.cov_params_robust[&#39;ar.L1&#39;][&#39;ar.L1&#39;] + fit_results.cov_params_robust[&#39;ma.L1&#39;][&#39;ma.L1&#39;] + 2*fit_results.cov_params_robust[&#39;ar.L1&#39;][&#39;ma.L1&#39;]) t_ar_list.append(t_ar) t_ar_ma_list.append(t_ar_ma) . df = pd.DataFrame({&#39;t_ar&#39;:t_ar_list, &#39;t_ar_ma&#39;:t_ar_ma_list}) print(&#39;null rejected:&#39;, len(df[((df.t_ar_ma&lt;-1.645))&amp;((df.t_ar&gt;1.645))])/(i+1)) print(&#39;null rejected:&#39;, len(df[((df.t_ar&gt;1.645))])/(i+1)) print(&#39;null rejected:&#39;, len(df[((df.t_ar_ma&lt;-1.645))])/(i+1)) print(&#39;null rejected:&#39;, len(df[((df.t_ar_ma&lt;-1.645))&amp;((df.t_ar&gt;0))])/(i+1)) . null rejected: 0.479 null rejected: 0.509 null rejected: 0.634 null rejected: 0.589 . t_ar_list = [] t_ar_ma_list = [] for i in tqdm_notebook(range(1000)): RW = pd.Series(generate_garch_11_ts(1000, 0.0001, 0, 0.4, 0.49, 0.00001)).cumsum() mod = sm.tsa.statespace.SARIMAX(RW, order=(1,1,1)) fit_results = mod.fit() t_ar = fit_results.params[&#39;ar.L1&#39;]/np.sqrt(fit_results.cov_params_robust[&#39;ar.L1&#39;][&#39;ar.L1&#39;]) t_ar_ma = (fit_results.params[&#39;ar.L1&#39;]+fit_results.params[&#39;ma.L1&#39;])/np.sqrt( fit_results.cov_params_robust[&#39;ar.L1&#39;][&#39;ar.L1&#39;] + fit_results.cov_params_robust[&#39;ma.L1&#39;][&#39;ma.L1&#39;] + 2*fit_results.cov_params_robust[&#39;ar.L1&#39;][&#39;ma.L1&#39;]) t_ar_list.append(t_ar) t_ar_ma_list.append(t_ar_ma) . df = pd.DataFrame({&#39;t_ar&#39;:t_ar_list, &#39;t_ar_ma&#39;:t_ar_ma_list}) print(&#39;null rejected:&#39;, len(df[((df.t_ar_ma&lt;-1.645))&amp;((df.t_ar&gt;1.645))])/(i+1)) print(&#39;null rejected:&#39;, len(df[((df.t_ar&gt;1.645))])/(i+1)) print(&#39;null rejected:&#39;, len(df[((df.t_ar_ma&lt;-1.645))])/(i+1)) print(&#39;null rejected:&#39;, len(df[((df.t_ar_ma&lt;-1.645))&amp;((df.t_ar&gt;0))])/(i+1)) . null rejected: 0.005 null rejected: 0.073 null rejected: 0.06 null rejected: 0.024 . Testing for Autocorrelations by AR(1) coefficient test with heteroskedasticity robust s.e. . To test whether there is a mean-reverting mispricing present, or the spread is better described by a pure random walk, we can use an autocorrelation test. Since the spread will almost certainly be heavy tailed. A Ljung-Box test, in its unmodified form, does not test our hypothesis. We could either modify it or simply test for joint significance of AR($p$) coefficients with heteroskedasticity robust standard errors. . t_ar_list = [] t_ar_ma_list = [] for i in tqdm_notebook(range(100)): # Common Gaussian shocks epsilon = np.random.normal(0.0001,0.02,1000) omega = generate_garch_11_ts(epsilon.shape[0], 0.001, 0, 0.4, 0.49, 0.00001) # Mean reverting &#39;mispricing&#39; R = ornstein_uhlenbeck(0.1, 0.02, epsilon.shape[0]+1, mu=0, x0=0) S = omega.cumsum() C = epsilon.cumsum() H = np.exp(C) A = H*np.exp(S + R[1:]) prices = pd.DataFrame(data={&#39;H&#39;:H, &#39;A&#39;:A}) prices.index = pd.date_range(start=&#39;1/1/2000&#39;, periods=len(prices), freq=&#39;B&#39;) y = (np.log(prices.A) - np.log(prices.A.iloc[0]) - (np.log(prices.H) - np.log(prices.H.iloc[0]))) mod = sm.tsa.statespace.SARIMAX(y, order=(1,1,0)) fit_results = mod.fit() t_ar = fit_results.params[&#39;ar.L1&#39;]/np.sqrt(fit_results.cov_params_robust[&#39;ar.L1&#39;][&#39;ar.L1&#39;]) t_ar_list.append(t_ar) df = pd.DataFrame({&#39;t_ar&#39;:t_ar_list}) print(&#39;null rejected:&#39;, len(df[df.t_ar&lt;-1.645])/(i+1)) . null rejected: 0.29 . t_ar_list = [] for i in tqdm_notebook(range(100)): RW = pd.Series(generate_garch_11_ts(1000, 0.0001, 0, 0.4, 0.49, 0.00001)).cumsum() mod = sm.tsa.statespace.SARIMAX(RW, order=(1,1,0)) fit_results = mod.fit() t_ar = fit_results.params[&#39;ar.L1&#39;]/np.sqrt(fit_results.cov_params_robust[&#39;ar.L1&#39;][&#39;ar.L1&#39;]) t_ar_list.append(t_ar) df = pd.DataFrame({&#39;t_ar&#39;:t_ar_list}) print(&#39;null rejected:&#39;, len(df[df.t_ar&lt;-1.645])/(i+1)) . null rejected: 0.07 . Testing for Autocorrelations by AR($p$) coefficient test with heteroskedasticity robust s.e. . from statsmodels.regression.linear_model import OLS f_list = [] for i in tqdm_notebook(range(1000)): n_obs = 1000 #Specific shocks omega = generate_garch_11_ts(n_obs, 0.001, 0, 0.4, 0.49, 0.00001) S = omega.cumsum() # Mean reverting &#39;mispricing&#39; R = ornstein_uhlenbeck(0.1, 0.02, n_obs, mu=0, x0=0) y = pd.Series(S + R) diff = y.diff() X = pd.DataFrame() for l in range(0,10): X[f&#39;lag_{l}&#39;] = diff.shift(l) X = X.dropna() reg = OLS(X.iloc[:,0], X.iloc[:,1:]) fit = reg.fit() # fit.summary() f_list.append(fit.f_test(np.eye(l),fit.cov_HC3).pvalue) df = pd.DataFrame({&#39;pvalue&#39;:f_list}) print(&#39;null rejected:&#39;, len(df[df.pvalue&lt;0.05])/(i+1)) . null rejected: 0.578 . from statsmodels.regression.linear_model import OLS f_list = [] for i in tqdm_notebook(range(1000)): RW = pd.Series(generate_garch_11_ts(1000, 0.0001, 0, 0.4, 0.49, 0.00001)).cumsum() diff = RW.diff() X = pd.DataFrame() for l in range(0,10): X[f&#39;lag_{l}&#39;] = diff.shift(l) X = X.dropna() reg = OLS(X.iloc[:,0], X.iloc[:,1:]) fit = reg.fit() # fit.summary() f_list.append(fit.f_test(np.eye(l),fit.cov_HC3).pvalue) df = pd.DataFrame({&#39;pvalue&#39;:f_list}) print(&#39;null rejected:&#39;, len(df[df.pvalue&lt;0.05])/(i+1)) . null rejected: 0.073 . Optimal Trading under Transaction costs . The nice thing about the ARIMA model is that a multi-period forecasts are easy to obtain. Hence, one can compute the expected return from now until some point in the future and compare the maximum cumulative return to the expected transaction costs. The decision to enter a position can then be based on the sign and magnitude of the difference. . n_obs = 50000 # Common Gaussain returns epsilon = np.random.normal(0.0,0.01, n_obs) # Gaussian specific innovations omega = np.random.normal(0, 0.001, epsilon.shape[0]) omega_H = np.random.normal(0, 0.001, epsilon.shape[0]) S = omega.cumsum() S_H = omega_H.cumsum() # Mean reverting process R = ornstein_uhlenbeck(0.01, 0.01, epsilon.shape[0]+1, mu=0, x0=0) R_H = ornstein_uhlenbeck(0.1, 0.01, epsilon.shape[0]+1, mu=0, x0=0) H = np.exp(np.cumsum(epsilon + omega_H + np.diff(R_H))) A = np.exp(np.cumsum(epsilon + omega + np.diff(R))) prices = pd.DataFrame(data={&#39;H&#39;:H, &#39;A&#39;:A}) prices.index = pd.date_range(start=&#39;1/1/2000&#39;, periods=len(prices), freq=&#39;B&#39;) Y = np.log(prices.A) - np.log(prices.A.iloc[0]) - (np.log(prices.H) - np.log(prices.H.iloc[0])) x = Y order = (1, 1, 1) mod = sm.tsa.statespace.SARIMAX(x, order=order) modl_fit = mod.fit() modl_fit.forecast(100).plot() plt.title(&#39;Multi-period spread forecast&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c1e846518&gt; . Text(0.5, 1.0, &#39;Multi-period spread forecast&#39;) .",
            "url": "https://jpwoeltjen.github.io/researchBlog/efficiency/spreads/time-series/stat-arb/2019/12/04/NonstationarySpreadsModeling.html",
            "relUrl": "/efficiency/spreads/time-series/stat-arb/2019/12/04/NonstationarySpreadsModeling.html",
            "date": " • Dec 4, 2019"
        }
        
    
  
    
        ,"post3": {
            "title": "Quantitative Risk Management",
            "content": "import pandas as pd import scipy.stats import numpy as np import pylab import scipy.stats as stats from matplotlib import pyplot as plt plt.style.use(&#39;seaborn&#39;) # %matplotlib notebook import seaborn as sns import random random.seed(1) . 1 VaR and Expected Shortfall . Definition 1.1 . Let $ alpha in (0, 1)$ be a fixed level. The Value-at-Risk (VaR) for level $ alpha in (0, 1)$ is defined as . $$VaR_ alpha(L_{n+1}) = inf { mathcal{l} in mathbb{R} : P(L_{n+1} &gt; mathcal{l}) leq 1- alpha } $$ . Definition 1.2 . Suppose that the conditional law of $ { L_{n+1}$ given $Z_0, . . . , Z_n$ is continuous. For fixed level $ alpha in (0, 1)$ the expected shortfall at level $ alpha$ is defined as . $$ES_ alpha(L_{n+1}) = mathbb{E}_n { L_{n+1} | L_{n+1}&gt;VaR_ alpha(L_{n+1}) } $$ . Let&#39;s study these risk measures with simulated log-normal returns and fat-tailed but stationary GARCH returns. . 1.1 GARCH . def generate_garch_11_ts(n, sigma_sq_0, mu, alpha, beta, omega): &quot;&quot;&quot; generate GARCH log returns &quot;&quot;&quot; nu = np.random.normal(0,1,n) r = np.zeros(n) epsilon = np.zeros(n) sigma_sq = np.zeros(n) sigma_sq[0] = sigma_sq_0 if min(alpha,beta)&lt;0: raise ValueError(&#39;alpha, beta need to be non-negative&#39;) if omega &lt;=0: raise ValueError(&#39;omega needs to be positive&#39;) if alpha+beta&gt;=1: print(&#39;alpha+beta&gt;=1, variance not defined --&gt; time series will not be weakly stationary&#39;) for i in range(n): if i &gt;0: sigma_sq[i] = omega + alpha * epsilon[i-1]**2 + beta * sigma_sq[i-1] epsilon[i] = (sigma_sq[i]**0.5) * nu[i] r[i] = mu + epsilon[i] return r . garch_returns = generate_garch_11_ts(250*15, 1e-5, 0.0, 0.5, 0.45, 1e-5) price = np.exp(np.cumsum(garch_returns)) fig, axes = plt.subplots(2,1) fig.tight_layout() for ax, y, name in zip(axes, [price,garch_returns], [&#39;GARCH Price Time Series&#39;,&#39; GARCH Returns&#39;]): ax.plot(y) ax.set(title=name) plt.show() . stats.probplot(garch_returns, dist=&quot;norm&quot;, plot=pylab) pylab.show() . sns.distplot(garch_returns, bins=200, label=&#39;GARCH-1-1 Returns&#39;) plt.legend() plt.show() . The QQ-Plot very clearly shows fat tails. For the histogram, however, one could falsely assume normality. . def VaR_log_normal(x, alpha, lookback): &quot;&quot;&quot; compute daily VaR given log-normal returns &quot;&quot;&quot; quantile = scipy.stats.norm.ppf(alpha) x = pd.Series(x) returns = (np.log(x)).diff() mu = returns.rolling(lookback).mean() sigma = returns.rolling(lookback).std() VaR = x*(1-np.exp((mu-sigma*quantile))) return VaR alpha = 0.99 lookback = 250 VaR = VaR_log_normal(price, alpha, lookback) . loss = -pd.Series(np.diff(price)) violation = (loss.values[lookback:]&gt;VaR.values[lookback:-1]).astype(int) violation . array([0, 0, 0, ..., 0, 0, 0]) . plt.plot(loss) plt.plot(VaR) plt.title(&#39;Loss vs. VaR&#39;) plt.show() . print(f&#39;Relative Frequency of Violations ({round(1-alpha,4)} expected under log-normal assumption): &#39;, violation.mean()) . Relative Frequency of Violations (0.01 expected under log-normal assumption): 0.022577879394112602 . 1.2 Log-Normal . log_normal_returns = np.random.normal(0,0.1/(250**0.5),len(garch_returns)) ln_price = np.exp(np.cumsum(log_normal_returns)) fig, axes = plt.subplots(2,1) fig.tight_layout() for ax, y, name in zip(axes, [ln_price,log_normal_returns], [&#39;LN Asset Price Time Series&#39;,&#39; Log-Normal Returns&#39;]): ax.plot(y) ax.set(title=name) plt.show() . stats.probplot((log_normal_returns), dist=&quot;norm&quot;, plot=pylab) pylab.show() . stats.probplot(np.exp(log_normal_returns), dist=&quot;norm&quot;, plot=pylab) pylab.show() . For comparison, below are platykurtic and leptokurtic distributions: . Platykurtic (negative excess kurtosis) . stats.probplot(np.random.uniform(0,1,log_normal_returns.shape), dist=&quot;norm&quot;, plot=pylab) pylab.show() . Leptokurtic (positive excess kurtosis) . stats.probplot(np.random.standard_t(3,log_normal_returns.shape), dist=&quot;norm&quot;, plot=pylab) pylab.show() . ln_VaR = VaR_log_normal(ln_price, alpha, lookback) ln_loss = -pd.Series(np.diff(ln_price)) ln_violation = (ln_loss.values[lookback:]&gt;ln_VaR.values[lookback:-1]).astype(int) . plt.plot(ln_loss) plt.plot(ln_VaR) plt.title(&#39;ln_Loss vs. ln_VaR&#39;) plt.show() . print(f&#39;Relative Frequency of Violations ({round(1-alpha,4)} expected under log-normal assumption): &#39;, ln_violation.mean()) . Relative Frequency of Violations (0.01 expected under log-normal assumption): 0.00943126607602172 . As can be seen from the above experiments the log-normal VaR estimate severely under estimates the risk if the return distribution is fat tailed. . def phi(x): return np.exp(-0.5*x**2)/np.sqrt(2*np.pi); def VaR_ES_var_covar (x, c, w, alpha): mu = np.mean(x,axis=0); sigma = np.cov(x); q = scipy.stats.norm.ppf(alpha) VaR = -(c + np.dot(np.transpose(w),np.transpose(mu))) + np.sqrt(np.dot(np.transpose(w),np.dot(sigma,w)))*q ES = -(c + np.dot(np.transpose(w),np.transpose(mu))) + np.sqrt(np.dot(np.transpose(w),np.dot(sigma,w)))/(1-alpha)*phi(q) return VaR, ES VaR, ES = VaR_ES_var_covar (garch_returns, 0, 1, alpha) print(&#39;VaR GARCH: &#39;, VaR) print(&#39;Expected Shortfall GARCH: &#39;, ES) print(&#39;Mean Violation GARCH: &#39;,np.mean(-garch_returns[garch_returns&lt;-VaR])) . VaR GARCH: 0.03146499939671988 Expected Shortfall GARCH: 0.03605991516868822 Mean Violation GARCH: 0.052666650092480304 . VaR, ES = VaR_ES_var_covar (log_normal_returns, 0, 1, alpha) print(&#39;VaR Log-Normal: &#39;, VaR) print(&#39;Expected Shortfall Log-Normal: &#39;, ES) print(&#39;Mean Violation Log-Normal: &#39;,np.mean(-log_normal_returns[log_normal_returns&lt;-VaR])) . VaR Log-Normal: 0.01472793412031715 Expected Shortfall Log-Normal: 0.016875034681794663 Mean Violation Log-Normal: 0.01717357925520597 . The Var-Covar method assumes iid Gaussian risk factor changes. It thus under-estimates ES of the GARCH model. The ES estimate of the log-normal returns is unbiased. . def VaR_ES_historic(x, alpha): x = np.sort(x) index = int(len(x)*(1-alpha))+1 VaR = x[index] ES = 1/index*np.sum(x[:index]) return -VaR,-ES . hist_VaR,hist_ES = VaR_ES_historic(garch_returns, alpha) print(&#39;VaR GARCH: &#39;, hist_VaR) print(&#39;Expected Shortfall GARCH: &#39;, hist_ES) . VaR GARCH: 0.034047583394326314 Expected Shortfall GARCH: 0.05577800317882427 . Some remarks regarding VaR and ES . Coherent risk measures . A coherent risk measure is a function ${ displaystyle varrho } $ that satisfies properties of monotonicity, sub-additivity, homogeneity, and translational invariance. . Monotonicity . ${ displaystyle mathrm {If} ;Z_{1},Z_{2} in { mathcal {L}} ; mathrm {and} ;Z_{1} leq Z_{2} ; mathrm {a.s.} , ; mathrm {then} ; varrho (Z_{1}) geq varrho (Z_{2})} $ That is, if portfolio ${ displaystyle Z_{2}} $ always has better values than portfolio ${ displaystyle Z_{1}}$ under almost all scenarios then the risk of ${ displaystyle Z_{2}} $ should be less than the risk of ${ displaystyle Z_{1}}$. . Sub-additivity . ${ displaystyle mathrm {If} ;Z_{1},Z_{2} in { mathcal {L}}, ; mathrm {then} ; varrho (Z_{1}+Z_{2}) leq varrho (Z_{1})+ varrho (Z_{2})} $ Indeed, the risk of two portfolios together cannot get any worse than adding the two risks separately: this is the diversification principle. In financial risk management, sub-additivity implies diversification is beneficial. . Positive homogeneity . ${ displaystyle mathrm {If} ; alpha geq 0 ; mathrm {and} ;Z in { mathcal {L}}, ; mathrm {then} ; varrho ( alpha Z)= alpha varrho (Z)} $ Loosely speaking, if you double your portfolio then you double your risk. In financial risk management, positive homogeneity implies the risk of a position is proportional to its size. . Translation invariance . If ${ displaystyle A}$ is a deterministic portfolio with guaranteed return ${ displaystyle a}$ and ${ displaystyle Z in { mathcal {L}}} $ then . $ { displaystyle varrho (Z+A)= varrho (Z)-a} varrho(Z + A) = varrho(Z) - a $ The portfolio ${ displaystyle A}$ is just adding cash ${ displaystyle a} $ to your portfolio ${ displaystyle Z}$. In particular, if ${ displaystyle a= varrho (Z)}$ then ${ displaystyle varrho (Z+A)=0}$. In financial risk management, translation invariance implies that the addition of a sure amount of capital reduces the risk by the same amount. . Convex risk measures . The notion of coherence has been subsequently relaxed. Indeed, the notions of Sub-additivity and Positive Homogeneity can be replaced by the notion of convexity: . Convexity . ${ displaystyle { text{If }}Z_{1},Z_{2} in { mathcal {L}}{ text{ and }} lambda in [0,1]{ text{ then }} varrho ( lambda Z_{1}+(1- lambda )Z_{2}) leq lambda varrho (Z_{1})+(1- lambda ) varrho (Z_{2})}$ . VaR is not a convex risk measure, meaning diversification with non-perfectly-correlated assets does not always lead to reduction in the risk measure, which is a serious flaw. In addition, VaR only provides information about the probability of a large loss but not its amount. ES does not suffer from these drawbacks. . 2 Extreme Value Theory . Definition 2.1 . A function $h : (0,+ infty) rightarrow (0,+ infty)$ with the limit . $$ g(t)= lim _{{t to infty }}{ frac {h(tx)}{h(x)}} = x^ rho $$ . for every $x &gt; 0$, is called regularly varying in $ infty$ with index $ rho in mathbb{R}$ (written $h in RV_ rho$) . For $ rho = 0$, and hence $g(t)=1$ we say that $h$ is slowly varying in $ infty$. . A random variable $X$ with cdf $F$ is called regularly varying if $ bar F in RV_{- alpha}$ for some $ alpha geq 0$, where $ bar F(x) = 1 - F(x)$. . To estimate the parameter $ alpha$ of a regularly varying random variable $X$, we use the Hill estimator. . Hill&#39;s tail-index estimator . $$ displaystyle hat alpha _{(k(n),n)}^{ text{Hill}}= left({ frac {1}{k(n)}} sum _{i=n-k(n)+1}^{n} ln(X_{(i,n)})- ln(X_{(n-k(n)+1,n)}) right)^{-1}$$ . Choose k graphically by looking at the Hill plot. If we are lucky, the plot looks approximately constant after some initial oscillations. This is the part where k and hence the estimate should be chosen. . def Hill_Estimator (x, k): y = np.sort(x)[::-1] return k / np.sum( np.log(y[:k-1]) - np.log(y[k]) ) def Hill_Plot(x): y=x[x&gt;0] n = len(y) a = [] for k in range(1,n): alpha_hat = Hill_Estimator(y,k) a.append(alpha_hat) plt.plot(range(1,n), a) plt.show() def VaR_ES_Hill(x, p, k): n = len(x) alpha = Hill_Estimator(x,k) y = np.sort(x)[::-1] VaR= (n/k*(1-p))**(-1/alpha)*y[k] ES=(1-1/alpha)**(-1)*VaR return VaR,ES . # n=1000 x = garch_returns[:1000]#np.random.standard_t(10, n)**2 Hill_Plot(x) . /Users/jan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in double_scalars This is separate from the ipykernel package so we can avoid doing imports until . k = 50 VaR, ES = VaR_ES_Hill(x,0.95,k) alpha = Hill_Estimator(x,k) print(&#39;Alpha_HILL: &#39;, alpha) print(&#39;VaR HILL: &#39;, VaR) print(&#39;ES HILL: &#39;, ES) . Alpha_HILL: 2.2348170736040154 VaR HILL: 0.019049928996944472 ES HILL: 0.034477257792572595 . Peaks Over Treshold (POT) . Let $X$ be a random variable with cdf $F$ . Define its excess distribution function over threshold $u$ as . $$ displaystyle F_{u}(y)=P(X-u leq y|X&gt;u)={ frac {F(u+y)-F(u)}{1-F(u)}}$$ . for $0 leq y leq x_{F}-u$ . . def mean_exess_fct(x, u): if u&gt;=max(x): raise ValueError(&quot; u must be smaller than max(x) &quot;) e = np.mean(x[x&gt;u]-u) return e def mean_exess_plot(x): x = pd.Series(x) x = x.sort_values(ascending=False) e = x[1:].apply(lambda u: mean_exess_fct(x, u)) plt.plot(x[1:],e,&#39;.&#39;) plt.xlabel(r&#39;$u$&#39;) plt.ylabel(r&#39;$e(u)$&#39;) plt.title(&#39;Mean Exess Plot&#39;) plt.show() . print(&#39;log-normal returns&#39;) mean_exess_plot(-log_normal_returns) print(&#39;GARCH returns&#39;) mean_exess_plot(-garch_returns) . log-normal returns . GARCH returns . If the mean exess plot behaves approximately linear with positive slope for some u, it is reasonable to assume that the tails are generalized pareto distributed and u should be chosen where this is the case. Here, for the GARCH returns, this would be around 0.025. Since this rule is nowhere true for log-normal returns, it is not reasonable to assume tails to have generalized pareto distribution. . def ml_estimator_pareto(u, x): y = x[x&gt;u]-u N_u = len(y) def log_likelihood(x): #negative log likelihood of generalized pareto distribution return N_u*np.log(x[1])+(1/x[0]+1)*np.sum(np.log(1+x[0]/x[1]*y)) #first guess x0=[1,2] #bounds bnds = [(0.000001,5),(0.000001,5)] res = scipy.optimize.minimize(log_likelihood, x0, bounds=bnds) if not res.success: raise ValueError(&#39;optimization unsuccessful&#39;) gamma = res.x[0] beta = res.x[1] return gamma, beta, N_u . def var_pot(p,gamma, beta, N_u, n, u): return u + beta/gamma *( (n/N_u*(1-p))**(-gamma) - 1 ) def es_pot(q, beta, gamma, u): return q + (beta + gamma*(q - u))/(1-gamma) . from scipy.stats import genpareto u = 0.025 n = len(garch_returns) gamma, beta, N_u = ml_estimator_pareto(u, -garch_returns) p = 0.99 print(f&#39;================= ML ESTIMATION RESULTS =============== n Gamma: {gamma} n Beta: {beta} n&#39;) fig, ax = plt.subplots(1, 1) x = np.linspace(genpareto.ppf(0.01, gamma, beta), genpareto.ppf(0.99, gamma,beta), 100) ax.plot(x, genpareto.pdf(x, gamma,beta), &#39;b-&#39;, lw=1, alpha=0.6, label=f&#39;genpareto pdf, gamma={round(gamma,3)}, beta={round(beta,3)}&#39;) ax.plot(x, genpareto.cdf(x, gamma,beta), &#39;r-&#39;, lw=1, alpha=0.6, label=f&#39;genpareto cdf, gamma={round(gamma,3)}, beta={round(beta,3)}&#39;) ax.set(title = &#39;Generalized Pareto Distribution&#39;) ax.legend() plt.show() . ================= ML ESTIMATION RESULTS =============== Gamma: 0.42139052785832587 Beta: 0.00964087399792817 . VaR = var_pot(p,gamma, beta, N_u, n, u) ES = es_pot(VaR, beta, gamma, u) print(f&#39;================= POT ESTIMATES =============== n VaR: {VaR} n ES: {ES} n&#39;) . ================= POT ESTIMATES =============== VaR: 0.03442025133801539 ES: 0.05794298875784114 . Copulas . To model dependence we use copulas. In essence, these are multivariate distribution functions that transform the margins to be uniformly distributed with support [0,1]. . By Sklar’s theorem, any multivariate joint distribution can be written in terms of univariate marginal distribution functions and a copula which describes the dependence structure between the two variables. . For example, a Gaussian copula $ C_{R}^{ text{Gauss}}(u)= Phi _{R} left( Phi ^{-1}(u_{1}), dots , Phi ^{-1}(u_{d}) right) $ is a multivariate Gaussian distribution function that transforms the marginals into uniform [0,1]. Its density is ${ displaystyle c_{R}^{ text{Gauss}}(u)={ frac {1}{ sqrt { det {R}}}} exp left(-{ frac {1}{2}}{ begin{pmatrix} Phi ^{-1}(u_{1}) vdots Phi ^{-1}(u_{d}) end{pmatrix}}^{T} cdot left(R^{-1}-I right) cdot { begin{pmatrix} Phi ^{-1}(u_{1}) vdots Phi ^{-1}(u_{d}) end{pmatrix}} right)}$. To use them we model multivariate Gaussian rvs with positive-definite covariance matrix. We then transform the marginal realizations to standard uniform rvs by applying the CDF. After that, we transform the uniform marginals into the desired distribution function by applying the inverse CDF. . multi_variate_gauss = stats.multivariate_normal(mean=[0, 0], cov=[[1., 0.7], [0.7, 1.]]) x = multi_variate_gauss.rvs(5000) x_uniform = stats.norm().cdf(x) h = sns.jointplot(x_uniform[:, 0], x_uniform[:, 1], kind=&#39;hex&#39;, stat_func=None) h.set_axis_labels(&#39;Y1&#39;, &#39;Y2&#39;); . x1_transform = stats.genpareto(c=0.25).ppf(x_uniform[:, 0]) x2_transform = stats.beta(a=10, b=2).ppf(x_uniform[:, 1]) sns.jointplot(x1_transform, x2_transform, kind=&#39;kde&#39;, xlim=(0, 5), ylim=(.5, 1.0), stat_func=None); . Compare this to the jointplot of independent rvs. . x1_transform = stats.genpareto(c=0.25).ppf(stats.uniform().rvs(5000)) x2_transform = stats.beta(a=10, b=2).ppf(stats.uniform().rvs(5000)) sns.jointplot(x1_transform, x2_transform, kind=&#39;kde&#39;, xlim=(0, 5), ylim=(.5, 1.0), stat_func=None); . Tail Dependence . The lower tail dependence is defined as $${ displaystyle lambda _{ ell }= lim _{q rightarrow 0} operatorname {P} (X_{2} leq F_{2}^{ leftarrow }(q) mid X_{1} leq F_{1}^{ leftarrow }(q)).}$$ where ${ displaystyle F^{ leftarrow }(q)= inf {x in mathbb {R} :F(x) geq q }} { displaystyle F^{ leftarrow }(q)= inf {x in mathbb {R} :F(x) geq q }}$, that is, the inverse of the cumulative probability distribution function for $q$. . The upper tail dependence is defined analogously as . $${ displaystyle lambda _{u}= lim _{q rightarrow 1} operatorname {P} (X_{2}&gt;F_{2}^{ leftarrow }(q) mid X_{1}&gt;F_{1}^{ leftarrow }(q)).}$$ . Gaussian copulas are easy to understand but lack the flexibility to model tail dependence and radial asymmetry, which is very important for risk management. For that we need to consider the class of Archimedean copulas ${ displaystyle C(u_{1}, dots ,u_{d}; theta )= psi ^{[-1]} left( psi (u_{1}; theta )+ cdots + psi (u_{d}; theta ); theta right)}$ like the Gumbel copula or the Clayton Copula. . from numpy.random import gamma def clayton_copula(n, delta, d=2): x = gamma(1/delta,1, size=n) v = stats.uniform().rvs([n,d]) u = (1-np.log(v)/x[:,None])**(-1/delta) return u n = 10000 delta = 6 u = clayton_copula(n, delta) h = sns.jointplot(u[:,0], u[:,1], kind=&#39;hex&#39;, stat_func=None) h.set_axis_labels(&#39;Y1&#39;, &#39;Y2&#39;); . x1_transform = stats.genpareto(c=0.25).ppf(u[:,0]) x2_transform = stats.beta(a=10, b=2).ppf(u[:,1]) sns.jointplot(x1_transform, x2_transform, kind=&#39;kde&#39;, xlim=(0, 5), ylim=(.5, 1.0), stat_func=None); . x = stats.uniform().rvs([n,3]) . kt = scipy.stats.kendalltau(x1_transform,x2_transform) print(&quot;Kendall&#39;s Tau: &quot;, kt) . Kendall&#39;s Tau: KendalltauResult(correlation=0.7569578157815782, pvalue=0.0) . sr = scipy.stats.spearmanr(x1_transform,x2_transform) print(&quot;Spearman&#39;s Rho: &quot;, sr) . Spearman&#39;s Rho: SpearmanrResult(correlation=0.9149789224377892, pvalue=0.0) . pc = scipy.stats.pearsonr(x1_transform,x2_transform) print(&#39;Pearson Corr: &#39;, pc) . Pearson Corr: (0.5324997734331705, 0.0) . lamda_l = 2**(-1/delta) print(&#39;Tail Dependence Coefficient: &#39;,lamda_l) . Tail Dependence Coefficient: 0.8908987181403393 .",
            "url": "https://jpwoeltjen.github.io/researchBlog/extreme%20value%20theory/risk/copulas/2019/03/25/QuantitativeRiskManagementNotes.html",
            "relUrl": "/extreme%20value%20theory/risk/copulas/2019/03/25/QuantitativeRiskManagementNotes.html",
            "date": " • Mar 25, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "Computational Finance",
            "content": "Consider a market containing two primary assets: a risk-free bond with price $B_t=e^{rt}$, a stock $S_t$ of geometric Brownian motion type $S(t)$ = $S(0) exp( mu t + sigma W(t))$, with parameters $ mu in mathbb{R}, sigma &gt; 0 $ and standard Brownian motion $W$. . A derivative or option is an asset whose payoff depends on the underlying. The payoff $X$ at time $T$ may be a function $f(S(T))$ of the underlying at time $T$ as e.g. $X = (S(T)-K)^{+}$ for a simple European call or it could be a more complex function of the whole past of $S$. In complete markets such options can be replicated perfectly. This means that there exists a self-financing portfolio $ phi = ( phi_0, phi_1)$ whose value at time $T$ equals $V_ phi(T) = X$. Absence of arbitrage implies that $V_ phi(t)$ is the only reasonable option price of the option at time $t$. It can be computed as conditional expectation . $$V_ phi(t) = B(t) mathbb{E}^ mathbb{Q} big[X/B(T) big| mathcal{F}_t big]$$ . of the discounted payoff under the unique equivalent martingale measure $ mathbb{Q}$, i.e. the unique probability measure $ mathbb{Q} sim mathbb{P}$ such that $S/B$ is a $ mathbb{Q}$-martingale. . American options are specified by an entire exercise process $X = (X(t))_{t in[0,T]}$ as e.g. $X(t) = (K-S(t))^{+}$ for an American put. In the complete case, the only reasonable price is the B-fold of the Snell envelope of $X/B$ relative to $ mathbb{Q}$. The $ mathbb{Q}$-Snell envelope is the smallest $ mathbb{Q}$-supermartingale dominating $X/B$. Again, $ mathbb{Q}$ denotes the unique equivalent martingale measure from above. One can write this fair price also as . $$V(t) = sup_{ tau} mathbb{E}^ mathbb{Q} big[X( tau)/B( tau) big| mathcal{F}_t big]$$ . where the supremum extends over all stopping times stopping between $t$ and $T$. One such stopping time is the first time $t$ such that $V (t) = X(t)$, i.e. the market price of the option equals the exercise price. . A Martingale is a sequence of random variables (i.e., a stochastic process) for which, at a particular time in the realized sequence, the expectation of the next value in the sequence is equal to the present observed value even given knowledge of all prior observed values. Supermartingale: $ mathbb{E}$ equal or less than current value. | The Snell envelope is the smallest supermartingale dominating a stochastic process. | Supremum: the smallest quantity that is greater than or equal to each of a given set or subset of quantities. | $ mathcal{F}_t$ is the sigma-field at time $t$, i.e. all the available information at time $t $ | . Binomial Trees . In the Cox-Ross-Rubinstein model (CRR) we consider equidistant times $0 = t_0,t_1,...,t_M = T$ with $t_i = i Delta t$ and hence $ Delta t = T/M.$ The bond moves according to $B(t_i) = e^{rt_i}$ . The stock goes up by a factor $u$ resp. down by a factor $d$ in each period, i.e., . $$ S(t_i) = begin{cases} S(t_{i-1})u&amp; text{with probability } p S(t_{i-1})d&amp; text{with probability } p-1 end{cases} $$, where $d &lt; e^{r Delta t} &lt; u$. More precisely, $p$ and $1-p$ denote the conditional probabilities of going up and down, repectively, given the past. Option prices in a properly chosen sequence of CRR models converge to the corresponding Black-Scholes notions if the number of time step tends to infinity. . For option pricing real-world transition probabilities do not matter. Instead we need to consider martingale probabilities, i.e. probabilities such that . $$ mathbb{E}^ mathbb{Q} big[S(t_i)/B(t_i) big| mathcal{F}_{t_{i-1}} big] = S(t_{i-1})/B(t_{i-1})$$. . If we denote the $Q$-transition probabilities by $q$ and $1-q$, respectively, the left-hand side equals $q frac{S(t_{i-1})u}{B(t_{i-1})e^{r Delta t}} + (1-q) frac{ S(t_{i-1})d}{B(t_{i-1})e^{r Delta t}} $ which equals the right-hand side iff . $$ frac{qu+(1-q)d}{e^{r Delta t}} = 1 $$ . or . $$ q = frac{e^{r Delta t}-d}{u-d}$$ . As previously mentioned a properly parameterized CRR model approximates BS as $M$ gets large. If we match the first two moments of the CRR and BS model we obtain . $$ begin{align} u = beta + sqrt{ beta^2 - 1 } d = beta - sqrt{ beta^2 - 1 } q = frac{ exp(r Delta t ) - d}{u-d} beta = frac{1}{2}( exp{(-r Delta t )} + exp{((r + sigma^2) Delta t)}) end{align}$$ European Options . Consider a European option with payoff $g(S(T))$ for some function $g$ as e.g. $g(x) = (x-K)^{+}$ (call) or $g(x) = (K-x)^{+}$ (put). We denote the fair price of the option at time $t_i$ by $V(S(t_i), t_i)$. The $Q$-martingale property for discounted option prices reads as . $$ mathbb{E}^ mathbb{Q} big[V (S(t_i), t_i)/B(t_i) big| mathcal{F}_{t_{i-1}} big] = V (S(t_{i-1}), t_{i-1})/B(t_{i-1})$$ . or . $$q frac{ V(S(t_{i-1})u,t_i)}{B(t_{i-1})e^{r Delta t}} + (1-q) frac{ V(S(t_{i-1})d,t_i)}{B(t_{i-1})e^{r Delta t}} = frac{ V(S(t_{i-1}),t_i)}{B(t_{i-1})}$$ . or . $$V (S(t_{i-1}), t_{i-1}) = e^{-r Delta t} (qV(S(t_{i-1})u, t_i) + (1-q)V(S(t_{i-1})d, t_i))$$ . We can compute option prices starting from $V (S(T ), T ) = g(S(T ))$ and moving backwards in time. . Since the model is complete, the payoff $g(S(T))$ can be replicated perfectly by a self- financing portfolio $ phi = ( phi_0, phi_1)$. Its value equals $ phi_0(t_i)B(t_i) + phi_1(t_i)S(t_i) = V (S(t_i), t_i)$. . Due to self-financeability . $$ phi_0(t_{i-1})B(t_{i-1}) + phi_1(t_{i-1})S(t_{i-1}) = phi_0(t_i)B(t_{i-1}) + phi_1(t_i)S(t_{i-1})$$ . leads to . $$ phi_0(t_i)B(t_{i-1}) + phi_1(t_i)S(t_{i-1}) = V (S(t_{i-1}), t_{i-1})$$ . $$ phi_0(t_i)B(t_{i-1}) + phi_1(t_i)S(t_{i-1}u) = V (S(t_{i-1}u), t_{i-1})$$ . $$ phi_0(t_i)B(t_{i-1}) + phi_1(t_i)S(t_{i-1}d) = V (S(t_{i-1}d), t_{i-1})$$ . Hence, . $$ phi_1(t_i) = frac{V (S(t_{i-1})u, t_i) - V (S(t_{i-1})d, t_i)}{ S(t_{i-1})(u-d)}$$ . $$ phi_0(t_i) = frac{V (S(t_{i-1}),t_{i-1})- phi_1(t_i)S(t_{i-1})}{B(t_{i-1})}$$ . American Options . American options have exercise process $g(S(t_i))$ at time $t_i$. . $$V(S(t_{i-1}), t_{i-1}) = max{ {g(S(t_{i-1})), mathbb{E}^ mathbb{Q} big[V (S(t_i), t_i)/B(t_i) big| mathcal{F}_{t_{i-1}} big] }}$$ . This leads to . $$V (S(t_{i-1}), t_{i-1}) = max{ {g(S(t_{i-1})),e^{-r Delta t} (qV(S(t_{i-1})u, t_i) + (1-q)V(S(t_{i-1})d, t_i)) }}$$ . The first optimal stopping time is . $$ tau_f = inf{ {t_i : V (S(t_i), t_i) = g(S(t_i)) }}$$ . and the last optimal stopping time is . $$ tau_s = inf{ {t_i : i = M text{ or } g(S(t_i)) &gt; e^{-r Delta t} (qV (S(t_i)u, t_{i+1}) + (1-q)V (S(t_i)d, t_{i+1})) }}$$ . import numpy as np def crr_bs_approx(S_0,r,sigma,T,M,K,EU,Type): dt=T/M #Set u,d,q such that first and second moments match BS beta = 0.5*(np.exp(-r*dt)+np.exp((r+(sigma**2))*dt)) u = beta + ((beta**2)-1)**0.5 d = 1/u#beta - ((beta**2)-1)**0.5 q = (np.exp(r*dt)-d)/(u-d) if Type == &#39;call&#39;: def g(St,K): return max(St - K, 0) elif Type == &#39;put&#39;: def g(St,K): return max(K - St, 0) else: raise Exception(&quot;Specify valid Type (&#39;put&#39;/&#39;call&#39;)&quot;) g = np.vectorize(g ,otypes=[np.float]) S = np.zeros((M+1,M+1)) S[0,0] = S_0 V = np.zeros((M+1,M+1)) for i in range(1,M+1): for j in range(i+1): S[j,i] = S_0*(u**j)*(d**(i-j)) V[:,-1] = g(S[:,-1],K) if EU == 1: for i in range(M-1,-1,-1): for j in range(i+1): V[j,i] = np.exp(-r*dt)*( q*V[j+1,i+1] + (1-q)*V[j,i+1]) elif EU == 0: for i in range(M-1,-1,-1): for j in range(i+1): V[j,i] = max(g(S[j,i],K) ,np.exp(-r*dt)*( q*V[j+1,i+1] + (1-q)*V[j,i+1])) else: raise Exception(&quot;Specify valid EU state (0/1)&quot;) return V[0,0] sigma=0.3 r=0.03 S_0=100 T=1 M=2000 K=100 EU=1 Type = &#39;call&#39; V = crr_bs_approx(S_0,r,sigma,T,M,K,EU,Type) print(f&quot;Fair value of option: {round(V,3)}&quot;) . Fair value of option: 13.282 . To check whether the approximation is any good compute BS call and compare: . from scipy.stats import norm import numpy as np def eu_call_bs(S_t,r,sigma,T,K,t): d_1 = (np.log(S_t/K)+(r+sigma**2/2)*(T-t))/(sigma*(T-t)**0.5) d_2 = d_1 - sigma*(T-t)**0.5 phi_1 = norm.cdf(d_1) cdf_d2 = norm.cdf(d_2) c = S_t * phi_1 - K*np.exp(-r*(T-t)) *cdf_d2 phi_0 = -K*np.exp(-r*T)*cdf_d2 return c, phi_0, phi_1 sigma=0.3 r=0.03 S_t=100 T=1 K=100 t = 0 c, phi_0, phi_1 = eu_call_bs(S_t,r,sigma,T,K,t) print(f&quot;The fair value of the option is {round(c,3)}, the hedging position in the the stock is {round(phi_1,3)}, the hedging position in the the bond is {round(phi_0,3)}.&quot;) . The fair value of the option is 13.283, the hedging position in the the stock is 0.599, the hedging position in the the bond is -46.587. . Monte Carlo . Once we are considering very complicated payoff structures many numerical methods relying on e.g. solving PDE’s or using integral transforms are not available anymore. In this case we can employ Monte Carlo methods, basically simulating many possible futures and taking the sample moments as an approximation of the real world population moments. For this we need a random number generator that can sample from the appropriate probability distribution. The needed law might not be implemented in the software package so we may need to implement a random number generator that takes only uniformly distributed random numbers as input. This can be done via . Inversion | Acceptance/rejection method | . import numpy as np import scipy.interpolate as interpolate from matplotlib import pyplot as plt def inverse_transform_sampling(data, n_bins=100, n_samples=1000): hist, bin_edges = np.histogram(data, bins=n_bins, density=True) cum_values = np.zeros(bin_edges.shape) cum_values[1:] = np.cumsum(hist*np.diff(bin_edges)) inv_cdf = interpolate.interp1d(cum_values, bin_edges) r = np.random.rand(n_samples) return inv_cdf(r) st_normal = np.random.normal(0,1,10000) mm_normal = np.append(st_normal, (np.random.normal(5,1,1000), np.random.normal(-10,1,1000))) c = inverse_transform_sampling(mm_normal,n_bins=100, n_samples=10000) plt.hist(mm_normal, alpha=0.5, bins=200, normed=True, color=&#39;b&#39;) plt.hist(c, alpha=0.5, bins=200, normed=True, color=&#39;r&#39;) plt.show() . import numpy as np from matplotlib import pyplot as plt from scipy.special import beta as beta # Density of the Beta distribution def Beta_density(x, alpha1, alpha2): return 1/beta(alpha1,alpha2) * (x)**(alpha1-1) * (1-x)**(alpha2-1) * (x&gt;=0) * (x&lt;=1) def Sample_Beta_AR(alpha1, alpha2, N): # Mode of the beta density x_max = (alpha1-1) / (alpha1+alpha2-2) # Constant C for the acceptance/rejection method. C is the optimal height of the rectangle in which # we through our imaginary darts. C = Beta_density(x_max, alpha1, alpha2) # Generate one sample by the acceptance/rejection method def SingleSample(): success = False while not success: U = np.random.rand(2,1) success = ( C*U[1] &lt;= Beta_density(U[0], alpha1, alpha2) ) return U[0] X = np.zeros(N) for n in range(N): X[n] = SingleSample() return X alpha1 = 2 alpha2 = 3 N = 200000 b_ar = Sample_Beta_AR (alpha1, alpha2, N) plt.hist(b_ar,alpha=0.5, bins=500, normed=False, color=&#39;b&#39;) # Compare with numpy&#39;s version of beta distribution b = np.random.beta(alpha1, alpha2, N) plt.hist(b,alpha=0.5, bins=500, normed=False, color=&#39;r&#39;) plt.show() . import numpy as np def mc_eu(S_0,K,r,sigma,T,N,g): def f(x): return np.exp(-r*T)*g(S_0*np.exp( (r-0.5*sigma**2)*T + (sigma*T**0.5*x) ), K) X = np.random.normal(0,1,N) Y = f(X) v_0 = np.mean(Y) epsilon = 1.96 * np.sqrt(np.var(Y)/N) return v_0, epsilon sigma=0.3 r=0.03 S_0=100 T=1 N=10000 K=100 g = lambda S, K: np.maximum((S - K),0) V,e = mc_eu(S_0,K,r,sigma,T,N,g) print(f&quot;Fair value of option: {round(V,3)}, n95% confidence interval: [ {round(V - e,3)} , {round(V+e,3)} ]&quot;) . Fair value of option: 13.462, 95% confidence interval: [ 13.036 , 13.889 ] . Variance reduction . The Monte Carlo estimator converges at the rate $ frac{1}{ sqrt{ N }}$ since its standard deviation is $ sigma( hat{V_N}) = frac{ sigma(f(X))}{ sqrt{N}}$. Since we don&#39;t have unlimited compute power this rate of convergence might be to slow for our purposes. We may can do better by trying to reduce $ sigma(f(X))$ through some tricks, e.g., by means of antithetic variables or control variates. Another problem arises if there are outcomes with very low probability that have a large effect on the option value, i.e., Black Swans. In this case, we may not get any samples from the part of the distribution that matter most and our estimate will be way off. To correct for that use importance sampling. This can be useful e.g. in pricing deep out of the money puts: . from scipy.stats import norm import numpy as np # Importance Sampling def mc_eu_is(S_0, K, r, sigma, T, N, g, mu): def f(x): return np.exp(-r*T-x*mu+0.5*mu**2)*g(S_0*np.exp( (r-0.5*sigma**2)*T + (sigma*T**0.5*x) ), K) X = np.random.normal(mu,1,N) Y = f(X) v_0 = np.mean(Y) # Compute radius of 95% confidence interval. epsilon = 1.96 * np.sqrt(np.var(Y)/N) return v_0, epsilon #Antithetic Variables def mc_eu_av (S_0, K, r, sigma, T, N, g): def f(x): return np.exp(-r*T) * g(S_0*np.exp((r-0.5*sigma**2)*T + sigma*np.sqrt(T)*x), K) X = np.random.normal(0,1,N) # Compute Monte-Carlo estimator using antithetic variables taking advantage of the symmetry of the gaussian. # Antithetic variables only increase the bang for the buck if random vars are negatively correlated. # Since X2 = -X1 this is clearly the case here. Y = (f(X)+f(-X)) / 2 v_0 = np.mean(Y) # Compute radius of 95% confidence interval. epsilon = 1.96 * np.sqrt(np.var(Y)/N) return v_0, epsilon #Closed Form BS def eu_put_bs(S_t, r, sigma, T, K, t): d_1 = (np.log(S_t/K)+(r+sigma**2/2)*(T-t))/(sigma*(T-t)**0.5) d_2 = d_1 - sigma*(T-t)**0.5 p = K*np.exp(-r*(T-t)) *norm.cdf(-d_2) - S_t *norm.cdf(-d_1) return p sigma=0.3 r=0.03 S_0=S_t=200 T=1 N=10000 K=100 g = lambda S, K: np.maximum((K-S),0) t = 0 mu = ( np.log(K/S_0) - (r-1/2*sigma**2)*T ) / ( sigma*np.sqrt(T) ) V,e = mc_eu_is(S_0,K,r,sigma,T,N,g,mu) print(f&quot;Fair value of OOM put with importance sampling: {round(V,3)}, 95% confidence interval: [ {round(V - e,3)} , {round(V+e,3)} ]&quot;) V,e = mc_eu_av(S_0,K,r,sigma,T,N,g) print(f&quot;Fair value of OOM put with antithetic variables: {round(V,3)}, 95% confidence interval: [ {round(V - e,3)} , {round(V+e,3)} ]&quot;) V,e = mc_eu(S_0,K,r,sigma,T,N,g) print(f&quot;Fair value of OOM put standard MC : {round(V,3)}, 95% confidence interval: [ {round(V - e,3)} , {round(V+e,3)} ]&quot;) V = eu_put_bs(S_t,r,sigma,T,K,t) print(f&quot;Fair value of OOM put BS: {round(V,3)}&quot;) . Fair value of OOM put with importance sampling: 0.109, 95% confidence interval: [ 0.106 , 0.111 ] Fair value of OOM put with antithetic variables: 0.109, 95% confidence interval: [ 0.09 , 0.127 ] Fair value of OOM put standard MC : 0.126, 95% confidence interval: [ 0.098 , 0.155 ] Fair value of OOM put BS: 0.109 . As can be seen above, the importance sampling technique estimates a much tighter confidence interval for the OOM put. The antithetic variables approach reduces the variance a bit. . Using control variables to reduce the variance of MC-estimators . Let&#39;s price a self-quanto call in the Black-Scholes model via the Monte-Carlo approach while reducing the variance with a European call as control variable. . from scipy.stats import norm import numpy as np def eu_call_bs(S_t,r,sigma,T,K,t): d_1 = (np.log(S_t/K)+(r+sigma**2/2)*(T-t))/(sigma*(T-t)**0.5) d_2 = d_1 - sigma*(T-t)**0.5 phi_1 = norm.cdf(d_1) cdf_d2 = norm.cdf(d_2) c = S_t * phi_1 - K*np.exp(-r*(T-t)) *cdf_d2 phi_0 = -K*np.exp(-r*T)*cdf_d2 return c, phi_0, phi_1 # function for computing the price of a self quanto call in the BS-model using Monte-Carlo with control variate (ST-K)^+ def EuOption_BS_MC_CV (S0, r, sigma, T, K, M): # Determine beta = Cov(ST*(ST-K)^+,(ST-K)^+) / Var((ST-K)^+) by Monte Carlo simulation. X = np.random.normal(0,1,M) ST = S0*np.exp( (r-0.5*sigma**2)*T + sigma*np.sqrt(T)*X ) VT = ST*np.maximum((ST-K),0) CT = np.maximum((ST-K),0) C0,_,_ = eu_call_bs(S0,r,sigma,T,K,t) #initial price of the call used as control variate,also used here to # help determine the empirical covariance instead of Covar = np.mean( (CT-np.mean(CT)) * (VT-np.mean(VT)) ) Covar = np.mean( (CT-C0*np.exp(r*T)) * (VT-np.mean(VT)) ) beta = Covar / np.var(CT) # Compute Monte Carlo estimator using the initial Call option price as control variate. X = np.random.normal(0,1,M) ST_hat = S0*np.exp( (r-0.5*sigma**2)*T + sigma*np.sqrt(T)*X ) Y = ST_hat*np.maximum((ST_hat-K),0)-beta*np.maximum((ST_hat-K),0) V0 = np.exp(-r*T)*np.mean(Y) + beta*C0 # Compute radius of 95% confidence interval epsilon = 1.96 * np.sqrt(np.var(Y)/M) return V0, epsilon, beta def EuOption_BS_MC (S0, r, sigma, T, K, M): X = np.random.normal(0,1,M) ST_hat = S0*np.exp( (r-0.5*sigma**2)*T + sigma*np.sqrt(T)*X ) Y = ST_hat*np.maximum((ST_hat-K),0) V0 = np.exp(-r*T)*np.mean(Y) # Compute radius of 95% confidence interval epsilon = 1.96 * np.sqrt(np.var(Y)/M) return V0, epsilon sigma=0.3 r=0.03 S0=100 T=1 K=100 t = 0 M=100000 V0, epsilon, beta = EuOption_BS_MC_CV (S0, r, sigma, T, K, M) print(f&quot;Control Variate n Option Price: {V0} n Epsilon: {epsilon} n Beta: {beta}&quot;) print(&quot; n&quot;) V0, epsilon= EuOption_BS_MC (S0, r, sigma, T, K, M) print(f&quot;Standard MC n Option Price: {V0} n Epsilon: {epsilon}&quot;) . Control Variate Option Price: 1999.7236047923905 Epsilon: 5.604362549148449 Beta: 179.43568394156813 Standard MC Option Price: 2015.4528087679719 Epsilon: 25.915084371493748 . Finite difference method . In addition to the fair value of the option we need to know how to hedge it. Hedging strategies often involve the derivative of the option with respect to variables such as the stock price. The finite difference approach gives us an approximation of the derivative. This works by simply nudging the variable $+h/2$ and $-h/2$ and observing how the function changes relative to $h$. More formally, for a sufficiently smooth function $z$ we have the Taylor expansion . $$ z( theta + h/2) = z( theta) + z&#39; ( theta) h/2 + z&#39;&#39; ( theta) h^2/8 + z&#39;&#39;&#39; ( theta) h^3/48 + O(h^4)$$ . and . $$ z( theta - h/2) = z( theta) - z&#39; ( theta) h/2 + z&#39;&#39; ( theta) h^2/8 - z&#39;&#39;&#39; ( theta) h^3/48 + O(h^4)$$ . where $O(h^4)$ stands for an expression such that $O(h^4)/h^4$ is bounded in a neighbourhood of $h = 0$. . This yields . $$z&#39;( theta) = frac{z( theta+h/2)-z( theta-h/2)}{h} + O(h^2)$$ . $$ = mathbb{E} big[ frac{Z( theta+h/2)-Z( theta-h/2)}{h} big] + O(h^2)$$ . The quantities $ mathbb{E} big[ Z( theta + h/2) big]$ and $ mathbb{E} big[ Z( theta - h/2) big]$ can be computed by Monte Carlo simulation as usual. Choosing a small $h$ reduces the bias due to the $O(h^2)$ term. . import numpy as np def mc_eu(S_0,K,r,sigma,T,N,g): x = np.random.normal(0,1,N) y = np.exp(-r*T)*g((S_0)*np.exp( (r-0.5*sigma**2)*T + (sigma*T**0.5*x) ), K) a = np.exp(-r*T)*g((S_0+h/2)*np.exp( (r-0.5*sigma**2)*T + (sigma*T**0.5*x) ), K) b = np.exp(-r*T)*g((S_0-h/2)*np.exp( (r-0.5*sigma**2)*T + (sigma*T**0.5*x) ), K) d = (a-b)/h delta = np.mean(d) V_0 = np.mean(y) return V_0, delta sigma=0.3 r=0.03 S_0=100 T=1 N=10**7 K=100 g = lambda S, K: np.maximum((S - K),0) h = 1 V_0, delta = mc_eu(S_0,K,r,sigma,T,N,g) print(f&quot;Option Value: {round(V_0,3)} n Hedging Position Stock {round(delta,3)} n Hedging Position Bond {round((V_0-delta*S_0),3)}&quot;) . Option Value: 13.285 Hedging Position Stock 0.599 Hedging Position Bond -46.576 . import numpy as np from scipy.misc import derivative def EuCallHedge_BS_MC_IP (St, r, sigma, g, T, t, N, K): X = np.random.normal(0,1,N) g_del = np.zeros(X.shape) # Compute derivative of g(St*exp( (r-0.5*sigma^2)*(T-t) + sigma*sqrt(T-t)*X)) w.r.t to St # This equeals g&#39;(St*exp( (r-0.5*sigma^2)*(T-t) + sigma*sqrt(T-t)*X ))* exp( (r-0.5*sigma^2)*(T-t) + # sigma*sqrt(T-t)*X by chain rule) for i in range(N): g_del[i] = derivative(g, St*np.exp( (r-0.5*sigma**2)*(T-t) + sigma*np.sqrt(T-t)*X[i]),args=(K,) ) Y = g_del * np.exp( -0.5*sigma**2*(T-t) + sigma*np.sqrt(T-t)*X) delta = np.mean(Y) # Compute 95% confidence interval epsilon = 1.96 * np.sqrt(np.var(Y)/N) c1=delta-epsilon c2=delta+epsilon return delta, c1, c2 sigma=0.3 r=0.03 S_t=100 t=0 T=1 N=10**5 K=100 g = lambda S, K: np.maximum((S - K),0) delta, c1, c2 = EuCallHedge_BS_MC_IP (St, r, sigma, g, T, t, N, K) print(f&quot;Delta via Infinitesimal Perturbation n Delta: {delta} n 95% Confidence Interval: [{c1},{c2}]&quot;) . Delta via Infinitesimal Perturbation Delta: 0.5999187977035505 95% Confidence Interval: [0.5959336800017455,0.6039039154053555] . Simulation of stochastic integrals . In some applications we need to simulate the whole path $(X (t))_{t in[0,T ]}$ of a stochastic process rather than just its terminal value $X(T)$. This happens e.g. if we want to compute prices of path-dependent claims as e.g. lookback, barrier, or Asian options. But even if we are interested only in$ X (T )$, it may be necessary to simulate the past as well because we do not know the law of $ X(T)$ in closed form. In this section we focus on diffusion-type processes, i.e., solution to stochastic differential equations of the form $$dX (t) = a(X (t), t)dt + b(X (t), t)dW (t)$$ . with some Wiener process $W$, some starting value $X(0)$, and given deterministic functions $a$, $b$. It is obviously impossible to simulate infinitely many numbers on a real computer. The goal is therefore to generate random paths of $X$ on some equidistant time grid $0 = t_0 &lt; t_1 &lt; ··· &lt; t_m = T$ with $t_i = i Delta t$ and $ Delta t = T/m$, i.e., $X(t_0),...,X(t_m)$. . Euler method . If $a$ and $b$ are sufficiently smooth and the grid is sufficiently dense $$X(t_i) approx X(t_{i-1})+ int_{t_{i-1}}^{t_i} a(X(t_{i-1}),t_{i-1})dt + int_{t_{i-1}}^{t_i} b(X(t_{i-1}),t_{i-1})dW(t)$$ . $$= X(t_{i-1}) + a(X(t_{i-1}), t_{i-1}) Delta t + b(X(t_{i-1}), t_{i-1}) Delta W_i$$ . with $W_i = W_{t_{i}} - W_{t_{i-1}}$ is a good approximation for . $$X(t_i) = X(t_{i-1})+ int_{t_{i-1}}^{t_i} a(X(t),t)dt + int_{t_{i-1}}^{t_i} b(X(t),t)dW(t)$$ . Let&#39;s simulate geometric Brownian motion where $a(X(t),t) = mu * X(t) $ and $b(X(t),t) = sigma * X(t) $ . import numpy as np from matplotlib import pyplot as plt import seaborn def geom_brownian_motion(m,T,a,b,X_0): dt = T/m r = np.random.normal(0,1,m) dW = r * np.sqrt(dt) X = np.zeros(m) t = np.zeros(m) t[0] = 0 X[0] = X_0 for i in range(1,m): t[i] = t[i-1]+dt X[i] = X[i-1] + a(X[i-1],t[i-1])*dt + b(X[i-1],t[i-1])*dW[i] return X, t m=1000 T=1 mu = 0.1 sigma = 0.3 a = lambda X, t: mu*X b = lambda X, t:sigma*X X_0 = 1 # Let&#39;s simulate n alternative future histories. n = 100 X = np.zeros((n,m)) for i in range(n): X[i],t = geom_brownian_motion(m,T,a,b,X_0) plt.plot(t,X[i],alpha=0.5) plt.title(&quot;Simulated geometric Brownian motion paths&quot;) plt.xlabel(&quot;time&quot;) plt.ylabel(&quot;Price&quot;) plt.show() X_mean = np.mean(X[:,-1]) print(f&quot;The mean stock price at time T is {X_mean}&quot;) . The mean stock price at time T is 1.1030173035417743 . And let us show by simulation that this is equivalent to $X(t) = X(0) exp(( mu - sigma^2/2)t + sigma W (t))$ with $t = T$ . . X = np.zeros(n) r = np.random.normal(0,1,m) X = X_0 *np.exp((mu-sigma**2/2)*T + sigma*r*T**0.5) X_mean = np.mean(X) print(f&quot;The mean stock price at time T is {X_mean}&quot;) . The mean stock price at time T is 1.1003810409996253 . MC Heston model . The Heston model extends the Black-Scholes model in that the variance of returns now is variable. To use MC methods we have to simulate the whole sample paths. Let&#39;s first simulate paths using the Euler method and then use straight forward MC methods to prive the option. . import numpy as np def SimPaths_Ito_Euler(X0 ,a ,b ,T ,m, N): Delta_t = T/(m-1) Delta_W = np.random.normal(0, np.sqrt(Delta_t),(N, m)) X = np.zeros(Delta_W.shape) X[:,0] = X0 * np.ones(N) for i in range(m-1): X[:,i+1]=X[:,i]+a(X[:,i],i*Delta_t)*Delta_t+b(X[:,i],i*Delta_t)*Delta_W[:,i] return X def Heston_EuOption_MC(S0, r, gamma, T, g, K): N, m = gamma.shape[0], gamma.shape[1] Delta_t = T/(m-1) X = np.random.normal(0,1,N) drift = np.dot((np.ones((N, m))*r - gamma * 0.5) , np.ones(m)) * Delta_t std = np.sqrt( np.dot(gamma , np.ones(m)) * Delta_t) ST = S0*np.exp(drift + std * X) V0 = np.exp(-r*T) * np.mean(g(ST,K)) return V0 K = 100 N=100000 m=100 S0 = 100 r = 0.03 nu0 = 0.3**2 kappa = 0.3**2 Lambda = 2.5 sigma_tilde = 0.2 T = 1 g = lambda S, K: np.maximum((S - K),0) # Function for the drift of the variance process in the heston model a = lambda x, t: kappa-Lambda*x # Function for the standard deviation of the variance process in the heston model b = lambda x, t: np.sqrt(x)*sigma_tilde gamma = SimPaths_Ito_Euler(nu0 ,a ,b ,T ,m, N) V0 = Heston_EuOption_MC(S0, r, gamma, T, g, K) print(f&quot;Option Value: {V0}&quot;) . Option Value: 10.851623171194033 . import numpy as np def UpOutPut_BS_MC_Richardson (S0, r, sigma, T, K, B, M, m): # Time step on the fine grid. delta_t = T/(2*m) S_fine = S0 * np.ones(M) S_coarse = S0 * np.ones(M) no_barrier_hit_fine = np.ones(M) no_barrier_hit_coarse = np.ones(M) # Loop over points on coarse grid. for k in range(m): # Simulate two increments of Brownian motion on the fine grid. delta_W_1 = np.random.normal(0,np.sqrt(delta_t),M) delta_W_2 = np.random.normal(0,np.sqrt(delta_t),M) # 1st Euler step on fine grid. S_fine = S_fine + r*S_fine*delta_t + sigma*S_fine*delta_W_1 no_barrier_hit_fine = no_barrier_hit_fine * (S_fine&lt;B) # 2nd Euler step on fine grid. S_fine = S_fine + r*S_fine*delta_t + sigma*S_fine*delta_W_2 no_barrier_hit_fine = no_barrier_hit_fine * (S_fine&lt;B) # Euler step on coarse grid. S_coarse = S_coarse + r*S_coarse*2*delta_t + sigma*S_coarse*(delta_W_1+delta_W_2) no_barrier_hit_coarse = no_barrier_hit_coarse * (S_coarse&lt;B) # Compute (discounted) payoffs for paths on fine and coarse grids. VT_fine = no_barrier_hit_fine * (np.exp(-r*T)*np.maximum( K - S_fine , 0)) VT_coarse = no_barrier_hit_coarse * (np.exp(-r*T)*np.maximum( K - S_coarse , 0)) # Compute Monte Carlo estimate. VT = 2*VT_fine - VT_coarse V0 = np.mean(VT) # Compute radius of 95% confidence interval epsilon = 1.96 * np.sqrt(np.var(VT)/M) return V0, epsilon S0 = 100 r = 0.05 sigma = 0.2 T = 1 K = 100 B = 110 M = 100000 m = 250 V0, epsilon = UpOutPut_BS_MC_Richardson (S0, r, sigma, T, K, B, M, m) print(f&quot;Option Value: {V0} n Radius of 95% Confidence Interval: {epsilon}&quot;) . Option Value: 4.28812361180373 Radius of 95% Confidence Interval: 0.05227428329961295 . Ito calculus . If we move from dicrete time to continuous time, we need a proper framework for working with continuous stochastic processes. We are working in a probability space ($ Omega$, $ mathcal{F}$ , $P$ ) consisting of the set $ Omega$ of possible outcomes, a $ sigma$-field $ mathcal{F}$ on $ Omega$ and a probability measure $P$ on $ mathcal{F}$. Standard calculus works for functions with finite variation, continuous stochastic functions, however, can have infinite variation, i.e., they can&#39;t be approximated by a smooth function no matter how local one gets. Many adapted stochastic proccesses can be decomposed into $X = X(0)+M+A$ with a martingale $M$ and a predictable process of finite variation $A$ according to Doop-Meyer. Our primary need from stochastic calculus is the stochastic intragal which represents financial gains. If $H$ is piecewise constant $H(t) = sum_{i=1}^{n}{V_i mathcal{1}_{[t_{i-1},t_i)}(t)}$ then the integral $$H bullet X := int H(t)dX(t)$$ is defined as . $$H bullet X(t):= int_{0}^{t} H(s)dX(s):= sum_{i=1}^n V_{i}(X(t_i)-X(t_{i-1}))$$ If $H(t$) stands for the number of shares in the portfolio at time $t$, $d$ for the assets, and $X(t)$ denotes the stock price at time $t$, then simple bookkeeping yields that $ int_0^t H(s)dX(s)$ stands for the financial gains of the portfolio from time 0 to $t$. . This can also be written as . $$dY (s) = H(s)dX(s)$$ . The integration by parts rule $$d(X(t)Y (t)) = X(t-)dY (t) + Y (t-)dX(t) + d[X, Y ](t)$$ turns out to be useful, where $[X, Y ](t)$ denotes the covariation process, which is the limit of expressions . $$ sum_{k=1}^n big(X( frac{k}{n} t)-X( frac{k-1}{n} t) big) big(Y( frac{k}{n} t)-(Y( frac{k-1}{n} t) big)$$ . if we let $n$ tend to infinity. In other words, $[X, Y ]$ sums up products of increments of $ X$ and $ Y$ over infinitesimal time intervals. For $X = Y$ it is called quadratic variation. This can also be written as $dX(t)dY (t) = d[X, Y ](t)$. It vanishes if both either $X$ or $Y$ is continuous and either of them is of finite variation. Moreover, $[X, Y ]$ is itself of finite variation. . The most import formula from stochastic calculus is Ito&#39;s formula $$f(X(t)) =f(X(0))+ int_0^t f&#39;(X(s))dX(s)+ frac{1}{2} int_0^t f&#39;&#39;(X(s))d[X,X](s)$$ . or in differential notation . $$df(X(t)) = f&#39;(X(t))dX(t) + frac{1}{2} f&#39;&#39;(X)d[X, X](t)$$ . One reoccuring proccess that involves the yet unknown proccess on both sides is the stochastic exponential . $$dZ(t) = Z(t)dX(t) text{ , } Z(0) = 1$$ . It has the unique solution . $$ mathcal{E}(X)(t)= exp big(X(t)-X(0)-2[X,X](t) big)$$ . The stochastic exponential of an Ito process is . $$ mathcal{E}(X)(t)= exp bigg( int_0^t big( mu(s) - 0.5 sigma^2(s) big) ds + int_0^t sigma(s) dW(s) bigg)$$ . Brownian motion can be written as $X(t) = mu t + sigma W(t)$ with drift rate $ mu$ , diffusion coefficient $ sigma$, and Wiener proccess $W$. The quadratic variation of Brownian motion equals $[X, X](t) = sigma^2t$ and in particular $[W, W](t) = t$ for a Wiener process $W$ . . $dX(t) = mu X(t)dt + sigma X(t)dW(t)$, $X(0) = x$ can be rephrased as $dX(t) = X(t)dY (t)$, $X(0) = x$ with $Y (t) = mu t + sigma W (t)$. By the stochastic exponential $X(t)=x mathcal{E} (Y)(t)=x exp big(( mu-0.5 sigma^2) t + sigma W(t) big)$. This process $X$ is called geometric Brownian motion. . Black-Scholes model . We are in a complete market with a bond $$B(t) = e^{rt}$$ and a stock $$S(t) = S(0) exp big( mu t + sigma W(t) big)$$ . In order to compute option prices we need the law of the stock price under the unique EMM $ mathbb{Q}$. The discounted price process $ hat{S}(t) = S(0) exp(( mu - r)t + sigma W (t))$ solves the stochastic differential equation $d hat S(t)= hat S(t) big( ( mu - r + 0.5 sigma^2) dt + sigma dW(t) big)$ because of the stochastic exponential. . Now consider the measure $ mathbb{Q} sim mathbb{P}$ with density process . $$dZ(t) = -Z(t) frac{ mu - r + 0.5 sigma^2}{ sigma}dW(t)$$ . with $$Z(0) = 1 $$ . Girsanov&#8217;s theorem . If the density process $Z$ of $ mathbb{Q} sim mathbb{P}$ reads as $dZ(t) = Z(t) sigma (t)dW(t)$ the Wiener process $W$ can be written as $dW (t) = dW^ mathcal{Q}(t) + sigma(t)dt$ . Hence, $$dW(t) = dW^ mathcal{Q}(t) - frac{ mu - r + 0.5 sigma^2}{ sigma}dt $$ with some $ mathcal{Q}$-Wiener process $W^ mathcal{Q}$ . . This yields $ d hat{S}(t) = hat{S}(t) sigma d W^ mathcal{Q}(t)$, which implies that $ hat S$ is a $ mathcal{Q}$-martingale because the drift part in the Ito process decomposition relative to $ mathcal{Q}$ vanishes. Hence we have found the unique EMM $ mathcal{Q}$. . Using $S = hat S B $ and the stochastic exponential we obtain $$ S(t)=S(0) exp big( (r - 0.5 sigma^2) t + sigma W^ mathcal{Q}(t) big)$$ . . The option value is of an European option with payoff $X = f(S(T))$ is $V (0) = B(0) mathbb{E}^ mathbb{Q} big[(f(S(T))/B(T)) big] $ . . $W^ mathcal{Q}(T)$ has the same law as $ sqrt{T}Y$ for a standard normal random variable $Y$. . Because $ mathbb{E}[g(Y)] = frac{1}{ sqrt{2 pi}} int_{- infty}^{+ infty} g(x) exp(-0.5x^2) dx$ . for any function g . $$V(0) = frac{1}{ sqrt{2 pi}} int_{- infty}^{+ infty} exp(-{rT}) f(S(0) exp big( (r - 0.5 sigma^2) T + sigma sqrt{T} x big) ) exp(-0.5x^2) dx$$ . The option value at time $t$ $$V (t) =B(t) mathbb{E}^ mathbb{Q} big[f(S(T))/B(T) big| mathcal{F}_t big]$$ . can be computed by . $$V(t) = frac{1}{ sqrt{2 pi}} int_{- infty}^{+ infty} exp(-{r(T-t)}) f(S(t) exp big( (r - 0.5 sigma^2) (T-t) + sigma sqrt{T-t} x big) ) exp(-0.5x^2) dx$$ . if we use the representation $S(T)=S(t) exp big((r - 2 (T - t)+ sigma (W^ mathbb{Q} (T) - W^ mathbb{Q} (t)) big)$ and observe that $W^ mathbb{Q} (T) - W^ mathbb{Q}(t)$ has the same law as $ sqrt{T-t}Y$ for a standard normal random variable $Y$. . PDE approach . Instead of pricing by integration one could also solve the PDE to get the option value. . With $ hat V(t) = hat v(t, S(t))$ and $dS(t) = S(t)rdt + S(t) sigma W^ mathbb{Q}(t)$ Ito&#39;s formula yields . $$ hat{dV(t)} = hat{dv(t, S(t))}$$ $$= big( partial_1 hat{v(t, S(t))} + partial_2 hat{v(t, S(t))}rS(t) + 0.5 partial_{22} hat{v(t, S(t))} sigma^2 S (t)^2 big)dt + partial_2 hat{v(t, S(t))} sigma S(t) dW^ mathbb{Q}(t) $$ . Since $ hat V$ is a $ mathbb{Q}$-martingale, it drift part has to vanish regardless of $t$ and the present value of $S(t)$. This is only possible if $ hat v$ satisfies the partial differential equation (PDE) . $$ partial_1 hat{v(t, x)} + partial_2 hat{v(t,x)}rx + 0.5 partial_{22} hat{v(t, x)} sigma^2 x^2 = 0 $$ . or $$ frac{ partial}{ partial t} hat{v(t, x)} = -rx frac{ partial}{ partial x } hat{v(t,x)} - 0.5 sigma^2 x^2 frac{ partial}{ partial x^2} hat{v(t, x)} $$ . Since $v(t,x) = hat{v(t,x)}e^{rt}$, this leads to the Black-Scholes PDE . $$ frac{ partial}{ partial t} {v(t, x)} = rv(t,x)-rx frac{ partial}{ partial x } {v(t,x)} - 0.5 sigma^2 x^2 frac{ partial}{ partial x^2} {v(t, x)} $$ . This is complemented by the final value $v(T,x) = f(x)$. . Solving this system of equations gives the fair value of an European option. . The option can be replicated by a self-financing strategy $ phi = ( phi_0, phi_1)$ that satisfies $d hat V phi(t) = phi_1(t)d hat S(t) = phi_1(t) hat S (t) sigma dW^ mathbb{Q}(t)$. . Comparing this with the result from Ito&#39;s formula yields $$ phi_1(t) hat S(t) sigma = partial_2 hat v(t, S(t)) sigma S(t)$$ or . $$ phi_1(t) =S(t)/ hat S(t) partial_2 hat v(t, S(t)) $$ $$ = e^{rt} partial_2 hat v(t, S(t)) $$ $$ = partial_2 v(t, S(t)) $$ . the partial derivative of the option price relative to the stock price yields the number of shares of stock in the replicating portfolio. This quantity is called Delta. . The remaining funds are invested in the bond. . $$ phi_0(t) = frac{ v(t, S(t)) - phi_1 S(t)}{B(t)} $$ . from scipy.stats import norm import numpy as np from matplotlib import pyplot as plt import seaborn def eu_call_bs(S_t,r,sigma,T,K,t): d_1 = (np.log(S_t/K)+(r+sigma**2/2)*(T-t))/(sigma*(T-t)**0.5) d_2 = d_1 - sigma*(T-t)**0.5 phi_1 = norm.cdf(d_1) cdf_d2 = norm.cdf(d_2) c = S_t * phi_1 - K*np.exp(-r*(T-t)) *cdf_d2 phi_0 = -K*np.exp(-r*T)*cdf_d2 return c, phi_0, phi_1 def BS_EuCall_FiDi_Explicit (r, sigma, a, b, m, nu_max, T, K): w = np.zeros(m + 1) # Time and space discretization step sizes delta_t = 0.5*sigma**2*T / nu_max delta_x = (b-a) / m # Check for stability of explicit scheme Lambda = delta_t / delta_x**2 if Lambda &gt;= 1/2: raise ValueError(&quot;Finite difference scheme unstable.&quot;) # Determine grid in time and space t_tilde = np.array(range(nu_max+1)) * delta_t x_tilde = a + np.array(range(m+1)) * delta_x # Compute auxiliary variables q = 2*r/sigma**2 q_minus = 0.5 * (q-1) q_plus = 0.5 * (q+1) # Boundary condition for t_tilde = 0, corresponding to payoff # of the option at maturity. w = np.maximum( np.exp(q_plus*x_tilde)-np.exp(q_minus*x_tilde), 0 ) # Boundary condition for x = b. Stays the same for all time points w[0] = 0 # Iterate through time layers. for nu in range(nu_max): w[1:-1] = Lambda*(w[0:-2]) + (1-2*Lambda)*w[1:-1] + Lambda*(w[2:]) # Boundary condition for x = b. w[-1] = np.exp(q_plus*b + q_plus**2*t_tilde[nu+1])-np.exp( q_minus*b + q_minus**2*t_tilde[nu+1] ) # retransformation of heat equation V0 =( K * w * np.exp(-q_minus*x_tilde - 0.5* sigma**2*T*(q_minus**2 + q))) # vector of initial stock prices S = K*np.exp(x_tilde) return V0, S r = 0.05 sigma = 0.2 a = -0.7 b = 0.4 m = 100 nu_max = 2000 T = 1 K = 100 t=0 V0,S = BS_EuCall_FiDi_Explicit (r, sigma, a, b, m, nu_max, T, K) plt.plot(S,V0,label=&#39;FiDi - Explicit&#39;) plt.title(&quot;European Call vs. Stock Price - Explicit Finite Differences Scheme&quot;) plt.xlabel(&quot;Stock Price&quot;) plt.ylabel(&quot;Call Price&quot;) C, _,_ = eu_call_bs(S,r,sigma,T,K,t) plt.plot(S,C,&#39;.&#39;,label=&#39;BS closed form&#39;) plt.legend() plt.show() . from scipy.stats import norm import numpy as np from matplotlib import pyplot as plt import seaborn def eu_call_bs(S_t,r,sigma,T,K,t): d_1 = (np.log(S_t/K)+(r+sigma**2/2)*(T-t))/(sigma*(T-t)**0.5) d_2 = d_1 - sigma*(T-t)**0.5 phi_1 = norm.cdf(d_1) cdf_d2 = norm.cdf(d_2) c = S_t * phi_1 - K*np.exp(-r*(T-t)) *cdf_d2 phi_0 = -K*np.exp(-r*T)*cdf_d2 return c, phi_0, phi_1 def BS_EuCall_FiDi_CrankNicholson(r, sigma, a, b, m, nu_max, T, K): w = np.zeros(m + 1) # Time and space discretization step sizes delta_t = 0.5*sigma**2*T / nu_max delta_x = (b-a) / m # Determine grid in time and space t_tilde = np.array(range(nu_max+1)) * delta_t x_tilde = a + np.array(range(m+1)) * delta_x # Compute auxiliary variables q = 2*r/sigma**2 qminus = 0.5 * (q-1) qplus = 0.5 * (q+1) Lambda = delta_t / delta_x**2 # tridiagonal matrix for the implicit scheme A_alpha = (1+Lambda)*np.ones(m-1) A_beta = -0.5*Lambda*np.ones(m-2) # defining the tridiagonal matrix A in the linear equation system Ax=b A = np.diag(np.ones(m-1)*(1+Lambda),0) + np.diag(np.ones(m-2)*-0.5*Lambda,-1) + np.diag(np.ones(m-2)*-0.5*Lambda,1) # Boundary condition for t_tilde = 0, corresponding to payoff # of the call option at maturity. w = np.maximum( np.exp(qplus*x_tilde) - np.exp(qminus*x_tilde), 0 ) # Boundary condition for x = a. w[0] = 0 # Boundary condition for x = b. w[-1] = np.exp( qplus*b + qplus**2*t_tilde[0] ) - np.exp(qminus*b + qminus**2*t_tilde[0]) # Iterate through time layers. for nu in range(nu_max): # Explicit part of Crank-Nicholson w[1:-1] = 0.5*Lambda*w[:-2] + (1-Lambda)*w[1:-1] + 0.5*Lambda*w[2:] # Boundary condition for x = b. w[-1] = np.exp( qplus*b + qplus**2*t_tilde[nu+1] ) - np.exp(qminus*b + qminus**2*t_tilde[nu+1]) # Modification for w_nu,m and implicit part of Crank-Nicholson w[-2] = w[-2] + 0.5*Lambda*w[-1] # Solving the linear equation system Ax=b for x w[1:-1] = np.linalg.solve(A, w[1:-1].transpose()) # retransformation of heat equation V0 =( K * w * np.exp(-qminus*x_tilde - 0.5* sigma**2*T*(qminus**2 + q))) # vector of initial stock prices S = K*np.exp(x_tilde) #phi_1(0) = derivative of V(0,S(0)) w.r.t. S(0), can be approximated by (V(0,S(0)+h)-V(0,S(0)))/h #here h = difference between two initial stock price values (usually h should be small, but it works okay ) phi1 = np.diff(V0) / np.diff(S) return V0, S, phi1 r = 0.05 sigma = 0.2 a = -0.7 b = 0.4 m = 100 nu_max = 2000 T = 1 K = 100 V0, S, phi1 = BS_EuCall_FiDi_CrankNicholson(r, sigma, a, b, m, nu_max, T, K) plt.plot(S[1:],phi1,label=&#39;FiDi - Crank-Nicholson&#39;) plt.title(&quot;Delta vs. Stock Price - Crank-Nicholson Scheme&quot;) plt.xlabel(&quot;Stock Price&quot;) plt.ylabel(&quot;Delta&quot;) _, _,phi1_cf= eu_call_bs(S,r,sigma,T,K,0) plt.plot(S[1:],phi1_cf[:-1],&#39;.&#39;,label=&#39;BS closed form&#39;) plt.legend() plt.show() . Valueing American puts by solving the Black Scholes PDE using finite differences . from scipy.stats import norm import numpy as np from matplotlib import pyplot as plt import seaborn def brennon_schwartz(A_alpha, A_beta, A_gamma, b_solve, g_discr): n = len(A_alpha) x = np.zeros(A_alpha.shape) alpha_hat = np.zeros(A_alpha.shape) b_hat = np.zeros(b_solve.shape) # initial values of alpha_hat and b_hat alpha_hat[-1] = A_alpha[-1] b_hat[-1]= b_solve[-1] # backward recursion for values of alpha_hat and b_hat for i in range(n-2, -1, -1): alpha_hat[i] = A_alpha[i] - A_beta[i]*A_gamma[i+1]/alpha_hat[i+1] b_hat[i] = b_solve[i] - A_beta[i]*b_hat[i+1]/alpha_hat[i+1] # first value of output x x[0] = max(b_hat[0]/alpha_hat[0], g_discr[0]) # forward recursion for values of output x for i in range(1,n): x[i] = np.maximum((b_hat[i]-A_gamma[i]*x[i-1])/alpha_hat[i],g_discr[i]) return x def BS_AmPut_FiDi_CN (r, sigma, a, b, m, nu_max, T, K): w = np.zeros(m + 1) # Time and space discretization step sizes delta_t = 0.5*sigma**2*T / nu_max delta_x = (b-a) / m # Determine grid in time and space. t_tilde = np.array(range(nu_max+1)) * delta_t x_tilde = a + np.array(range(m+1)) * delta_x # Compute auxiliary variables. q = 2*r/sigma**2 qminus = 0.5 * (q-1) qplus = 0.5 * (q+1) Lambda = delta_t / delta_x**2 # Function for the transformed payoff of the american put def g(t_tilde, x_tilde): return np.exp(qplus**2*t_tilde)*np.maximum(np.exp(x_tilde*qminus)-np.exp(x_tilde*qplus),0) # tridiagonal matrix for the implicit scheme, used in the Brennon-Schwartz algorithm A_alpha = (1+Lambda)*np.ones(m-1) A_beta = -0.5*Lambda*np.ones(m-1) A_beta[-1]= 0 A_gamma = -0.5*Lambda*np.ones(m-1) A_gamma[0] = 0 # Boundary condition for t_tilde = 0, corresponding to payoff of the call option at maturity. w = g(t_tilde[0], x_tilde) b_solve = np.zeros(m-1) # Iterate through time layers. for nu in range(nu_max): g_discr = g(t_tilde[nu],x_tilde) # Boundary condition for x = b. w[-1] = g_discr[-1] # boundary condition for x = a. w[0] = g_discr[0] # Creating the vector b b_solve[1:-1] = w[2:-2] + 0.5*Lambda*(w[1:-3] - 2*w[2:-2] + w[3:-1]) # First and last value for b b_solve[0] = w[1]+ 0.5*Lambda*(w[2] - 2*w[1] + g_discr[0] + g(t_tilde[nu+1],a)) b_solve[-1] = w[-2] + 0.5*Lambda*(g_discr[-1] - 2*w[-2] + w[-3] + g(t_tilde[nu+1],b)) # Using the Brennon Schwartz algorithm w[1:-1] = brennon_schwartz(A_alpha, A_beta, A_gamma, b_solve, g_discr[1:-1]) # retransformation of heat equation V0 =( K * w * np.exp(-qminus*x_tilde - 0.5* sigma**2*T*(qminus**2 + q))) # vector of initial stock prices S = K*np.exp(x_tilde) return V0, S def crr_bs_approx(S_0,r,sigma,T,M,K,EU,Type): dt=T/M #Set u,d,q such that first and second moments match BS beta = 0.5*(np.exp(-r*dt)+np.exp((r+(sigma**2))*dt)) u = beta + ((beta**2)-1)**0.5 d = 1/u#beta - ((beta**2)-1)**0.5 q = (np.exp(r*dt)-d)/(u-d) if Type == &#39;call&#39;: def g(St,K): return max(St - K, 0) elif Type == &#39;put&#39;: def g(St,K): return max(K - St, 0) else: raise Exception(&quot;Specify valid Type (&#39;put&#39;/&#39;call&#39;)&quot;) g = np.vectorize(g ,otypes=[np.float]) S = np.zeros((M+1,M+1)) S[0,0] = S_0 V = np.zeros((M+1,M+1)) for i in range(1,M+1): for j in range(i+1): S[j,i] = S_0*(u**j)*(d**(i-j)) V[:,-1] = g(S[:,-1],K) if EU == 1: for i in range(M-1,-1,-1): for j in range(i+1): V[j,i] = np.exp(-r*dt)*( q*V[j+1,i+1] + (1-q)*V[j,i+1]) elif EU == 0: for i in range(M-1,-1,-1): for j in range(i+1): V[j,i] = max(g(S[j,i],K) ,np.exp(-r*dt)*( q*V[j+1,i+1] + (1-q)*V[j,i+1])) else: raise Exception(&quot;Specify valid EU state (0/1)&quot;) return V[0,0] r = 0.05 sigma = 0.2 a = -0.7 b = 0.4 m = 100 nu_max = 2000 T = 1 K = 100 V0,S = BS_AmPut_FiDi_CN (r, sigma, a, b, m, nu_max, T, K) plt.plot(S,V0,label=&#39;FiDi&#39;) plt.title(&quot;Put vs. Stock Price&quot;) plt.xlabel(&quot;Stock Price&quot;) plt.ylabel(&quot;Put Value&quot;) V0_crr = np.zeros(S.shape) for i in range(len(S)): V0_crr[i] = crr_bs_approx(S[i],r,sigma,T,200,K,0,&#39;put&#39;) plt.plot(S,V0_crr,&#39;.&#39;, label=&#39;CRR&#39;) plt.legend() plt.show() . Hedging Error . Since the option price depends on $ sigma$, which is not known and must be estimated, an interesting question to ask is what happens if one puts in a &#39;wrong&#39; $ sigma^*$. . Consider the delta hedged portfolio . $$ Pi_t = V_t - Delta_t S_t - frac{(V_t - Delta_t S_t)}{B_t} B_t $$ . $$d Pi_t = dV_t - Delta_t dS_t - (V_t - Delta_t S_t) r dt $$ $ tag{1}$ . By Ito&#39;s lemma . $$ begin{align} dV_t &amp;= frac{ partial V}{ partial t} dt + frac{ partial V}{ partial t} dS_t + frac{1}{2} frac{ partial^2 V}{ partial S^2} d[S_t,S_t] &amp;= theta_t dt + Delta_t dS_t + frac{1}{2} Gamma_t d[S_t,S_t] end{align}$$ $ tag{2}$ . Plugging (2) into (1) yields (3) . $$ d Pi_t = ( theta_t + rS_t Delta_t - rV_t)dt - frac{1}{2} Gamma d[S_t,S_t] $$ $ tag{3}$ . In the Black-Scholes model . $$ begin{align} theta_t + r S_t Delta_t + frac{1}{2} Gamma S_t^2 sigma^2 - r V_t = 0 theta_t + r S_t Delta_t - r V_t = - frac{1}{2} Gamma S_t^2 sigma^2 end{align} $$$ tag{4}$ . Plugging (4) into (3) . $$ begin{align} d Pi_t &amp;= - frac{1}{2} Gamma S_t^2 sigma^2 dt - frac{1}{2} Gamma d[S_t,S_t] &amp;= frac{1}{2} Gamma S_t^2 left( frac{d[S_t,S_t]}{(S_t)^2 dt} - sigma^2 right) dt &amp;= frac{1}{2} Gamma S_t^2 left( sigma^{2*} - sigma^2 right) dt end{align}$$Where the last equality follows from the fact that the quadratic variation of geometric Brownian motion $ d[S_t,S_t] = sigma^{2*} S_t^2 dt$ . The total $PnL_t$ due to the wrong $ sigma^{*}$ is given by the integral . $$PnL_t = int_0^T e^{-r(T-t)} frac{1}{2} Gamma S_t^2 left( sigma^{2*} - sigma^2 right) dt$$ . Options pricing via Laplace Transform . Often the probability density function of the stock price are not known but characteristic functions are available instead. In these cases we can use Laplace transforms to price options. We consider a model with bond and stock of the form . $$ begin{align} B(t) = e^{rt} S(t) = e^{X(t)} end{align} $$ . If the payoff of the claim under consideration is of the form $f(X(T))$, the initial fair option price equals . $$ begin{align} V(t) = B(0) mathbb{E}^ mathbb{Q} big[[f(X(T))/B(T) big] = e^{-rT} mathbb{E}^ mathbb{Q} big[[f(X(T)) big] end{align} $$ . For very simple payoffs, the expectation can be calculated explicitly in many models, namely for $f(x) = e^{zx} $ with some constant $z$. For more complicated ones we consider payoffs of the form . $$ f(x)=e^{zx} =e^{ Re(z)x(cos( Im(z)x)+ i sin( Im(z)x))}$$ . where $z in mathbb{C}$ and $i = sqrt{-1} $. . The corresponding option price equals . $$V(0) = e^{-rT} mathbb{E}^ mathbb{Q}(e^{zX(T))}) = e^{-rT} chi(-i z)$$ . with $ chi(u) = mathbb{E}^ mathbb{Q}(e^{i u X(T)})$. . $ chi(u)$ is called (extended) characteristic function of $ X (T )$ in $u$ and it is known in closed form for many processes $X$. . The key idea is to write an arbitrary, more complicated payoff (e.g. a European call) in the form . $$ f(x) = int rho(z) e^{xz} dz$$ . with some function $ rho(z)$. Such a representation can be viewed as a generalized linear combination of “simple” payoffs $e^{zx}$. . import math from scipy.integrate import quad import numpy as np def bs_eu_call(S0, r, sigma, T, K, R): # Laplace transform of f(x) = (e^x - K)^+ def f_tilde(z): return K**(1-z) / (z*(z-1)) # Characteristic function of log(S(T)) in the Black-Scholes model def chi(u): return np.exp( 1j*u*(np.log(S0)+r*T) - (1j*u+u**2)*sigma**2/2*T ) # Integrand for the Laplace transform method def integrand(u): return np.exp(-r*T)/math.pi * ( f_tilde(R+1j*u)*chi(u-1j*R) ).real # option price V0 , err = quad(integrand, 0, 50) return V0 S0 = 100 r = 0.03 sigma = 0.3 T = 1 K = 100 R = 1.1 V0_BS = bs_eu_call(S0, r, sigma, T, K, R) print(f&quot;Option price via Laplace Transform in BS framework: {round(V0_BS,3)}&quot;) . Option price via Laplace Transform in BS framework: 13.283 . import math from scipy.integrate import quad import numpy as np def heston_eu_call_hedge_laplace(St, r, nut, kappa, Lambda, sigma_tilde, t, T, K, K_tilde, R, R_tilde): # Laplace transform of the function f(x) = (e^x - K)^+, def f_tilde_K(z): return K**(1-z) / (z*(z-1)) # Laplace transform of the function f(x) = (e^x - K_tilde)^+, def f_tilde_K_tilde(z): return K_tilde**(1-z) / (z*(z-1)) # Characteristic function of log(S(T)) at timepoint t in the Heston model, def chi (u): d = np.sqrt(Lambda**2+sigma_tilde**2*(1j*u+u**2)) n = math.cosh(d*(T-t)/2) + Lambda*math.sinh(d*(T-t)/2)/d z1 = np.exp(Lambda*(T-t)/2) z2 = (1j*u+u**2)*math.sinh(d*(T-t)/2)/d return np.exp( 1j*u*(np.log(St)+r*(T-t))) * (z1/n)**(2*kappa/sigma_tilde**2) * np.exp(-nut*z2/n) # derivative of Characteristic function of log(S(T)) w.r.t. S(t) def delx_chi(u): return 1j*u/St * chi(u) # derivative of Characteristic function of log(S(T)) w.r.t. gamma(t) def delnu_chi(u): d = np.sqrt(Lambda**2+sigma_tilde**2*(1j*u+u**2)) n = math.cosh(d*(T-t)/2) + Lambda*math.sinh(d*(T-t)/2)/d z2 = (1j*u+u**2)*math.sinh(d*(T-t)/2)/d return -z2/n * chi(u); # general Integrand for the Laplace transform method, arguments are (besides &quot;u&quot;) # the laplace transform we want to use (for different calls or puts) and characteristic funtion or derivative def laplace_integrand (u, f_tilde, del_func): return np.exp(-r*T)/math.pi * ( f_tilde(R+1j*u)*del_func(u-1j*R) ).real # price of the option and liquidly traded Call Vt, _ = quad(laplace_integrand, 0, 50 ,args=(f_tilde_K_tilde,chi)) Ct, _ = quad(laplace_integrand, 0, 50 , args=(f_tilde_K, chi)) # derivates of the option price and liquidly traded Call with respect to the variance process (gamma_t) del3v,_ = quad(laplace_integrand, 0, 50 ,args=(f_tilde_K_tilde, delnu_chi)) del3c,_ = quad(laplace_integrand, 0, 50 ,args=(f_tilde_K, delnu_chi)) # derivates of the option price and liquidly traded Call with respect to S_t del2v, _ = quad(laplace_integrand, 0, 50, args=(f_tilde_K_tilde, delx_chi)) del2c, _ = quad(laplace_integrand, 0, 50, args=(f_tilde_K, delx_chi)) # hedge phi2 = del3v/del3c phi1 = del2v - phi2*del2c phi0 = (Vt - phi1*St - phi2*Ct)*np.exp(-r*t) return Vt, phi0, phi1, phi2 St = 100 r = 0.03 nut = 0.3**2 kappa = 0.3**2 Lambda = 2.5 sigma_tilde = 0.2 T = 1 K = 95 K_tilde = 100 R = 1.1 R_tilde = 1.1 t=0 Vt, phi0, phi1, phi2 = heston_eu_call_hedge_laplace(St, r, nut, kappa, Lambda, sigma_tilde, t, T, K, K_tilde, R, R_tilde) print(f&quot; Option Price: {Vt} n Hedge Position Call: {phi2} n Hedge Position Stock: {phi1} n Hedge Position Bond: {phi0} n&quot;) . Option Price: 9.755867884013995 Hedge Position Call: 1.0634215306663264 Hedge Position Stock: -0.12638679472545133 Hedge Position Bond: 9.309745759697751 . /Users/jan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part /Users/jan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:23: ComplexWarning: Casting complex values to real discards the imaginary part /Users/jan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:35: ComplexWarning: Casting complex values to real discards the imaginary part /Users/jan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:36: ComplexWarning: Casting complex values to real discards the imaginary part . from IPython.display import display import numpy as np from matplotlib import pyplot as plt import pandas as pd import seaborn def BS_EuCall_FFT(S0, r, sigma, T, K, R, N, M): &#39;&#39;&#39; The fast Fourier transform method allows us to compute option prices for many strikes in an efficient manner. &#39;&#39;&#39; Delta=M/N kappa_1=np.log(S0)-(N/2)*2*np.pi/M kappa=kappa_1+np.array(range(N))*2*np.pi/M def g(u): # Laplace transform of the function f_0(x) = (e^x - e^0)^+, def f_tilde_0(z): return 1 / (z*(z-1)) # Characteristic function of log(S(T)) in the Black-Scholes model, def chi (u): return np.exp( 1j*u*(np.log(S0)+r*T) - (1j*u+u**2)*sigma**2/2*T ) y=np.ones(len(u),dtype=complex)# Important: You need to tell numpy the dtype=complex for i in range(len(u)): y[i]=f_tilde_0(R+1j*u[i])*chi(u[i]-1j*R) return y print(y) x=g((np.array(range(1,N+1))-0.5)*Delta)*Delta*np.exp(-1j*(np.array(range(N)))*Delta*kappa_1) #Computing the discrete fourier transform of x x_hat=np.fft.fft(x) #Computing the prices of options with values in kappa V_kappa=1/np.pi*np.exp(-r*T+(1-R)*kappa)*((x_hat*np.exp(-1j/2*Delta*kappa)).real) #Computing the prices of option with value in K using linear interpolation in log-strikes V0=np.interp(np.log(K),kappa, V_kappa ) return V0 S0 = 100 r = 0.05 sigma = 0.2 T = 1 K = np.array(range(80,131)) R = 1.1 N = 2**11 M = 50 #time the function %timeit V=BS_EuCall_FFT(S0, r, sigma, T, K, R, N, M) option_prices = pd.DataFrame() option_prices[&#39;Strike_Price&#39;] = K option_prices[&#39;Option_Price&#39;] = V display(option_prices) plt.plot(K,V, &#39;.&#39;) plt.title(&quot;Call vs. Strike Price -- fast Fourier transform&quot;) plt.xlabel(&quot;Strike Price&quot;) plt.ylabel(&quot;Call Value&quot;) plt.show() . 100 loops, best of 3: 17 ms per loop . Strike_Price Option_Price . 0 80 | 24.606408 | . 1 81 | 23.766824 | . 2 82 | 22.937541 | . 3 83 | 22.118311 | . 4 84 | 21.308892 | . 5 85 | 20.509052 | . 6 86 | 19.718568 | . 7 87 | 18.937222 | . 8 88 | 18.164805 | . 9 89 | 17.468357 | . 10 90 | 16.795490 | . 11 91 | 16.130059 | . 12 92 | 15.471901 | . 13 93 | 14.820857 | . 14 94 | 14.176777 | . 15 95 | 13.539513 | . 16 96 | 12.908922 | . 17 97 | 12.284865 | . 18 98 | 11.667209 | . 19 99 | 11.055824 | . 20 100 | 10.450584 | . 21 101 | 10.013594 | . 22 102 | 9.580910 | . 23 103 | 9.152447 | . 24 104 | 8.728124 | . 25 105 | 8.307862 | . 26 106 | 7.891583 | . 27 107 | 7.479213 | . 28 108 | 7.070679 | . 29 109 | 6.665910 | . 30 110 | 6.264838 | . 31 111 | 5.867396 | . 32 112 | 5.473518 | . 33 113 | 5.083141 | . 34 114 | 4.798694 | . 35 115 | 4.581994 | . 36 116 | 4.367170 | . 37 117 | 4.154190 | . 38 118 | 3.943022 | . 39 119 | 3.733637 | . 40 120 | 3.526004 | . 41 121 | 3.320094 | . 42 122 | 3.115878 | . 43 123 | 2.913330 | . 44 124 | 2.712422 | . 45 125 | 2.513128 | . 46 126 | 2.315421 | . 47 127 | 2.119278 | . 48 128 | 1.924673 | . 49 129 | 1.779225 | . 50 130 | 1.698612 | .",
            "url": "https://jpwoeltjen.github.io/researchBlog/derivatives/valuation/monte%20carlo/stochastic%20calculus/2019/01/03/ComputationalFinanceNotes.html",
            "relUrl": "/derivatives/valuation/monte%20carlo/stochastic%20calculus/2019/01/03/ComputationalFinanceNotes.html",
            "date": " • Jan 3, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This blog is written by Jan P. Woeltjen. .",
          "url": "https://jpwoeltjen.github.io/researchBlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jpwoeltjen.github.io/researchBlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}