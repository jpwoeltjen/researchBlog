{
  
    
        "post0": {
            "title": "Inference in Bayesian Neural Networks",
            "content": "HMC . Idea . HMC is a gradient-based MCMC sampler that employs Hamiltonian dynamics to traverse the parameter space of models. | A state proposed in this way can be distant from the current state but nevertheless have a high probability of acceptance. | We want to sample from the posterior $ pi left(q right):= p(q mid mathcal{D}) propto p( mathcal{D} mid q) p(q)$, where $q in mathbb{R}^{D}$ is a point in parameter space. | Focus computation on areas of high probability mass. | Probability mass is the product of density and volume -- not just density. | In high dimensions, the area of high probability mass, the typical set, may be far away from the mode. | . . (Betancourt, 2017) . This may seem strange since it is difficult to intuit the behaviour in high dimensions. But it is true. | . The higher the dimension the thinner the typical set. | Finding the thin typical set in a high dimensional space is difficult. | . . (Betancourt, 2017) . Use a Markov chain to find and stay within the typical set. | Distribution of points we can jump to specified by Markov transition operator $ tau left(q mid q^{ prime} right)$. | Needs to be engineered to satisfy $ pi(q)= int tau left(q mid q^{ prime} right) pi left(q^{ prime} right) mathrm{d} q^{ prime}$ | In the limit MCMC is unbiased: $ lim _{N rightarrow infty} frac{1}{N} sum_{n=0}^{N} f left(q_{n} right) rightarrow mathbb{E}_{ pi}[f]$. | Note that marginalization and higher moments are just expectations. | . . (Betancourt, 2017) . In the Random Walk Metropolis algorithm the transition distribution is given by begin{equation} tau left(q, q^{ prime} right)= mathcal{N} left(q^{ prime} mid q, sigma^{2} right) min left(1, frac{ pi left(q^{ prime} right)}{ pi(q)} right) end{equation} . | Random Walk Metropolis is inefficient since random proposals in high dimensions almost certainly do not remain in the typical set. Thus the rejection probability is exeedingly high. . | . . (Betancourt, 2017) . We need to exploit the geometry of the high dimensional surface to &#39;surf&#39; the typical set. | . . (Betancourt, 2017) . Nice visualization of different algorithms https://chi-feng.github.io/mcmc-demo/app.html#HamiltonianMC,banana | For that we compute the gradient $ frac{ partial pi(q)}{ partial q}$, ($ pi(q)$ is known up to a normalizing constant). | But $ frac{ partial pi(q)}{ partial q}$ points to the mode. This is not what we need. (We do not optimize). We want to move along the typical set. | The underlying mathematics to do so are the same as simulating the total energy of a frictionless ball rolling in a bowl that has the shape of the log-probability distribution turned on its head via the Hamiltonian $H(p, q)$ in classical physics. | We construct a vector field aligned with the typical set using the Hamiltonian $H(p, q)$ based on $q$ and randomly sampled momentum $p in mathbb{R}^{D}$. . | Instead of just the position in sample space, we now need to consider a $2D$-dimensional phase space $q rightarrow(p, q)$, and the joint distribution of the positions and momenta $ pi(q) = e^{-V(q)} rightarrow pi(p, q)= pi(p mid q) pi(q) = e^{-H(q, p)}$. . | The Hamiltonian is given by begin{equation} begin{aligned} H(p, q) &amp;=- log pi(p mid q) pi(q) &amp;=- log pi(p mid q)- log pi(q) &amp;=: quad K(p, q)+V(q) end{aligned} end{equation} . | We evolve the system with Hamilton&#39;s equations as follows: begin{equation} begin{aligned} frac{ mathrm{d} q}{ mathrm{d} t} &amp;=+ frac{ partial H}{ partial p}= frac{ partial K}{ partial p} frac{ mathrm{d} p}{ mathrm{d} t} &amp;=- frac{ partial H}{ partial q}=- frac{ partial K}{ partial q}- frac{ partial V}{ partial q} end{aligned} end{equation} . | The proposal behaves like a satelite in orbit around a planet. | We need to add just the right amount of momentum in order to not crash into the mode or escape into infinity. . | Randomly sampled momenta facilitate a random walk between level sets. The vector field facilitates efficient exploration within the level set. . | . . (Betancourt, 2017) . For how long should we simulate the trajectory of each momentum draw? Tradeoff: exploit the geometric information as much as possible by staying on the trajectory | but not as long until we end up where we started. (E.g. No U-turn sampler (NUTS) (Hoffman &amp; Gelman, 2014)) | . | . . (Betancourt, 2017) . In practice we numerically approximate the trajectory using symplectic integrators, e.g. the Leapfrog method, which unlike other standard numerical methods do not introduce bias. | . . (Neal &amp; others, 2011) . optimize step size $ epsilon$: not too big so that rejection probability is too high. | not too small so that simulation is too expensive. | . | . . (Betancourt, 2017) . In theory the total energy given by $H$ is preserved so that all proposals are accepted. In practice however, we require a Metropolis-Hastings step to either accept or reject the proposed parameters to correct for any possible error due to approximating the dynamics with discrete steps. | . begin{equation} begin{array}{c} q rightarrow q+ epsilon frac{ partial K}{ partial p} p rightarrow p- epsilon left( frac{ partial K}{ partial q}+ frac{ partial V}{ partial q} right) pi(a c c e p t)= min left(1, frac{ pi left( Phi_{ tau}(p, q) right)}{ pi(p, q)} right) end{array} end{equation} When we choose the momentum distribution as multivariate Gaussian with covariance equal to the identity: $$ K({p}, {q})= frac{1}{2} {p}^{T} {p}+ mathrm{const.} $$ so $$ frac{ partial K}{ partial {p}}={p} $$ and $$ frac{ partial K}{ partial {q}}={0} $$ Hamilton&#39;s equations simplify to: $$ begin{aligned} frac{d {q}}{d t} &amp;={p} frac{d {p}}{d t}=&amp;- frac{ partial V}{ partial {q}} end{aligned} $$ . | So the update equations are: begin{equation} begin{array}{c} q rightarrow q+ epsilon p p rightarrow p- epsilon frac{ partial V}{ partial q} pi(a c c e p t)= min left(1, frac{ pi left( Phi_{ tau}(p, q) right)}{ pi(p, q)} right) end{array} end{equation} . | The proposal for $q$ looks a lot like gradient descent with momentum (see https://distill.pub/2017/momentum/). . | . Langevin dynamics uses similar ideas. The proposal distribution is given by $$ tau left(q^{ prime} mid q right)= mathcal{N} left(q+ frac{1}{2} sigma^{2} nabla log pi(q), sigma^{2} I right) $$ | Then follows a Metropolis-Hastings step. | Tries to move in directions of increasing $ pi(q)$ | Looks a lot like gradient descent with noise. | . Advantages . Asymptotically unbiased. | Very efficient and principled way to draw samples from the posterior. | When it fails it tells us. | . Current limitations . Computing the gradient over the full data set is expensive if $n$ is large. | There are fundamental obstructions using stochastic gradient (mini-batch) algorithms for scaling to large datasets (especially in wide data regimes). (Betancourt, 2015) | (Chen et al., 2014) use a friction term and a decreasing step size to counteract noise due to the stochastic gradient and avoid a Metropolis-Hastings step, which (over the full training set) would be prohibitively expensive. | There are other approaches to scaling HMC that do not rely on SG (Cobb &amp; Jalaian, 2020). | . | . The future . HMC is a general and uniquely positioned algorithm to efficiently sample from the posterior. Though current challenges exist in scaling this approach to very large data sets, future gains in computational power may alleviate these concerns. | There are efforts to make SG-MCMC work for time series (i.e. dependent data) (Aicher et al., 2019), (Aicher et al., 2019), (missing reference) propose a cyclical step size to explore multi-modal posteriors of BNN more effectively. This is a cheap approximation to multi-chain HMC. They show improvements empirically on imagenet data scale. | . Implementation . from autograd import grad import autograd.numpy as np import scipy.stats as st class Metropolis: def __init__(self, log_p, q_0): self.q = q_0 self.qs = [np.array(q_0)] self.accepts = [] self.log_p = log_p def metropolis_check(self, log_p_new, log_p_old): if np.log(np.random.rand()) &lt; log_p_new - log_p_old: self.accept = True else: self.accept = False self.q = np.copy(self.qs[-1]) def sample(self, n_samples): for _ in range(n_samples): # 1. propose q self.q = self.qs[-1] + np.random.normal(0,1) # 2. Metropolis check self.metropolis_check(self.log_p(self.q), self.log_p(self.qs[-1])) # append to samples self.qs.append(self.q) self.accepts.append(self.accept) class HMC(Metropolis): def __init__(self, log_p, q_0): super().__init__(log_p, q_0) # Momentum dist pi( mathbf{p} mid mathbf{q})= mathcal{N}( mathbf{0}, I) self.mom = st.norm(0, 1) # List of momenta self.ps = [self.mom.rvs(1)] # Gradient of the potential energy w.r.t. q self.dVdq = grad(lambda x: -self.log_p(x)) def sample(self, n_samples, path_len, step_size): for _ in range(n_samples): # 1. resample momentum from N(0,1) self.p = self.mom.rvs(1) # 2. simulate system with numerical integrator energy_start = self.log_joint_p(self.q, self.p) self.leapfrog(path_len, step_size) energy_end = self.log_joint_p(self.q, self.p) # 3. check that numerical integrator conserved energy self.metropolis_check(energy_start, energy_end) # append to the sample lists self.qs.append(self.q) self.accepts.append(self.accept) self.ps.append(self.p) if self.accept else self.ps.append(self.ps[-1]) def log_joint_p(self, q, p): &quot;&quot;&quot;log pi(q, p)&quot;&quot;&quot; return self.log_p(q) + np.sum(self.mom.logpdf(p)) def leapfrog(self, path_len, step_size): &quot;&quot;&quot;Leapfrog integrator. &quot;&quot;&quot; q, p = np.copy(self.q), np.copy(self.p) p -= step_size * self.dVdq(q) / 2 for _ in range(int(path_len / step_size) - 1): q += step_size * p p -= step_size * self.dVdq(q) q += step_size * p p -= step_size * self.dVdq(q) / 2 self.q, self.p = q, -p . def log_p_normal(mu, sigma): &quot;&quot;&quot; logp(x | mu, sigma) = -0.5 * (log(2π) + log(σ) + 0.5 * ((x - μ)/σ)^2) &quot;&quot;&quot; def logp(x): return -0.5 * (np.log(2 * np.pi * sigma * sigma) + ((x - mu) / sigma) ** 2) return logp . n_samples = 10000 sampler = HMC(log_p_normal(0,1), np.array([0.])) sampler.sample(n_samples, step_size=0.1, path_len=1) . p_accept = np.array(sampler.accepts, dtype=int).mean() print(&#39;HMC acceptance probability:&#39;, p_accept) . HMC acceptance probability: 0.9993 . plt.hist(np.array(sampler.qs[1:]).flatten(), 100, density=True, color=&#39;darkred&#39;, alpha=0.7) x = np.linspace(-5,5,101) plt.plot(x, st.norm(0,1).pdf(x), color=&#39;gray&#39;, alpha=0.9) plt.title(&#39;Histogram of HMC samples from $ mathcal{N}(0, I)$&#39;); . n_samples = 10000 sampler = Metropolis(log_p_normal(0,1), 0.) sampler.sample(n_samples) . p_accept = np.array(sampler.accepts, dtype=int).mean() print(&#39;RWM acceptance probability:&#39;, p_accept) . RWM acceptance probability: 0.7079 . plt.hist(np.array(sampler.qs[1:]).flatten(), 100, density=True, color=&#39;darkred&#39;, alpha=0.7) x = np.linspace(-5,5,101) plt.plot(x, st.norm(0,1).pdf(x), color=&#39;gray&#39;, alpha=0.9) plt.title(&#39;Histogram of Random Walk Metropolis samples from $ mathcal{N}(0, I)$&#39;); . In one dimension, HMC is not advantageous relative to RWM. But in high dimensions, most proposals of RWM will be rejected while HMC will still accept most of them. Note the HMC acceptance probability of almost 1 in the example above! . Variational Inference . We are again interested in the posterior $p(z mid x, alpha)= frac{p(z, x mid alpha)}{ int_{z} p(z, x mid alpha)}$. | Assume that $x=x_{1: n}$ are observations and $z=z_{1: m}$ are hidden variables (e.g. parameters). We assume additional parameters $ alpha$ that are fixed (e.g. hyperparameters). | The evidence $$ int_{z} p(z, x mid alpha)$$ is intractable. | VI turns inference into optimization problem. | Posit a variational family of distributions over the parameters $$q left(z_{1: m} mid nu right),$$ which has nice properties so that computing the needed expectations is tractable. | Note that this restriction is why VI is generally not unbiased. | Fit the variational parameters $ nu$ such that the KL divergence between the parameterized variational distribution and the posterior is minimized. | Problem is: how to do this minimization if the input $p(z mid x)$ is intractable? Is is the reason we are doing the optimization after all. | Solution: maximize the Evidence Lower BOund (ELBO), which, as the name suggests, is a lower bound on the (log) evidence. | . . The ELBO can be derived as follows: begin{equation} begin{aligned} log p(x) &amp;= log int_{z} p(x, z) &amp;= log int_{z} p(x, z) frac{q(z)}{q(z)} &amp;= log left( mathrm{E}_{q} left[ frac{p(x, z)}{q(z)} right] right) &amp; geq mathrm{E}_{q}[ log p(x, z)]- mathrm{E}_{q}[ log q(z)] =: text{ELBO}, end{aligned} end{equation} where the inequality follows from Jensen&#39;s inequality. | Note that the second term of the ELBO, $- mathrm{E}_{q}[ log q(z)]$, is the entropy. | . Why does maximizing the ELBO yield the same optimal variational parameters as minimizing the KL divergence? Because the KL divergence is equal to the negative ELBO plus the log evidence $ log p(x)$ (which does not depend on $q$): begin{equation} begin{aligned} operatorname{KL}(q(z) | p(z mid x)) &amp;= mathrm{E}_{q} left[ log frac{q(z)}{p(z mid x)} right] &amp;= mathrm{E}_{q}[ log q(z)]- mathrm{E}_{q}[ log p(z mid x)] &amp;= mathrm{E}_{q}[ log q(z)]- mathrm{E}_{q}[ log p(z, x)]+ log p(x) &amp;=- left( mathrm{E}_{q}[ log p(z, x)]- mathrm{E}_{q}[ log q(z)] right)+ log p(x) &amp;= - text{ELBO} + log p(x) end{aligned} end{equation} | . Note that maximizing the ELBO is a tradeoff between the MAP estimate and the prior: begin{equation} begin{aligned} text{ELBO} &amp;= mathrm{E}_{q}[ log p(x, z)]- mathrm{E}_{q}[ log q(z)] &amp;= mathrm{E}_{q}[ log p(x mid z) p(z)]- mathrm{E}_{q}[ log q(z)] &amp;= mathrm{E}_{q}[ log p(x mid z)] - mathrm{E}_{q}[ log frac{q(z)}{p(z)}] &amp;= mathrm{E}_{q}[ log p(x mid z)] - operatorname{KL}(q(z) | p(z)) end{aligned} end{equation} | The first term is maximized with a Dirac delta function at the MAP or maximum likelihood estimate of $z$. | The second term is the negative KL divergence of the variational distribution and the prior. | . There are several ways to do the optimization, e.g.: Mean field approximation | Stochastic variational inference | Black box variational inference | . | . Advantages . Efficient and scalable to huge datasets. | . Limitations . Is asymtotically not unbiased. | Finds a unimodal approximation (use ensembeles). | Currently deep ensembles do better than VI (but ensembling VI probably is better still). | . References . Betancourt, M. (2017). A conceptual introduction to Hamiltonian Monte Carlo. ArXiv Preprint ArXiv:1701.02434. | Hoffman, M. D., &amp; Gelman, A. (2014). The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. J. Mach. Learn. Res., 15(1), 1593–1623. | Neal, R. M., &amp; others. (2011). MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 2(11), 2. | Betancourt, M. (2015). The fundamental incompatibility of scalable Hamiltonian Monte Carlo and naive data subsampling. International Conference on Machine Learning, 533–540. | Chen, T., Fox, E., &amp; Guestrin, C. (2014). Stochastic gradient hamiltonian monte carlo. International Conference on Machine Learning, 1683–1691. | Cobb, A. D., &amp; Jalaian, B. (2020). Scaling Hamiltonian Monte Carlo Inference for Bayesian Neural Networks with Symmetric Splitting. ArXiv Preprint ArXiv:2010.06772. | Aicher, C., Ma, Y.-A., Foti, N. J., &amp; Fox, E. B. (2019). Stochastic gradient mcmc for state space models. SIAM Journal on Mathematics of Data Science, 1(3), 555–587. | Aicher, C., Putcha, S., Nemeth, C., Fearnhead, P., &amp; Fox, E. B. (2019). Stochastic Gradient MCMC for Nonlinear State Space Models. | .",
            "url": "https://jpwoeltjen.github.io/researchBlog/bayesian/deep%20learning/2020/11/18/BayesianInference.html",
            "relUrl": "/bayesian/deep%20learning/2020/11/18/BayesianInference.html",
            "date": " • Nov 18, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Bayesian Deep Learning for Financial Markets",
            "content": "We want to estimate the posterior predictive distribution of asset returns $y$ given features $x$ and training data $ mathcal{D}$ $$p(y mid x, mathcal{D})= int p(y mid x, w) p(w mid mathcal{D}) d w.$$ . In the Bayesian framework, we use probability theory to reason about models. We are honest about the uncertainty we have about the models and their parameters and avoid picking any single one. Instead, we ensemble models and parameters, weighting by their posterior probability distribution -- marginalization instead of optimization. The posterior is given by Bayes&#39; Theorem: . $$p left(w mid mathcal{D} right)= frac{p left( mathcal{D} mid w right) p(w)}{ int p left( mathcal{D} mid w right) p(w) d w}.$$ . There are several different ways to compute or approximate this posterior. The Bayesian framework is much more than any specific algorithm, though. Its distinguishing feature is the use of probability distributions instead of point estimates. Any model can be viewed through a Bayesian lens. . In the case of infinite iid data, Bayesian and frequentist methods yield the same result. Then the posterior is the Dirac delta function with infinite density at the maximum likelihood estimate. The number of observations may be a misleading measure to assess whether one is &#39;near&#39; this ideal case. It is important to distinguish tall and wide data regimes. In a tall data regime, we have lots of independent training examples from the same distribution. Wide data regimes, on the other hand, may also have many training examples but they are not independent of each other and require a more complex model. . Financial data are exceptionally noisy and structurally complex. In addition, uncertainty estimates are critically important for the decision-making process. These are exactly the conditions where a Bayesian framework is most advantageous. In the following, I’ll detail some of the advantages a Bayesian framework brings to the modeling of financial data. . Quantification of epistemic uncertainty . The ultimate goal of financial forecasting is to make profitable decisions. | For optimal decisions we need to account for uncertainty, this is embodied in the common mean-variance optimization approach due to Markowitz (1952). | However, mean-variance portfolio optimization based on maximum likelihood estimates neglects epistemic uncertainty. | Instead, use the empirical mean and variance of the posterior predictive distribution. | Even more generally, maximize utility over portfolio weights $ boldsymbol{ omega}$: $ max _{ omega} mathrm{E} left[U left( boldsymbol{ omega}^{ prime} mathbf{y}_{T+1} right) right]= max _{ omega} int U left( boldsymbol{ omega}^{ prime} mathbf{y}_{T+1} right) p left( mathbf{y}_{T+1} mid mathcal{D} right) mathrm{d} mathbf{y}_{T+1}$ subject to $ boldsymbol{ omega}^{ prime} mathbf{1}=1$. | Allows for custom utility functions and non-normality (skew, kurtosis). | Portfolio Optimization within a Bayesian Framework. | . Epistemic uncertainty is the uncertainty about the model -- uncertainty which can be explained away given enough data. By quantifying it we: . Naturally adjust for parameter uncertainty of different models and/or assets (due to unequal amounts of data or otherwise weaker evidence). | Naturally downscale portfolio allocations when a new data point is far away from training data (e.g. data anomalies, financial crises, regime shifts). . | On the Uncertainty Quantification of Deep Neural Networks. . | . | . . Better generalization . Architecture selection . Consider neural networks as non-parametric models (i.e. having infinitely many parameters). | The number of parameters should not be used as a measure of complexity. | Instead, consider the model&#39;s support (which solutions are a priori possible) and inductive bias (which solutions are a priori likely). . The support is the range of datasets for which $p( mathcal{D} mid mathcal{M})&gt;0 .$ | The inductive biases is the relative prior probabilities of different datasets -- the distribution of support given by $p( mathcal{D} mid mathcal{M})$. | . | The evidence, or marginal likelihood, $$p( mathcal{D} mid mathcal{M})= int p( mathcal{D} mid mathcal{M}, w) p(w) d w,$$ is the probability we would generate a dataset if we were to randomly sample from the prior over functions $p(f(x))$ induced by a prior over parameters $p(w)$. . | . . Wilson, Andrew Gordon, and Pavel Izmailov. &quot;Bayesian deep learning and a probabilistic perspective of generalization.&quot; arXiv preprint arXiv:2002.08791 (2020). . Model comparison and ensembling . The “Bayesian way” to compare models is to compute the marginal likelihood of each model $p left( mathcal{D} mid mathcal{M}_{k} right),$ i.e. the probability of the observed data $ mathcal{D}$ given the model $ mathcal{M}_{k}$. | Then by Bayes&#39; theorem: $p left( mathcal{M}_{k} mid mathcal{D} right) propto p left( mathcal{D} mid mathcal{M}_{k} right) p left( mathcal{M}_{k} right)$ | Bayes factor is a likelihood ratio of the marginal likelihood of two competing models: begin{equation} K= frac{ operatorname{p} left( mathcal{D} mid mathcal{M}_{1} right)}{ operatorname{p} left( mathcal{D} mid mathcal{M}_{2} right)}= frac{ int operatorname{p} left(w_{1} mid mathcal{M}_{1} right) operatorname{p} left( mathcal{D} mid w_{1}, mathcal{M}_{1} right) d w_{1}}{ int operatorname{p} left(w_{2} mid mathcal{M}_{2} right) operatorname{p} left( mathcal{D} mid w_{2}, mathcal{M}_{2} right) d w_{2}}= frac{ frac{ operatorname{p} left( mathcal{M}_{1} mid mathcal{D} right) operatorname{p}( mathcal{D})}{ operatorname{p} left( mathcal{M}_{1} right)}}{ frac{ operatorname{p} left( mathcal{M}_{2} mid mathcal{D} right) operatorname{p}( mathcal{D})}{ operatorname{p} left( mathcal{M}_{2} right)}}= frac{ operatorname{p} left( mathcal{M}_{1} mid mathcal{D} right)}{ operatorname{p} left( mathcal{M}_{2} mid mathcal{D} right)} frac{ operatorname{p} left( mathcal{M}_{2} right)}{ operatorname{p} left( mathcal{M}_{1} right)} end{equation} . | Bayesian model averaging doesn&#39;t select any single model but computes average weighted with each model&#39;s marginal posterior: $$p(y mid x, mathcal{D})= sum_{k=1}^{K} p left(y mid x, mathcal{D}, mathcal{M}_{k} right) p left( mathcal{M}_{k} mid mathcal{D} right),$$ where the weights have been marginalized as follows:$$ p left(y mid x, mathcal{D}, mathcal{M}_{k} right) = int p left(y mid x, w, mathcal{M}_k right) p(w mid mathcal{D}, mathcal{M}_k) dw. $$ . | Complex models are automatically penalized via Bayesian Occam&#39;s razor: the more parameters (with positive probability), the more the prior is spread over parameters (remember: it must integrate to 1) and the more unlikely the model becomes. . | Randomly initialized neural networks find different but equally performing local optima. A Bayesian model average is hence especially desirable for neural networks. . | Eliminates double descent (more parameters increase performance monotonically) . . | . . Wilson, Andrew Gordon, and Pavel Izmailov. &quot;Bayesian deep learning and a probabilistic perspective of generalization.&quot; arXiv preprint arXiv:2002.08791 (2020). . Many empirical studies find better test performance of BMAs (e.g. (Fortunato et al., 2019)). | . Bayesian weight decay and automatic relevance determination . . Source: Zoubin Ghahramani (University of Cambridge) Bayesian Deep Learning Workshop &quot;History of Bayesian Neural Networks (Keynote talk)&quot; https://www.youtube.com/watch?v=FD8l2vPU5FY&amp;feature=youtu.be . We want the model to automatically exclude features that aren&#39;t relevant and the predictions should be smooth. This helps generalization and also reduces transaction costs. | . Bayesian Regularization of Deep Neural Networks | . Adaptiveness . Change point estimation. | Random walk priors. non-stationary data. | Adaptive Bayesian Neural Network Based on Random Walk Priors. | . | Optimal updating. $p left(w mid mathcal{D}_{1} right)= frac{p left( mathcal{D}_{1} mid w right) p(w)}{ int p left( mathcal{D}_{1} mid w right) p(w) d w}$. | Set the prior to the previous posterior and recompute: $p left(w mid mathcal{D}_{2}, mathcal{D}_{1} right)= frac{p left( mathcal{D}_{2} mid w right) p left(w mid mathcal{D}_{1} right)}{ int p left( mathcal{D}_{2} mid w right) p left(w mid mathcal{D}_{1} right) d w}$. | . | Flexibility . Non-normality. Use heavy-tailed likelihood function with fitted tail parameter. | . | Heteroskedasticity. Use volatility model to account for volatility clusters. | Efficient Estimation of Predictive Models using High-frequency High-dimensional Data. | . | Encode domain expertise via priors and other inductive biases. . Hierarchical Bayesian Neural Network for Efficient High-dimensional Asset Return Prediction. | . | Glue together different components. . | . References . Fortunato, M., Blundell, C., &amp; Vinyals, O. (2019). Bayesian Recurrent Neural Networks. | .",
            "url": "https://jpwoeltjen.github.io/researchBlog/bayesian/deep%20learning/2020/11/09/financial_bayesian_dl.html",
            "relUrl": "/bayesian/deep%20learning/2020/11/09/financial_bayesian_dl.html",
            "date": " • Nov 9, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Bayesian Regularization of Deep Neural Networks",
            "content": "When fitting flexible models to noisy financial data we need to take special care to avoid overfitting. In addition to bad generalization and thus higher test-set loss, overfit models also increase transaction costs of trading algorithms based on them. Frequent changes in the mean return prediction lead to frequent changes in optimal portfolio weights and thus overtrading. Here we focus on the following two kinds of regularization: . promoting smoothness. | promoting sparsity in the input weights (or automatic feature selection). | Noisy data with lots of irrelevant features . Let&#39;s simulate some data. . 500 training observations | 1 relevant feature | 50 irrelevant features | observation noise | non-linear | . These data will be predicted via 3 different models. First, a deep ensemble is fitted with $ ell_1$ and $ ell_2$ regularization. Then a Bayesian neural network with Gaussian priors is tried. Finally, a Bayesian neural network with Gaussian priors and automatic relevance determination is shown to substatially improve generalization. This is achieved via inverse-gamma priors on the feature-specific variance of the Gaussian weight priors. . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Set matplotlib settings %matplotlib inline plt.style.use(&#39;default&#39;) %config InlineBackend.figure_format=&#39;retina&#39; %load_ext autoreload %autoreload 2 . n = 1000 X = np.sort(np.random.normal(0,1,n)) Y = 1*X**2 + 1*np.random.normal(0,1, n) . train_indexes = np.sort(np.random.choice(np.arange(n), n//2)) k = 50 irrelevant_features = np.random.normal(0,1,k*n).reshape(n,k) X = X[:,None] print(X.shape, irrelevant_features.shape) X = np.concatenate((X,irrelevant_features),1) X_train, Y_train = X[train_indexes], Y[train_indexes, None] X_test, Y_test = X[np.sort(~train_indexes)], Y[np.sort(~train_indexes), None] X_train.shape, Y_train.shape . (1000, 1) (1000, 50) . ((500, 51), (500, 1)) . plt.plot(X_train[:,0], Y_train.squeeze(), &#39;.&#39;, label=&#39;Train&#39;) plt.plot(X_test[:,0], Y_test.squeeze(), &#39;.&#39;, label=&#39;Test&#39;) plt.legend() plt.xlabel(&quot;$X_0$&quot;) plt.ylabel(&quot;$Y$&quot;) plt.title(&quot;Dependent variable vs. the only relevant feature $X_0$&quot;) plt.show(); . Deep enembles with PyTorch . We fit multiple neural networks with $ ell_1$ and $ ell_2$ penalties. The drawback of this approach is that the penalty parameter has to chosen ad hoc and selection must be based on validation set performance. Generally, the parameter needs to be adjusted to fit a complex vs. simple model. . import torch import torch.nn as nn import torch.nn.functional as F from torch import optim from tqdm import tqdm from collections import OrderedDict def fit_model(model, epochs, X, Y, X_valid, Y_valid): optimizer = optim.Adam(model.parameters(), weight_decay=1e-3, lr=1e-3) for epoch in (range(epochs)): # trainig mode model = model.train() model.zero_grad() Y_hat = model(X) loss = criterion(Y_hat, Y) # promote sparsity on input layer l1 = torch.sqrt(model.lin1.weight.abs().sum() + model.lin1.bias.abs().sum()) factor = 1e-3 loss += factor * l1 loss.backward() optimizer.step() if (epoch+1) % 100 ==0: with torch.no_grad(): model.eval() Y_hat = model(X_valid) def predict(model, X): with torch.no_grad(): model.eval() Y_hat = model(X) return Y_hat . print(&quot;GPU available:&quot;, torch.cuda.is_available()) device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) . GPU available: False . N = 20 n_neurons = 5 dropout = 0. epochs = 5000 criterion = nn.MSELoss() # Train set X_train_tensor = torch.tensor(X_train).float().to(device) Y_train_tensor = torch.tensor(Y_train).float().to(device) # Validation set X_test_tensor = torch.tensor(X_test).float().to(device) Y_test_tensor = torch.tensor(Y_test).float().to(device) n_features = X_train_tensor.shape[1] models = [nn.Sequential(OrderedDict([ (&#39;lin1&#39;, nn.Linear(n_features, n_neurons)), (&#39;bn1&#39;,nn.BatchNorm1d(n_neurons)), (&#39;relu1&#39;, nn.ReLU()), (&#39;lin2&#39;, nn.Linear(n_neurons, n_neurons)), (&#39;bn2&#39;,nn.BatchNorm1d(n_neurons)), (&#39;relu2&#39;, nn.ReLU()), (&#39;lin3&#39;, nn.Linear(n_neurons, n_neurons)), (&#39;bn3&#39;,nn.BatchNorm1d(n_neurons)), (&#39;relu3&#39;, nn.ReLU()), (&#39;lin4&#39;, nn.Linear(n_neurons, n_neurons)), (&#39;bn4&#39;,nn.BatchNorm1d(n_neurons)), (&#39;relu4&#39;, nn.ReLU()), (&#39;out&#39;, nn.Linear(n_neurons, 1)), ])) for i in range(N)] for i, model in tqdm(enumerate(models)): model.to(device) fit_model(model, epochs, X_train_tensor, Y_train_tensor, X_test_tensor, Y_test_tensor) . 20it [05:50, 17.54s/it] . y_hat_list = [] for model in models: y_hat_list.append(predict(model, X_test_tensor).cpu().numpy().squeeze()) df = pd.DataFrame(np.array(y_hat_list).T, index=X_test[:,0]) df.sort_index(inplace=True) df.plot(legend=False, alpha=0.05, c=&#39;b&#39;) plt.plot(X_test[:,0], Y_test.squeeze(), &#39;.&#39;, alpha=0.1, c=&#39;r&#39;) plt.xlabel(&quot;$X_0$&quot;) plt.ylabel(&quot;Y&quot;) plt.title(&quot;Samples from the posterior predictive distribution.&quot;) plt.show() . percentiles = df.quantile([0.05, 0.95], axis=1).T df.mean(1).plot(alpha=1, c=&#39;b&#39;) plt.plot(X_test[:,0], Y_test.squeeze(), &#39;.&#39;, alpha=0.1, c=&#39;r&#39;) plt.fill_between(X_test[:,0], percentiles.iloc[:,0], percentiles.iloc[:,1], color=&#39;lightblue&#39;) plt.xlabel(&quot;$X_0$&quot;) plt.ylabel(&quot;Y&quot;) plt.title(&quot;Mean predictions with 90% CI&quot;); . print(&quot;MSE:&quot;, ((df.mean(1) - Y_test.squeeze())**2).mean()) . MSE: 3.7483445671161286 . The test loss is bad. The mean prediction is very volatile. This will lead to inaccurate predictions and high transaction costs. . Numpyro . import os from functools import partial import torch import pyro import pyro.distributions as dist # for CI testing smoke_test = (&#39;CI&#39; in os.environ) assert pyro.__version__.startswith(&#39;1.5.0&#39;) pyro.enable_validation(True) pyro.set_rng_seed(1) pyro.enable_validation(True) . &quot;&quot;&quot; Bayesian Neural Network ======================= Adapted from https://github.com/pyro-ppl/numpyro/blob/master/examples/bnn.py &quot;&quot;&quot; import argparse import os import time import matplotlib import matplotlib.pyplot as plt import numpy as np import pandas as pd from jax import vmap import jax.numpy as jnp import jax.random as random import numpyro from numpyro import handlers import numpyro.distributions as dist from numpyro.infer import MCMC, NUTS args = pd.Series({&#39;num_samples&#39;:3000, &#39;num_warmup&#39;:1000, &#39;num_chains&#39;:1, &#39;num_hidden&#39;:5,&#39;device&#39;:&#39;cpu&#39;}) numpyro.set_platform(platform=args.device) if args.device == &#39;cpu&#39;: numpyro.set_host_device_count(args.num_chains) # # the non-linearity we use in our neural network def nonlin(x): return jnp.maximum(0, x) # helper function for HMC inference def run_inference(model, args, rng_key, X, Y, D_H): start = time.time() kernel = NUTS(model) mcmc = MCMC(kernel, args.num_warmup, args.num_samples, num_chains=args.num_chains, progress_bar=False if &quot;NUMPYRO_SPHINXBUILD&quot; in os.environ else True) mcmc.run(rng_key, X, Y, D_H) mcmc.print_summary() print(&#39; nMCMC elapsed time:&#39;, time.time() - start) return mcmc.get_samples() # helper function for prediction def predict(model, rng_key, samples, X, D_H): model = handlers.substitute(handlers.seed(model, rng_key), samples) # note that Y will be sampled in the model because we pass Y=None here model_trace = handlers.trace(model).get_trace(X=X, Y=None, D_H=D_H) return model_trace[&#39;Y&#39;][&#39;value&#39;] . Standard Bayesian neural network via NUTS . def model(X, Y, D_H): D_X, D_Y = X.shape[1], 1 w1 = numpyro.sample(&quot;w1&quot;, dist.Normal(jnp.zeros((D_X, D_H)), jnp.ones((D_X, D_H)))) # D_X D_H b1 = numpyro.sample(&quot;b1&quot;, dist.Normal(jnp.zeros((D_H)), jnp.ones((D_H)))) z1 = nonlin(jnp.matmul(X, w1) + b1) w2 = numpyro.sample(&quot;w2&quot;, dist.Normal(jnp.zeros((D_H, D_H)), jnp.ones((D_H, D_H)))) # D_H D_H b2 = numpyro.sample(&quot;b2&quot;, dist.Normal(jnp.zeros((D_H)), jnp.ones((D_H)))) z2 = nonlin(jnp.matmul(z1, w2) + b2) w3 = numpyro.sample(&quot;w3&quot;, dist.Normal(jnp.zeros((D_H, D_H)), jnp.ones((D_H, D_H)))) # D_H D_H b3 = numpyro.sample(&quot;b3&quot;, dist.Normal(jnp.zeros((D_H)), jnp.ones((D_H)))) z3 = nonlin(jnp.matmul(z2, w3) + b3) w4 = numpyro.sample(&quot;w4&quot;, dist.Normal(jnp.zeros((D_H, D_H)), jnp.ones((D_H, D_H)))) # D_H D_H b4 = numpyro.sample(&quot;b4&quot;, dist.Normal(jnp.zeros((D_H)), jnp.ones((D_H)))) z4 = nonlin(jnp.matmul(z3, w4) + b4) w5 = numpyro.sample(&quot;w5&quot;, dist.Normal(jnp.zeros((D_H, D_Y)), jnp.ones((D_H, D_Y)))) # D_H D_Y b5 = numpyro.sample(&quot;b5&quot;, dist.Normal(jnp.zeros((D_Y)), jnp.ones((D_Y)))) z5 = jnp.matmul(z4, w5) + b5 # we put a prior on the observation noise # here we could trivially account for heteroskedasticity. prec_obs = numpyro.sample(&quot;prec_obs&quot;, dist.Gamma(3.0, 1.0)) sigma_obs = 1.0 / jnp.sqrt(prec_obs) # observe data numpyro.sample(&quot;Y&quot;, dist.Normal(z5, sigma_obs), obs=Y) . N, D_H = len(X_train), args.num_hidden # do inference rng_key, rng_key_predict = random.split(random.PRNGKey(0)) samples = run_inference(model, args, rng_key, X_train, Y_train, D_H) . sample: 100%|██████████| 4000/4000 [16:51&lt;00:00, 3.96it/s, 1023 steps of size 1.10e-03. acc. prob=0.73] . mean std median 5.0% 95.0% n_eff r_hat b1[0] -0.44 0.38 -0.40 -0.97 0.17 10.54 1.02 b1[1] 1.17 0.23 1.18 0.78 1.52 19.96 1.02 b1[2] -0.38 0.19 -0.39 -0.67 -0.05 38.04 1.01 b1[3] -1.65 0.22 -1.63 -1.98 -1.29 24.75 1.00 b1[4] -0.57 0.29 -0.56 -1.03 -0.09 18.05 1.00 b2[0] -2.28 0.50 -2.23 -3.05 -1.41 54.41 1.04 b2[1] -1.72 0.46 -1.70 -2.37 -0.81 37.70 1.03 b2[2] 1.83 0.37 1.83 1.24 2.40 32.02 1.02 b2[3] -1.70 0.51 -1.67 -2.60 -0.95 20.09 1.00 b2[4] 0.34 0.16 0.31 0.15 0.51 21.94 1.04 b3[0] -1.84 0.49 -1.79 -2.65 -1.10 25.37 1.12 b3[1] 1.16 0.25 1.14 0.77 1.57 53.15 1.00 b3[2] 0.58 0.20 0.55 0.27 0.84 27.03 1.00 b3[3] -1.81 0.43 -1.76 -2.43 -1.16 16.11 1.29 b3[4] -1.54 0.45 -1.50 -2.25 -0.80 26.51 1.08 b4[0] -0.18 0.41 -0.15 -0.83 0.40 80.21 1.02 b4[1] -0.17 0.34 -0.20 -0.74 0.39 20.64 1.01 b4[2] -0.44 0.17 -0.42 -0.69 -0.20 62.28 1.04 b4[3] 1.40 0.30 1.37 0.86 1.83 19.44 1.11 b4[4] -0.35 0.61 -0.26 -1.31 0.69 49.00 1.09 b5[0] -2.32 0.20 -2.29 -2.60 -2.00 21.25 1.05 prec_obs 18.74 2.71 18.74 13.96 23.12 54.74 1.03 w1[0,0] -2.96 0.41 -2.90 -3.72 -2.34 20.35 1.04 w1[0,1] 0.31 0.13 0.30 0.13 0.55 34.93 1.14 w1[0,2] -1.89 0.21 -1.90 -2.18 -1.50 10.06 1.16 w1[0,3] 0.67 0.13 0.66 0.44 0.85 18.91 1.06 w1[0,4] -0.18 0.15 -0.18 -0.41 0.06 30.52 1.03 w1[1,0] 1.13 0.17 1.12 0.85 1.41 9.74 1.18 w1[1,1] -0.48 0.17 -0.46 -0.78 -0.25 6.58 1.03 w1[1,2] -0.54 0.09 -0.54 -0.67 -0.39 25.02 1.01 w1[1,3] -0.48 0.17 -0.48 -0.75 -0.17 8.46 1.00 w1[1,4] 0.58 0.15 0.58 0.34 0.82 27.07 1.04 w1[2,0] 1.03 0.21 1.01 0.68 1.39 11.83 1.19 w1[2,1] 0.92 0.18 0.93 0.61 1.19 5.99 1.40 w1[2,2] -0.66 0.11 -0.65 -0.82 -0.48 9.17 1.21 w1[2,3] 0.33 0.14 0.33 0.11 0.58 13.36 1.00 w1[2,4] -0.68 0.17 -0.68 -0.94 -0.39 40.50 1.00 w1[3,0] -0.65 0.19 -0.65 -0.97 -0.35 8.45 1.33 w1[3,1] -0.11 0.12 -0.11 -0.30 0.08 9.25 1.29 w1[3,2] -0.07 0.13 -0.08 -0.30 0.12 9.55 1.38 w1[3,3] -0.28 0.10 -0.27 -0.44 -0.11 22.46 1.00 w1[3,4] 1.57 0.20 1.56 1.25 1.89 39.51 1.00 w1[4,0] 0.84 0.15 0.83 0.62 1.07 26.18 1.03 w1[4,1] 0.49 0.11 0.48 0.30 0.65 29.37 1.16 w1[4,2] 0.33 0.09 0.34 0.18 0.49 26.25 1.05 w1[4,3] -0.71 0.16 -0.71 -0.95 -0.43 10.43 1.07 w1[4,4] 1.30 0.21 1.28 0.95 1.62 6.11 1.66 w1[5,0] 0.87 0.18 0.85 0.61 1.19 14.29 1.11 w1[5,1] 1.54 0.28 1.55 1.10 2.02 5.70 1.53 w1[5,2] -2.10 0.21 -2.12 -2.38 -1.71 10.60 1.07 w1[5,3] -0.84 0.21 -0.82 -1.16 -0.51 4.44 1.66 w1[5,4] -0.12 0.23 -0.09 -0.49 0.28 12.91 1.13 w1[6,0] -0.44 0.14 -0.43 -0.67 -0.21 5.56 1.51 w1[6,1] 1.52 0.23 1.51 1.11 1.84 8.99 1.27 w1[6,2] -2.08 0.17 -2.08 -2.32 -1.78 33.36 1.00 w1[6,3] 1.27 0.20 1.25 0.96 1.60 11.13 1.45 w1[6,4] -0.48 0.16 -0.49 -0.73 -0.19 16.20 1.02 w1[7,0] 0.24 0.15 0.22 0.01 0.51 8.19 1.07 w1[7,1] -1.25 0.16 -1.23 -1.53 -0.98 19.62 1.10 w1[7,2] -0.26 0.10 -0.25 -0.41 -0.09 9.19 1.24 w1[7,3] 0.16 0.10 0.15 -0.01 0.32 11.53 1.19 w1[7,4] -0.78 0.19 -0.79 -1.08 -0.47 34.85 1.01 w1[8,0] 1.98 0.25 1.95 1.60 2.40 22.65 1.02 w1[8,1] 0.13 0.09 0.13 -0.02 0.28 37.09 1.06 w1[8,2] 0.05 0.12 0.06 -0.12 0.25 8.84 1.26 w1[8,3] 0.70 0.15 0.71 0.47 0.94 5.94 1.49 w1[8,4] -0.05 0.14 -0.04 -0.29 0.16 11.50 1.12 w1[9,0] 0.12 0.13 0.10 -0.08 0.35 6.39 1.36 w1[9,1] 0.25 0.12 0.26 0.04 0.44 25.84 1.01 w1[9,2] 1.16 0.14 1.14 0.93 1.36 5.23 1.46 w1[9,3] -0.87 0.12 -0.88 -1.05 -0.67 27.55 1.13 w1[9,4] -0.22 0.22 -0.20 -0.47 0.12 15.49 1.00 w1[10,0] -1.64 0.23 -1.63 -2.02 -1.30 11.39 1.24 w1[10,1] 0.02 0.15 0.01 -0.22 0.29 6.26 1.40 w1[10,2] 0.37 0.08 0.37 0.25 0.51 53.13 1.02 w1[10,3] 0.40 0.14 0.39 0.19 0.62 13.94 1.02 w1[10,4] 0.15 0.17 0.15 -0.11 0.43 13.22 1.01 w1[11,0] -0.83 0.14 -0.82 -1.09 -0.62 28.91 1.09 w1[11,1] -0.17 0.11 -0.17 -0.33 0.03 32.72 1.00 w1[11,2] -0.45 0.11 -0.44 -0.63 -0.26 12.58 1.12 w1[11,3] 1.79 0.21 1.79 1.45 2.16 19.56 1.00 w1[11,4] -0.55 0.16 -0.55 -0.79 -0.29 25.89 1.23 w1[12,0] -0.57 0.20 -0.53 -0.94 -0.27 8.68 1.02 w1[12,1] 0.52 0.12 0.52 0.33 0.72 26.08 1.00 w1[12,2] -0.87 0.13 -0.85 -1.08 -0.67 19.52 1.04 w1[12,3] 0.91 0.16 0.90 0.63 1.14 7.21 1.43 w1[12,4] -0.25 0.14 -0.24 -0.49 -0.01 23.59 1.01 w1[13,0] -1.32 0.23 -1.32 -1.70 -0.94 10.02 1.09 w1[13,1] 0.53 0.17 0.55 0.24 0.80 5.26 1.50 w1[13,2] -1.15 0.17 -1.15 -1.42 -0.87 7.73 1.12 w1[13,3] 1.52 0.15 1.53 1.28 1.76 34.04 1.00 w1[13,4] -0.45 0.17 -0.45 -0.73 -0.19 11.61 1.29 w1[14,0] -0.26 0.14 -0.27 -0.48 -0.04 12.84 1.01 w1[14,1] -1.71 0.20 -1.67 -2.04 -1.41 12.72 1.20 w1[14,2] 1.00 0.15 0.99 0.76 1.27 7.45 1.09 w1[14,3] 0.79 0.13 0.77 0.58 1.01 17.10 1.13 w1[14,4] -1.10 0.16 -1.09 -1.34 -0.82 37.49 1.11 w1[15,0] -0.35 0.16 -0.34 -0.61 -0.11 6.68 1.11 w1[15,1] -0.49 0.21 -0.48 -0.87 -0.16 3.96 1.69 w1[15,2] 0.61 0.14 0.59 0.39 0.86 5.65 1.27 w1[15,3] 2.18 0.19 2.16 1.87 2.47 29.19 1.16 w1[15,4] 0.40 0.20 0.42 0.07 0.70 12.94 1.39 w1[16,0] 0.13 0.13 0.15 -0.10 0.33 10.52 1.26 w1[16,1] 1.24 0.22 1.23 0.90 1.59 5.36 1.78 w1[16,2] 0.03 0.10 0.03 -0.14 0.19 34.44 1.00 w1[16,3] -0.16 0.12 -0.16 -0.36 0.02 41.79 1.01 w1[16,4] 0.76 0.18 0.74 0.45 1.05 24.19 1.13 w1[17,0] -0.11 0.15 -0.09 -0.36 0.11 11.93 1.00 w1[17,1] -1.46 0.18 -1.47 -1.73 -1.14 23.20 1.03 w1[17,2] -0.01 0.09 -0.01 -0.16 0.15 19.38 1.17 w1[17,3] -0.44 0.13 -0.43 -0.65 -0.23 7.77 1.23 w1[17,4] -0.57 0.16 -0.57 -0.80 -0.30 30.65 1.13 w1[18,0] 0.23 0.19 0.20 -0.05 0.53 11.99 1.10 w1[18,1] -0.37 0.12 -0.37 -0.57 -0.18 20.44 1.06 w1[18,2] -1.76 0.13 -1.76 -1.97 -1.55 29.97 1.00 w1[18,3] 0.90 0.13 0.91 0.67 1.11 23.90 1.00 w1[18,4] -0.79 0.20 -0.78 -1.13 -0.49 26.13 1.25 w1[19,0] -0.80 0.16 -0.79 -1.06 -0.55 17.98 1.08 w1[19,1] 0.96 0.16 0.94 0.70 1.21 27.63 1.02 w1[19,2] 0.48 0.09 0.48 0.31 0.62 17.50 1.11 w1[19,3] -0.33 0.11 -0.33 -0.49 -0.14 28.61 1.01 w1[19,4] 1.72 0.23 1.70 1.35 2.10 19.35 1.10 w1[20,0] -0.33 0.14 -0.32 -0.53 -0.11 25.27 1.03 w1[20,1] -0.97 0.20 -0.96 -1.30 -0.66 8.87 1.34 w1[20,2] -0.70 0.12 -0.68 -0.90 -0.50 6.54 1.39 w1[20,3] 1.99 0.16 1.99 1.75 2.23 40.49 1.02 w1[20,4] -0.19 0.20 -0.19 -0.53 0.11 11.22 1.04 w1[21,0] 0.90 0.20 0.87 0.61 1.30 10.73 1.05 w1[21,1] -0.67 0.20 -0.68 -1.01 -0.34 9.48 1.30 w1[21,2] -1.25 0.17 -1.23 -1.54 -0.95 4.38 1.64 w1[21,3] -0.83 0.15 -0.83 -1.07 -0.58 6.17 1.44 w1[21,4] 2.00 0.25 1.99 1.59 2.44 34.31 1.08 w1[22,0] -1.02 0.17 -1.02 -1.29 -0.73 6.74 1.38 w1[22,1] 1.28 0.20 1.28 0.97 1.61 9.17 1.08 w1[22,2] 0.20 0.13 0.19 -0.02 0.42 7.63 1.23 w1[22,3] -2.16 0.20 -2.17 -2.47 -1.79 38.26 1.00 w1[22,4] -1.11 0.17 -1.12 -1.40 -0.83 38.67 1.09 w1[23,0] 1.03 0.25 0.99 0.63 1.39 9.10 1.10 w1[23,1] 0.02 0.13 0.02 -0.19 0.23 13.22 1.03 w1[23,2] -0.51 0.11 -0.50 -0.67 -0.32 13.97 1.09 w1[23,3] -0.31 0.10 -0.31 -0.47 -0.16 30.15 1.08 w1[23,4] 1.98 0.25 1.96 1.61 2.39 25.13 1.03 w1[24,0] 1.30 0.20 1.30 0.95 1.64 24.78 1.08 w1[24,1] -0.01 0.10 -0.01 -0.16 0.16 66.38 1.00 w1[24,2] 0.07 0.11 0.06 -0.13 0.23 12.33 1.00 w1[24,3] -0.63 0.14 -0.62 -0.84 -0.40 22.35 1.01 w1[24,4] 0.50 0.19 0.49 0.17 0.80 15.19 1.07 w1[25,0] -0.18 0.20 -0.20 -0.46 0.19 5.51 1.44 w1[25,1] 0.37 0.12 0.36 0.17 0.56 12.28 1.23 w1[25,2] 1.47 0.15 1.46 1.23 1.70 15.32 1.04 w1[25,3] 0.76 0.13 0.76 0.53 0.96 11.18 1.19 w1[25,4] 0.36 0.16 0.36 0.10 0.60 24.21 1.11 w1[26,0] 0.66 0.14 0.66 0.44 0.90 17.82 1.01 w1[26,1] -0.22 0.16 -0.23 -0.46 0.06 9.53 1.03 w1[26,2] 0.02 0.12 0.02 -0.17 0.22 6.51 1.37 w1[26,3] -0.07 0.13 -0.06 -0.28 0.14 10.37 1.03 w1[26,4] -0.47 0.17 -0.46 -0.74 -0.19 23.61 1.11 w1[27,0] 2.03 0.26 2.01 1.60 2.41 21.65 1.03 w1[27,1] -1.29 0.16 -1.28 -1.56 -1.04 20.19 1.02 w1[27,2] 0.58 0.11 0.58 0.40 0.74 16.17 1.01 w1[27,3] -0.36 0.13 -0.36 -0.59 -0.17 21.33 1.03 w1[27,4] -2.42 0.27 -2.39 -2.86 -1.98 25.33 1.15 w1[28,0] -1.07 0.17 -1.05 -1.35 -0.79 14.66 1.02 w1[28,1] 1.59 0.21 1.59 1.22 1.90 6.29 1.45 w1[28,2] 0.22 0.14 0.19 0.04 0.50 4.49 1.67 w1[28,3] 0.01 0.15 0.00 -0.22 0.27 5.77 1.11 w1[28,4] 0.66 0.15 0.66 0.39 0.90 31.69 1.09 w1[29,0] -0.36 0.11 -0.35 -0.54 -0.17 29.59 1.00 w1[29,1] 1.00 0.18 1.01 0.71 1.28 8.25 1.14 w1[29,2] 1.35 0.12 1.34 1.16 1.53 21.66 1.05 w1[29,3] 1.22 0.18 1.19 0.94 1.55 6.77 1.27 w1[29,4] 0.10 0.14 0.11 -0.13 0.33 8.14 1.40 w1[30,0] -0.25 0.12 -0.25 -0.44 -0.06 66.16 1.00 w1[30,1] 0.91 0.18 0.90 0.64 1.20 7.53 1.27 w1[30,2] 0.76 0.13 0.77 0.53 0.95 9.20 1.00 w1[30,3] 1.06 0.12 1.05 0.87 1.25 27.15 1.07 w1[30,4] -0.14 0.16 -0.12 -0.37 0.14 20.45 1.01 w1[31,0] 0.55 0.14 0.53 0.34 0.79 17.42 1.01 w1[31,1] -0.52 0.17 -0.52 -0.76 -0.24 7.66 1.32 w1[31,2] -0.51 0.09 -0.51 -0.66 -0.36 28.29 1.02 w1[31,3] -0.41 0.09 -0.41 -0.56 -0.27 51.39 1.01 w1[31,4] -0.96 0.25 -0.98 -1.34 -0.54 6.47 1.28 w1[32,0] -0.76 0.17 -0.76 -1.02 -0.48 10.46 1.14 w1[32,1] 0.20 0.14 0.20 -0.02 0.42 7.66 1.04 w1[32,2] 1.52 0.17 1.52 1.26 1.81 6.19 1.39 w1[32,3] 0.20 0.12 0.18 0.03 0.43 6.14 1.14 w1[32,4] 0.37 0.15 0.36 0.14 0.64 28.05 1.02 w1[33,0] 1.16 0.16 1.16 0.89 1.41 31.28 1.00 w1[33,1] -0.82 0.17 -0.81 -1.10 -0.53 23.45 1.00 w1[33,2] -0.23 0.13 -0.22 -0.44 -0.03 7.72 1.19 w1[33,3] -0.07 0.10 -0.07 -0.22 0.09 23.90 1.01 w1[33,4] -1.86 0.28 -1.84 -2.32 -1.43 17.30 1.01 w1[34,0] -0.47 0.20 -0.47 -0.80 -0.15 12.99 1.05 w1[34,1] 0.42 0.12 0.42 0.23 0.60 11.02 1.28 w1[34,2] 1.72 0.17 1.71 1.45 1.98 13.45 1.02 w1[34,3] 0.41 0.11 0.42 0.22 0.58 13.07 1.06 w1[34,4] 1.24 0.21 1.24 0.88 1.56 28.50 1.10 w1[35,0] -1.11 0.14 -1.11 -1.35 -0.89 31.76 1.03 w1[35,1] 1.05 0.18 1.03 0.74 1.33 8.79 1.14 w1[35,2] 0.97 0.14 0.98 0.73 1.18 10.11 1.08 w1[35,3] 1.08 0.15 1.10 0.83 1.31 11.07 1.20 w1[35,4] 0.19 0.16 0.19 -0.08 0.43 11.32 1.08 w1[36,0] 0.25 0.17 0.26 -0.03 0.52 4.74 1.64 w1[36,1] -1.08 0.14 -1.07 -1.30 -0.87 38.11 1.00 w1[36,2] -0.20 0.11 -0.22 -0.37 -0.03 16.23 1.00 w1[36,3] -0.13 0.13 -0.13 -0.36 0.04 14.85 1.06 w1[36,4] -0.60 0.18 -0.59 -0.90 -0.31 12.72 1.26 w1[37,0] -0.71 0.14 -0.70 -0.95 -0.50 21.16 1.01 w1[37,1] -1.19 0.22 -1.17 -1.55 -0.83 10.78 1.18 w1[37,2] 1.07 0.12 1.06 0.89 1.27 11.17 1.24 w1[37,3] 0.03 0.11 0.04 -0.15 0.20 10.32 1.24 w1[37,4] 0.32 0.15 0.31 0.08 0.56 29.01 1.02 w1[38,0] -0.98 0.20 -0.98 -1.29 -0.66 12.73 1.02 w1[38,1] -0.27 0.12 -0.27 -0.47 -0.07 24.83 1.12 w1[38,2] 1.16 0.16 1.17 0.90 1.43 7.76 1.32 w1[38,3] 0.02 0.10 0.03 -0.16 0.20 14.06 1.03 w1[38,4] -0.38 0.15 -0.39 -0.62 -0.13 14.77 1.12 w1[39,0] 0.05 0.17 0.06 -0.22 0.35 11.29 1.25 w1[39,1] -2.04 0.33 -1.98 -2.61 -1.56 13.02 1.02 w1[39,2] 0.27 0.12 0.26 0.06 0.46 8.16 1.11 w1[39,3] -0.33 0.13 -0.35 -0.51 -0.09 16.93 1.00 w1[39,4] 0.76 0.16 0.76 0.49 1.02 25.06 1.06 w1[40,0] -0.17 0.24 -0.22 -0.50 0.27 5.08 1.08 w1[40,1] -0.90 0.14 -0.89 -1.12 -0.67 36.81 1.01 w1[40,2] 0.90 0.11 0.89 0.70 1.08 10.43 1.17 w1[40,3] -0.90 0.14 -0.89 -1.12 -0.67 18.84 1.06 w1[40,4] -0.38 0.15 -0.38 -0.65 -0.14 21.42 1.16 w1[41,0] 1.19 0.21 1.19 0.86 1.51 5.24 1.46 w1[41,1] -1.84 0.23 -1.82 -2.25 -1.48 18.10 1.05 w1[41,2] 1.40 0.15 1.42 1.13 1.64 10.53 1.13 w1[41,3] -0.11 0.13 -0.11 -0.33 0.08 10.73 1.03 w1[41,4] -1.15 0.19 -1.15 -1.48 -0.83 17.16 1.03 w1[42,0] 0.93 0.15 0.92 0.69 1.20 22.48 1.14 w1[42,1] -0.87 0.24 -0.86 -1.27 -0.50 3.69 1.86 w1[42,2] -1.56 0.17 -1.56 -1.83 -1.31 22.71 1.06 w1[42,3] 1.77 0.18 1.76 1.49 2.08 13.79 1.19 w1[42,4] 1.47 0.24 1.46 1.06 1.85 11.55 1.01 w1[43,0] 1.12 0.19 1.12 0.80 1.40 5.24 1.52 w1[43,1] -0.63 0.13 -0.63 -0.83 -0.43 9.76 1.23 w1[43,2] -0.47 0.13 -0.45 -0.68 -0.26 13.57 1.17 w1[43,3] -1.63 0.15 -1.63 -1.86 -1.40 44.72 1.01 w1[43,4] 0.36 0.15 0.35 0.11 0.60 58.86 1.04 w1[44,0] -0.77 0.11 -0.76 -0.96 -0.59 43.33 1.00 w1[44,1] -0.32 0.17 -0.32 -0.58 -0.01 9.65 1.27 w1[44,2] 0.74 0.10 0.73 0.59 0.91 15.02 1.10 w1[44,3] 0.50 0.11 0.50 0.32 0.68 19.12 1.04 w1[44,4] 1.94 0.27 1.92 1.53 2.43 29.52 1.04 w1[45,0] 0.81 0.15 0.81 0.58 1.05 21.11 1.06 w1[45,1] -1.09 0.15 -1.08 -1.35 -0.85 12.48 1.07 w1[45,2] -0.71 0.12 -0.69 -0.90 -0.51 10.71 1.09 w1[45,3] 0.75 0.18 0.74 0.46 1.04 8.12 1.20 w1[45,4] -1.12 0.24 -1.13 -1.52 -0.78 17.82 1.21 w1[46,0] -1.31 0.17 -1.31 -1.56 -1.02 38.17 1.05 w1[46,1] 0.89 0.14 0.88 0.64 1.11 26.80 1.03 w1[46,2] -0.82 0.10 -0.83 -0.99 -0.65 10.26 1.25 w1[46,3] 1.32 0.16 1.32 1.04 1.56 15.81 1.01 w1[46,4] 0.79 0.21 0.80 0.47 1.14 7.81 1.18 w1[47,0] 0.28 0.11 0.27 0.10 0.47 31.06 1.00 w1[47,1] 1.30 0.18 1.29 1.01 1.58 15.75 1.00 w1[47,2] 1.91 0.15 1.90 1.65 2.16 27.48 1.00 w1[47,3] 1.68 0.18 1.68 1.37 1.98 10.98 1.23 w1[47,4] 0.70 0.16 0.69 0.42 0.96 33.93 1.10 w1[48,0] -0.23 0.17 -0.25 -0.50 0.05 5.73 1.59 w1[48,1] -1.39 0.19 -1.37 -1.70 -1.07 16.34 1.03 w1[48,2] -1.53 0.18 -1.55 -1.82 -1.24 8.74 1.06 w1[48,3] -0.02 0.10 -0.02 -0.18 0.14 45.88 1.00 w1[48,4] 1.01 0.19 0.99 0.74 1.33 12.32 1.30 w1[49,0] -0.71 0.13 -0.71 -0.91 -0.50 31.28 1.01 w1[49,1] 0.03 0.15 0.04 -0.23 0.24 13.42 1.05 w1[49,2] -0.55 0.12 -0.55 -0.73 -0.34 11.64 1.05 w1[49,3] 0.05 0.13 0.06 -0.14 0.27 7.43 1.33 w1[49,4] -0.07 0.18 -0.06 -0.36 0.23 10.85 1.36 w1[50,0] -0.87 0.18 -0.86 -1.19 -0.59 9.85 1.19 w1[50,1] -0.28 0.17 -0.28 -0.56 0.01 9.58 1.02 w1[50,2] 1.15 0.15 1.13 0.91 1.38 11.04 1.23 w1[50,3] -0.91 0.16 -0.92 -1.16 -0.63 20.38 1.01 w1[50,4] 0.06 0.16 0.06 -0.19 0.34 20.55 1.01 w2[0,0] -1.05 0.61 -0.93 -2.01 -0.24 25.48 1.07 w2[0,1] 0.22 0.07 0.22 0.12 0.34 25.82 1.05 w2[0,2] 0.66 0.17 0.63 0.40 0.93 24.43 1.01 w2[0,3] 0.69 0.21 0.68 0.38 1.08 17.41 1.02 w2[0,4] 0.26 0.08 0.25 0.14 0.38 30.97 1.04 w2[1,0] 0.35 0.10 0.34 0.19 0.51 7.89 1.31 w2[1,1] -0.78 0.21 -0.76 -1.09 -0.38 35.82 1.01 w2[1,2] 1.19 0.27 1.17 0.74 1.58 19.43 1.04 w2[1,3] -0.34 0.12 -0.33 -0.52 -0.13 17.31 1.00 w2[1,4] 1.09 0.25 1.06 0.65 1.47 39.88 1.03 w2[2,0] -1.04 0.53 -0.97 -1.89 -0.22 28.42 1.15 w2[2,1] 0.72 0.19 0.70 0.41 1.05 40.72 1.01 w2[2,2] -1.21 0.29 -1.20 -1.65 -0.73 14.89 1.00 w2[2,3] 0.13 0.05 0.12 0.05 0.20 16.29 1.00 w2[2,4] 0.12 0.05 0.12 0.04 0.21 5.36 1.39 w2[3,0] -1.20 0.51 -1.12 -1.96 -0.41 53.20 1.05 w2[3,1] 0.23 0.09 0.22 0.09 0.36 28.30 1.01 w2[3,2] -0.21 0.05 -0.21 -0.29 -0.12 34.68 1.09 w2[3,3] 0.66 0.21 0.64 0.31 1.02 16.10 1.01 w2[3,4] -1.59 0.40 -1.52 -2.30 -1.00 24.46 1.09 w2[4,0] -0.49 0.16 -0.47 -0.75 -0.24 7.35 1.44 w2[4,1] 0.04 0.03 0.04 -0.01 0.09 33.19 1.00 w2[4,2] -1.17 0.24 -1.16 -1.58 -0.77 27.72 1.09 w2[4,3] 0.10 0.03 0.10 0.04 0.15 23.91 1.00 w2[4,4] -0.80 0.20 -0.79 -1.09 -0.44 42.81 1.10 w3[0,0] -1.36 0.64 -1.35 -2.43 -0.37 20.74 1.00 w3[0,1] 0.17 0.98 0.15 -1.34 1.84 194.86 1.00 w3[0,2] -1.24 0.99 -1.29 -2.82 0.27 42.50 1.00 w3[0,3] 1.08 1.02 1.09 -0.87 2.58 66.84 1.05 w3[0,4] 0.49 0.92 0.55 -1.20 1.85 69.85 1.02 w3[1,0] -0.19 0.09 -0.18 -0.33 -0.04 53.16 1.01 w3[1,1] 1.32 0.45 1.26 0.59 1.97 35.14 1.00 w3[1,2] -0.73 0.38 -0.62 -1.31 -0.20 9.06 1.30 w3[1,3] 0.64 0.24 0.60 0.28 0.98 37.53 1.00 w3[1,4] 0.53 0.22 0.49 0.22 0.82 20.84 1.01 w3[2,0] -0.31 0.13 -0.28 -0.48 -0.14 22.55 1.02 w3[2,1] -0.34 0.12 -0.32 -0.51 -0.15 24.22 1.03 w3[2,2] 1.56 0.39 1.54 0.87 2.16 47.39 1.00 w3[2,3] 1.87 0.67 1.70 0.94 3.00 20.37 1.14 w3[2,4] 0.99 0.30 0.96 0.53 1.48 42.45 1.04 w3[3,0] 0.57 0.21 0.54 0.25 0.87 32.46 1.00 w3[3,1] -0.69 0.25 -0.64 -1.07 -0.35 24.51 1.00 w3[3,2] -0.91 0.35 -0.85 -1.44 -0.27 17.25 1.04 w3[3,3] -0.82 0.41 -0.71 -1.30 -0.28 10.16 1.03 w3[3,4] -0.24 0.12 -0.22 -0.41 -0.07 18.18 1.00 w3[4,0] 1.11 0.31 1.08 0.60 1.62 50.48 1.00 w3[4,1] 1.32 0.37 1.28 0.71 1.82 46.96 1.05 w3[4,2] 0.24 0.12 0.23 0.07 0.42 29.90 1.00 w3[4,3] 0.62 0.25 0.57 0.26 0.95 10.28 1.28 w3[4,4] -0.82 0.30 -0.77 -1.28 -0.36 33.97 1.04 w4[0,0] 0.01 0.23 0.02 -0.39 0.34 63.86 1.01 w4[0,1] -2.05 0.60 -2.04 -2.91 -0.95 105.89 1.01 w4[0,2] 0.19 0.16 0.16 -0.06 0.43 4.89 1.62 w4[0,3] 1.02 0.34 0.98 0.47 1.54 24.09 1.01 w4[0,4] 1.04 0.48 0.97 0.28 1.72 59.97 1.01 w4[1,0] -0.02 0.19 -0.02 -0.35 0.27 23.66 1.00 w4[1,1] 0.31 0.14 0.30 0.10 0.52 36.23 1.06 w4[1,2] 0.43 0.12 0.42 0.24 0.62 35.78 1.01 w4[1,3] -2.13 0.55 -2.06 -3.13 -1.32 37.57 1.09 w4[1,4] -0.09 0.21 -0.06 -0.44 0.23 17.99 1.01 w4[2,0] 1.46 0.50 1.40 0.65 2.24 98.94 1.03 w4[2,1] -0.59 0.26 -0.56 -1.00 -0.18 30.75 1.13 w4[2,2] 1.13 0.37 1.08 0.64 1.72 21.23 1.22 w4[2,3] 0.60 0.17 0.58 0.32 0.85 44.09 1.01 w4[2,4] -1.43 0.55 -1.34 -2.28 -0.59 50.19 1.02 w4[3,0] -0.86 0.37 -0.80 -1.46 -0.29 60.83 1.05 w4[3,1] 0.91 0.45 0.82 0.29 1.67 14.21 1.33 w4[3,2] -2.75 0.64 -2.76 -3.76 -1.69 45.64 1.04 w4[3,3] -0.43 0.18 -0.41 -0.69 -0.14 27.22 1.17 w4[3,4] 0.02 0.32 0.03 -0.46 0.49 50.79 1.02 w4[4,0] -1.27 0.52 -1.19 -2.06 -0.44 74.50 1.03 w4[4,1] -0.30 0.16 -0.27 -0.54 -0.04 46.45 1.10 w4[4,2] -1.24 0.50 -1.14 -2.02 -0.53 19.22 1.09 w4[4,3] -0.69 0.28 -0.66 -1.13 -0.26 33.88 1.00 w4[4,4] -0.37 0.87 -0.34 -1.75 1.07 43.67 1.06 w5[0,0] 0.47 0.21 0.42 0.19 0.75 69.28 1.03 w5[1,0] 0.88 0.34 0.81 0.38 1.31 43.09 1.08 w5[2,0] 2.50 0.49 2.45 1.70 3.28 68.59 1.08 w5[3,0] 2.14 0.45 2.11 1.37 2.81 12.77 1.19 w5[4,0] 0.54 0.24 0.48 0.17 0.88 32.71 1.00 Number of divergences: 1 MCMC elapsed time: 1018.3158366680145 . How does it scale with n? . 500: sample: 100%|██████████| 1000/1000 [02:21&lt;00:00, 7.07it/s, 1023 steps of size 2.79e-03. acc. prob=0.75] . 5000: sample: 100%|██████████| 1000/1000 [13:40&lt;00:00, 1.22it/s, 1023 steps of size 1.30e-03. acc. prob=0.78] . 50000: sample: 100%|██████████| 1000/1000 [1:49:49&lt;00:00, 6.59s/it, 1023 steps of size 4.97e-04. acc. prob=0.92] . # predict Y (test) at inputs X vmap_args = (samples, random.split(rng_key_predict, args.num_samples * args.num_chains)) predictions = vmap(lambda samples, rng_key: predict(model, rng_key, samples, X_train, D_H))(*vmap_args) predictions = predictions[..., 0] # compute mean prediction and confidence interval around median mean_prediction = jnp.mean(predictions, axis=0) percentiles = np.percentile(predictions, [5.0, 95.0], axis=0) fig, ax = plt.subplots(1, 1) # plot training data ax.plot(X_train[:, 0], Y_train[:, 0], &#39;kx&#39;) # plot 90% credible interval of predictions ax.fill_between(X_train[:,0], percentiles[0, :], percentiles[1, :], color=&#39;lightblue&#39;) # plot mean prediction ax.plot(X_train[:,0], mean_prediction, &#39;blue&#39;, ls=&#39;solid&#39;, lw=2.0) ax.set(xlabel=&quot;$X_0$&quot;, ylabel=&quot;Y&quot;, title=&quot;Mean predictions on train set with 90% CI&quot;) plt.tight_layout() plt.show() . # predict Y (test) at inputs X vmap_args = (samples, random.split(rng_key_predict, args.num_samples * args.num_chains)) predictions = vmap(lambda samples, rng_key: predict(model, rng_key, samples, X_test, D_H))(*vmap_args) predictions = predictions[..., 0] # compute mean prediction and confidence interval around median mean_prediction = jnp.mean(predictions, axis=0) percentiles = np.percentile(predictions, [5.0, 95.0], axis=0) . fig, ax = plt.subplots(1, 1) # plot training data ax.plot(X_test[:, 0], Y_test[:, 0], &#39;kx&#39;) # plot 90% credible interval of predictions ax.fill_between(X_test[:,0], percentiles[0, :], percentiles[1, :], color=&#39;lightblue&#39;) # plot mean prediction ax.plot(X_test[:,0], mean_prediction, &#39;blue&#39;, ls=&#39;solid&#39;, lw=2.0) ax.set(xlabel=&quot;$X_0$&quot;, ylabel=&quot;Y&quot;, title=&quot;Mean predictions on test set with 90% CI&quot;) plt.tight_layout() plt.show() . print(f&quot;MSE: {np.mean((mean_prediction - Y_test.squeeze())**2)}&quot;) . MSE: 3.8549249172210693 . The test loss is again very bad. Irrelevant features drown the signal. To improve the model we will use automatic relevance determination. . Bayesian neural network with automatic relevance determination via NUTS . . Source: Zoubin Ghahramani (University of Cambridge) Bayesian Deep Learning Workshop &quot;History of Bayesian Neural Networks (Keynote talk)&quot; https://www.youtube.com/watch?v=FD8l2vPU5FY&amp;feature=youtu.be . A Gaussian prior on the weights promotes smoothness and letting their individual variance parameters go to zero promotes sparsity. . x = jnp.linspace(0, 0.1, 101) px = jnp.exp(dist.InverseGamma(2, 0.001).log_prob(x)) plt.plot(x, px) plt.gca().set(title=&quot;Inverse-Gamma PDF&quot;, xlabel=&quot;x&quot;, ylabel=&quot;Probability Density&quot;) plt.show() . def model(X, Y, D_H): D_X, D_Y = X.shape[1], 1 s1 = numpyro.sample(&quot;s1&quot;, dist.InverseGamma(2*jnp.ones((D_X)), 0.001*jnp.ones((D_X)))) w1 = numpyro.sample(&quot;w1&quot;, dist.Normal(jnp.zeros((D_X, D_H)), jnp.repeat(s1[:,None],D_H, axis=1))) b1 = numpyro.sample(&quot;b1&quot;, dist.Normal(jnp.zeros((D_H)), jnp.ones((D_H)))) z1 = nonlin(jnp.matmul(X, w1) + b1) w2 = numpyro.sample(&quot;w2&quot;, dist.Normal(jnp.zeros((D_H, D_H)), jnp.ones((D_H, D_H)))) # D_H D_H b2 = numpyro.sample(&quot;b2&quot;, dist.Normal(jnp.zeros((D_H)), jnp.ones((D_H)))) z2 = nonlin(jnp.matmul(z1, w2) + b2) w3 = numpyro.sample(&quot;w3&quot;, dist.Normal(jnp.zeros((D_H, D_H)), jnp.ones((D_H, D_H)))) # D_H D_H b3 = numpyro.sample(&quot;b3&quot;, dist.Normal(jnp.zeros((D_H)), jnp.ones((D_H)))) z3 = nonlin(jnp.matmul(z2, w3) + b3) w4 = numpyro.sample(&quot;w4&quot;, dist.Normal(jnp.zeros((D_H, D_H)), jnp.ones((D_H, D_H)))) # D_H D_H b4 = numpyro.sample(&quot;b4&quot;, dist.Normal(jnp.zeros((D_H)), jnp.ones((D_H)))) z4 = nonlin(jnp.matmul(z3, w4) + b4) w5 = numpyro.sample(&quot;w5&quot;, dist.Normal(jnp.zeros((D_H, D_Y)), jnp.ones((D_H, D_Y)))) # D_H D_Y b5 = numpyro.sample(&quot;b5&quot;, dist.Normal(jnp.zeros((D_Y)), jnp.ones((D_Y)))) z5 = jnp.matmul(z4, w5) + b5 # we put a prior on the observation noise # here we could trivially account for heteroskedasticity. prec_obs = numpyro.sample(&quot;prec_obs&quot;, dist.Gamma(3.0, 1.0)) sigma_obs = 1.0 / jnp.sqrt(prec_obs) # observe data numpyro.sample(&quot;Y&quot;, dist.Normal(z5, sigma_obs), obs=Y) . N, D_H = len(X_train), args.num_hidden # do inference rng_key, rng_key_predict = random.split(random.PRNGKey(0)) samples = run_inference(model, args, rng_key, X_train, Y_train, D_H) . sample: 100%|██████████| 4000/4000 [17:02&lt;00:00, 3.91it/s, 1023 steps of size 2.26e-03. acc. prob=0.71] . mean std median 5.0% 95.0% n_eff r_hat b1[0] 0.04 0.81 0.14 -1.29 1.30 113.62 1.01 b1[1] 0.00 0.80 0.05 -1.39 1.26 189.46 1.00 b1[2] 0.01 0.79 0.04 -1.35 1.30 126.70 1.00 b1[3] -0.02 0.87 0.04 -1.55 1.35 128.06 1.01 b1[4] -0.06 0.83 -0.01 -1.52 1.24 84.99 1.00 b2[0] -0.15 0.97 -0.18 -1.73 1.51 282.77 1.00 b2[1] -0.28 0.94 -0.29 -1.70 1.35 355.59 1.00 b2[2] -0.07 0.96 -0.09 -1.49 1.65 352.74 1.00 b2[3] -0.11 0.96 -0.14 -1.81 1.36 269.46 1.03 b2[4] -0.12 0.99 -0.11 -1.80 1.42 257.78 1.00 b3[0] -0.07 1.00 -0.10 -1.63 1.60 540.70 1.00 b3[1] -0.14 1.01 -0.15 -1.79 1.49 371.45 1.00 b3[2] -0.12 0.99 -0.12 -1.73 1.51 614.96 1.00 b3[3] -0.14 0.96 -0.14 -1.75 1.44 580.49 1.00 b3[4] -0.23 0.99 -0.25 -1.84 1.39 605.47 1.00 b4[0] -0.15 1.02 -0.15 -1.83 1.52 456.65 1.00 b4[1] -0.12 1.05 -0.12 -1.83 1.56 554.47 1.00 b4[2] -0.15 1.01 -0.16 -1.84 1.46 622.95 1.00 b4[3] -0.11 0.99 -0.12 -1.77 1.43 537.57 1.00 b4[4] -0.17 1.04 -0.17 -1.92 1.49 425.45 1.00 b5[0] -0.46 0.88 -0.45 -1.90 0.95 425.39 1.00 prec_obs 1.03 0.07 1.03 0.92 1.15 212.18 1.00 s1[0] 0.57 0.49 0.43 0.08 1.13 114.53 1.01 s1[1] 0.00 0.00 0.00 0.00 0.00 191.34 1.01 s1[2] 0.00 0.00 0.00 0.00 0.00 228.65 1.01 s1[3] 0.00 0.00 0.00 0.00 0.00 175.13 1.01 s1[4] 0.00 0.01 0.00 0.00 0.01 26.26 1.01 s1[5] 0.00 0.00 0.00 0.00 0.00 177.74 1.01 s1[6] 0.00 0.00 0.00 0.00 0.00 117.70 1.03 s1[7] 0.00 0.00 0.00 0.00 0.00 310.96 1.00 s1[8] 0.00 0.00 0.00 0.00 0.00 195.29 1.00 s1[9] 0.00 0.00 0.00 0.00 0.00 123.61 1.00 s1[10] 0.00 0.00 0.00 0.00 0.00 219.02 1.00 s1[11] 0.00 0.00 0.00 0.00 0.00 190.19 1.00 s1[12] 0.00 0.00 0.00 0.00 0.00 95.40 1.01 s1[13] 0.00 0.00 0.00 0.00 0.00 181.14 1.00 s1[14] 0.00 0.00 0.00 0.00 0.00 73.88 1.00 s1[15] 0.00 0.00 0.00 0.00 0.00 271.88 1.00 s1[16] 0.00 0.00 0.00 0.00 0.00 189.76 1.00 s1[17] 0.00 0.00 0.00 0.00 0.00 162.15 1.00 s1[18] 0.00 0.00 0.00 0.00 0.00 183.07 1.00 s1[19] 0.00 0.00 0.00 0.00 0.00 159.17 1.00 s1[20] 0.00 0.00 0.00 0.00 0.00 234.69 1.00 s1[21] 0.00 0.00 0.00 0.00 0.00 118.84 1.00 s1[22] 0.00 0.00 0.00 0.00 0.01 54.85 1.02 s1[23] 0.00 0.00 0.00 0.00 0.00 171.87 1.02 s1[24] 0.00 0.00 0.00 0.00 0.00 231.65 1.01 s1[25] 0.00 0.00 0.00 0.00 0.00 178.01 1.00 s1[26] 0.00 0.00 0.00 0.00 0.00 141.46 1.00 s1[27] 0.00 0.00 0.00 0.00 0.00 145.65 1.01 s1[28] 0.00 0.00 0.00 0.00 0.00 221.29 1.00 s1[29] 0.00 0.00 0.00 0.00 0.00 257.78 1.00 s1[30] 0.00 0.00 0.00 0.00 0.00 306.53 1.00 s1[31] 0.00 0.00 0.00 0.00 0.00 98.21 1.01 s1[32] 0.00 0.00 0.00 0.00 0.00 272.26 1.00 s1[33] 0.00 0.00 0.00 0.00 0.00 107.61 1.01 s1[34] 0.00 0.00 0.00 0.00 0.00 166.36 1.00 s1[35] 0.00 0.00 0.00 0.00 0.00 204.54 1.00 s1[36] 0.00 0.00 0.00 0.00 0.00 102.29 1.00 s1[37] 0.00 0.00 0.00 0.00 0.00 65.26 1.02 s1[38] 0.00 0.00 0.00 0.00 0.00 116.77 1.01 s1[39] 0.00 0.00 0.00 0.00 0.00 142.79 1.00 s1[40] 0.00 0.00 0.00 0.00 0.00 166.23 1.00 s1[41] 0.00 0.00 0.00 0.00 0.00 98.51 1.00 s1[42] 0.00 0.00 0.00 0.00 0.00 247.63 1.00 s1[43] 0.00 0.00 0.00 0.00 0.00 286.97 1.00 s1[44] 0.00 0.00 0.00 0.00 0.00 194.79 1.01 s1[45] 0.00 0.00 0.00 0.00 0.00 69.88 1.00 s1[46] 0.00 0.00 0.00 0.00 0.00 256.14 1.00 s1[47] 0.00 0.00 0.00 0.00 0.00 94.02 1.02 s1[48] 0.00 0.00 0.00 0.00 0.00 216.58 1.00 s1[49] 0.00 0.00 0.00 0.00 0.00 116.46 1.00 s1[50] 0.00 0.00 0.00 0.00 0.00 231.49 1.00 w1[0,0] 0.03 0.71 -0.00 -1.04 1.14 88.17 1.02 w1[0,1] 0.00 0.79 0.05 -1.00 1.24 144.52 1.00 w1[0,2] -0.15 0.74 -0.12 -1.14 1.19 91.90 1.02 w1[0,3] 0.05 0.75 0.09 -1.09 1.22 111.70 1.00 w1[0,4] 0.08 0.79 0.02 -1.06 1.17 88.11 1.01 w1[1,0] 0.00 0.00 -0.00 -0.00 0.00 953.73 1.00 w1[1,1] -0.00 0.00 0.00 -0.00 0.00 527.69 1.00 w1[1,2] -0.00 0.00 -0.00 -0.00 0.00 462.83 1.01 w1[1,3] -0.00 0.00 -0.00 -0.00 0.00 773.38 1.00 w1[1,4] 0.00 0.00 0.00 -0.00 0.00 642.55 1.00 w1[2,0] -0.00 0.00 -0.00 -0.00 0.00 681.22 1.00 w1[2,1] 0.00 0.00 -0.00 -0.00 0.00 562.23 1.00 w1[2,2] 0.00 0.00 0.00 -0.00 0.00 1576.68 1.00 w1[2,3] -0.00 0.00 0.00 -0.00 0.00 972.24 1.00 w1[2,4] 0.00 0.00 0.00 -0.00 0.00 1186.24 1.00 w1[3,0] -0.00 0.00 -0.00 -0.00 0.00 348.30 1.00 w1[3,1] -0.00 0.00 -0.00 -0.00 0.00 401.21 1.01 w1[3,2] 0.00 0.00 -0.00 -0.00 0.00 652.51 1.00 w1[3,3] 0.00 0.00 0.00 -0.00 0.00 402.55 1.00 w1[3,4] -0.00 0.00 -0.00 -0.00 0.00 458.36 1.00 w1[4,0] -0.00 0.01 -0.00 -0.01 0.01 29.45 1.10 w1[4,1] -0.00 0.01 -0.00 -0.02 0.01 30.83 1.08 w1[4,2] -0.00 0.01 -0.00 -0.01 0.01 114.50 1.01 w1[4,3] -0.00 0.01 -0.00 -0.01 0.01 63.46 1.02 w1[4,4] -0.00 0.01 -0.00 -0.01 0.01 55.07 1.04 w1[5,0] -0.00 0.00 -0.00 -0.00 0.00 536.69 1.01 w1[5,1] 0.00 0.00 0.00 -0.00 0.00 411.01 1.00 w1[5,2] -0.00 0.00 0.00 -0.00 0.00 397.04 1.01 w1[5,3] 0.00 0.00 0.00 -0.00 0.00 835.34 1.00 w1[5,4] -0.00 0.00 -0.00 -0.00 0.00 788.49 1.00 w1[6,0] -0.00 0.00 -0.00 -0.00 0.00 485.26 1.00 w1[6,1] -0.00 0.00 0.00 -0.00 0.00 292.33 1.00 w1[6,2] 0.00 0.00 0.00 -0.00 0.00 167.25 1.01 w1[6,3] -0.00 0.00 -0.00 -0.00 0.00 648.88 1.00 w1[6,4] 0.00 0.00 0.00 -0.00 0.00 424.59 1.00 w1[7,0] 0.00 0.00 0.00 -0.00 0.00 1003.97 1.00 w1[7,1] 0.00 0.00 0.00 -0.00 0.00 1342.09 1.00 w1[7,2] 0.00 0.00 0.00 -0.00 0.00 946.14 1.00 w1[7,3] 0.00 0.00 0.00 -0.00 0.00 908.19 1.01 w1[7,4] 0.00 0.00 -0.00 -0.00 0.00 1201.98 1.00 w1[8,0] 0.00 0.00 0.00 -0.00 0.00 510.04 1.00 w1[8,1] 0.00 0.00 0.00 -0.00 0.00 1020.12 1.00 w1[8,2] 0.00 0.00 0.00 -0.00 0.00 512.75 1.00 w1[8,3] 0.00 0.00 0.00 -0.00 0.00 1145.12 1.00 w1[8,4] 0.00 0.00 0.00 -0.00 0.00 799.15 1.00 w1[9,0] 0.00 0.00 0.00 -0.00 0.00 676.26 1.00 w1[9,1] 0.00 0.00 0.00 -0.00 0.00 165.01 1.01 w1[9,2] -0.00 0.00 0.00 -0.00 0.00 346.41 1.00 w1[9,3] -0.00 0.00 0.00 -0.00 0.00 413.00 1.00 w1[9,4] -0.00 0.00 0.00 -0.00 0.00 589.86 1.00 w1[10,0] 0.00 0.00 0.00 -0.00 0.00 561.87 1.00 w1[10,1] 0.00 0.00 0.00 -0.00 0.00 627.22 1.00 w1[10,2] -0.00 0.00 -0.00 -0.00 0.00 569.26 1.00 w1[10,3] 0.00 0.00 0.00 -0.00 0.00 474.69 1.00 w1[10,4] 0.00 0.00 -0.00 -0.00 0.00 1261.12 1.00 w1[11,0] 0.00 0.00 0.00 -0.00 0.00 937.18 1.00 w1[11,1] 0.00 0.00 0.00 -0.00 0.00 566.36 1.00 w1[11,2] 0.00 0.00 0.00 -0.00 0.00 1116.23 1.00 w1[11,3] 0.00 0.00 0.00 -0.00 0.00 1097.87 1.00 w1[11,4] 0.00 0.00 0.00 -0.00 0.00 1392.35 1.00 w1[12,0] 0.00 0.00 0.00 -0.00 0.00 205.07 1.00 w1[12,1] 0.00 0.00 0.00 -0.00 0.00 277.25 1.00 w1[12,2] 0.00 0.00 0.00 -0.00 0.00 107.20 1.01 w1[12,3] 0.00 0.00 0.00 -0.00 0.00 247.61 1.00 w1[12,4] 0.00 0.00 0.00 -0.00 0.00 515.73 1.00 w1[13,0] 0.00 0.00 0.00 -0.00 0.00 374.07 1.01 w1[13,1] 0.00 0.00 0.00 -0.00 0.00 403.01 1.00 w1[13,2] 0.00 0.00 0.00 -0.00 0.00 596.69 1.00 w1[13,3] 0.00 0.00 0.00 -0.00 0.00 881.66 1.00 w1[13,4] 0.00 0.00 0.00 -0.00 0.00 389.27 1.00 w1[14,0] 0.00 0.00 0.00 -0.00 0.00 113.52 1.00 w1[14,1] 0.00 0.00 0.00 -0.00 0.00 456.17 1.00 w1[14,2] 0.00 0.00 0.00 -0.00 0.00 298.18 1.00 w1[14,3] 0.00 0.00 0.00 -0.00 0.00 169.78 1.00 w1[14,4] 0.00 0.00 0.00 -0.00 0.00 314.92 1.00 w1[15,0] -0.00 0.00 0.00 -0.00 0.00 1050.52 1.00 w1[15,1] 0.00 0.00 0.00 -0.00 0.00 1251.63 1.00 w1[15,2] -0.00 0.00 0.00 -0.00 0.00 1596.20 1.00 w1[15,3] 0.00 0.00 -0.00 -0.00 0.00 1615.30 1.00 w1[15,4] -0.00 0.00 -0.00 -0.00 0.00 1070.63 1.00 w1[16,0] -0.00 0.00 -0.00 -0.00 0.00 685.92 1.00 w1[16,1] 0.00 0.00 0.00 -0.00 0.00 533.94 1.00 w1[16,2] 0.00 0.00 -0.00 -0.00 0.00 513.15 1.01 w1[16,3] -0.00 0.00 -0.00 -0.00 0.00 561.66 1.00 w1[16,4] -0.00 0.00 0.00 -0.00 0.00 517.43 1.01 w1[17,0] -0.00 0.00 -0.00 -0.00 0.00 782.43 1.00 w1[17,1] -0.00 0.00 -0.00 -0.00 0.00 998.37 1.00 w1[17,2] 0.00 0.00 -0.00 -0.00 0.00 647.47 1.00 w1[17,3] 0.00 0.00 0.00 -0.00 0.00 1186.38 1.00 w1[17,4] -0.00 0.00 0.00 -0.00 0.00 968.63 1.00 w1[18,0] 0.00 0.00 0.00 -0.00 0.00 392.96 1.00 w1[18,1] -0.00 0.00 0.00 -0.00 0.00 345.80 1.00 w1[18,2] 0.00 0.00 0.00 -0.00 0.00 540.79 1.00 w1[18,3] 0.00 0.00 0.00 -0.00 0.00 993.77 1.00 w1[18,4] -0.00 0.00 -0.00 -0.00 0.00 290.81 1.00 w1[19,0] 0.00 0.00 0.00 -0.00 0.00 468.23 1.00 w1[19,1] -0.00 0.00 -0.00 -0.00 0.00 433.31 1.00 w1[19,2] -0.00 0.00 -0.00 -0.00 0.00 1077.28 1.00 w1[19,3] -0.00 0.00 -0.00 -0.00 0.00 495.56 1.00 w1[19,4] -0.00 0.00 -0.00 -0.00 0.00 807.85 1.00 w1[20,0] -0.00 0.00 -0.00 -0.00 0.00 514.05 1.00 w1[20,1] 0.00 0.00 0.00 -0.00 0.00 685.01 1.00 w1[20,2] -0.00 0.00 -0.00 -0.00 0.00 453.71 1.00 w1[20,3] -0.00 0.00 0.00 -0.00 0.00 517.96 1.00 w1[20,4] 0.00 0.00 0.00 -0.00 0.00 579.04 1.00 w1[21,0] 0.00 0.00 0.00 -0.00 0.00 275.52 1.01 w1[21,1] -0.00 0.00 0.00 -0.00 0.00 408.52 1.00 w1[21,2] -0.00 0.00 -0.00 -0.00 0.00 395.44 1.00 w1[21,3] 0.00 0.00 -0.00 -0.00 0.00 175.77 1.01 w1[21,4] -0.00 0.00 0.00 -0.00 0.00 136.99 1.01 w1[22,0] 0.00 0.00 0.00 -0.00 0.01 65.79 1.00 w1[22,1] 0.00 0.00 0.00 -0.00 0.01 138.28 1.00 w1[22,2] -0.00 0.00 0.00 -0.00 0.00 133.07 1.01 w1[22,3] 0.00 0.00 0.00 -0.01 0.00 60.83 1.03 w1[22,4] 0.00 0.00 0.00 -0.00 0.00 107.34 1.00 w1[23,0] -0.00 0.00 -0.00 -0.00 0.00 638.47 1.00 w1[23,1] -0.00 0.00 -0.00 -0.00 0.00 731.90 1.00 w1[23,2] -0.00 0.00 -0.00 -0.00 0.00 528.85 1.00 w1[23,3] 0.00 0.00 0.00 -0.00 0.00 956.54 1.00 w1[23,4] -0.00 0.00 -0.00 -0.00 0.00 265.75 1.00 w1[24,0] -0.00 0.00 -0.00 -0.00 0.00 1008.92 1.00 w1[24,1] -0.00 0.00 -0.00 -0.00 0.00 1699.13 1.00 w1[24,2] -0.00 0.00 0.00 -0.00 0.00 1482.53 1.00 w1[24,3] -0.00 0.00 0.00 -0.00 0.00 831.83 1.00 w1[24,4] 0.00 0.00 0.00 -0.00 0.00 1715.13 1.00 w1[25,0] -0.00 0.00 -0.00 -0.00 0.00 808.89 1.00 w1[25,1] 0.00 0.00 0.00 -0.00 0.00 1339.80 1.00 w1[25,2] 0.00 0.00 0.00 -0.00 0.00 1539.64 1.00 w1[25,3] 0.00 0.00 -0.00 -0.00 0.00 767.21 1.00 w1[25,4] -0.00 0.00 -0.00 -0.00 0.00 1131.12 1.00 w1[26,0] 0.00 0.00 0.00 -0.00 0.00 1034.90 1.00 w1[26,1] -0.00 0.00 -0.00 -0.00 0.00 570.38 1.00 w1[26,2] 0.00 0.00 0.00 -0.00 0.00 1383.00 1.00 w1[26,3] -0.00 0.00 -0.00 -0.00 0.00 423.64 1.00 w1[26,4] 0.00 0.00 0.00 -0.00 0.00 1156.87 1.00 w1[27,0] -0.00 0.00 -0.00 -0.00 0.00 350.31 1.00 w1[27,1] -0.00 0.00 0.00 -0.00 0.00 561.08 1.00 w1[27,2] -0.00 0.00 -0.00 -0.00 0.00 246.99 1.01 w1[27,3] -0.00 0.00 -0.00 -0.00 0.00 365.11 1.01 w1[27,4] -0.00 0.00 -0.00 -0.00 0.00 223.80 1.00 w1[28,0] 0.00 0.00 0.00 -0.00 0.00 742.01 1.00 w1[28,1] 0.00 0.00 0.00 -0.00 0.00 930.48 1.00 w1[28,2] -0.00 0.00 -0.00 -0.00 0.00 1066.02 1.00 w1[28,3] 0.00 0.00 0.00 -0.00 0.00 1280.79 1.00 w1[28,4] -0.00 0.00 -0.00 -0.00 0.00 1925.85 1.00 w1[29,0] 0.00 0.00 0.00 -0.00 0.00 1941.13 1.00 w1[29,1] 0.00 0.00 0.00 -0.00 0.00 1881.68 1.00 w1[29,2] -0.00 0.00 -0.00 -0.00 0.00 1776.24 1.00 w1[29,3] 0.00 0.00 0.00 -0.00 0.00 987.25 1.00 w1[29,4] -0.00 0.00 -0.00 -0.00 0.00 1394.32 1.00 w1[30,0] -0.00 0.00 0.00 -0.00 0.00 1132.10 1.00 w1[30,1] -0.00 0.00 0.00 -0.00 0.00 1544.31 1.00 w1[30,2] 0.00 0.00 -0.00 -0.00 0.00 1976.32 1.00 w1[30,3] -0.00 0.00 0.00 -0.00 0.00 1552.22 1.00 w1[30,4] 0.00 0.00 0.00 -0.00 0.00 2266.97 1.00 w1[31,0] 0.00 0.00 0.00 -0.00 0.00 265.68 1.00 w1[31,1] -0.00 0.00 -0.00 -0.00 0.00 685.88 1.00 w1[31,2] -0.00 0.00 -0.00 -0.00 0.00 107.73 1.00 w1[31,3] 0.00 0.00 0.00 -0.00 0.00 453.50 1.00 w1[31,4] -0.00 0.00 -0.00 -0.00 0.00 1144.57 1.00 w1[32,0] -0.00 0.00 0.00 -0.00 0.00 1901.31 1.00 w1[32,1] -0.00 0.00 -0.00 -0.00 0.00 1291.60 1.00 w1[32,2] -0.00 0.00 -0.00 -0.00 0.00 1818.32 1.00 w1[32,3] 0.00 0.00 0.00 -0.00 0.00 1076.02 1.00 w1[32,4] -0.00 0.00 0.00 -0.00 0.00 1525.74 1.00 w1[33,0] 0.00 0.00 -0.00 -0.00 0.00 333.31 1.00 w1[33,1] 0.00 0.00 -0.00 -0.00 0.00 456.21 1.01 w1[33,2] -0.00 0.00 0.00 -0.00 0.00 230.04 1.00 w1[33,3] -0.00 0.00 -0.00 -0.00 0.00 133.74 1.00 w1[33,4] 0.00 0.00 -0.00 -0.00 0.00 181.14 1.01 w1[34,0] -0.00 0.00 -0.00 -0.00 0.00 610.31 1.00 w1[34,1] 0.00 0.00 0.00 -0.00 0.00 314.78 1.01 w1[34,2] -0.00 0.00 -0.00 -0.00 0.00 677.34 1.00 w1[34,3] -0.00 0.00 -0.00 -0.00 0.00 566.56 1.01 w1[34,4] 0.00 0.00 0.00 -0.00 0.00 457.22 1.00 w1[35,0] -0.00 0.00 -0.00 -0.00 0.00 682.17 1.00 w1[35,1] -0.00 0.00 -0.00 -0.00 0.00 929.54 1.00 w1[35,2] -0.00 0.00 -0.00 -0.00 0.00 334.55 1.00 w1[35,3] 0.00 0.00 0.00 -0.00 0.00 896.55 1.00 w1[35,4] -0.00 0.00 -0.00 -0.00 0.00 713.94 1.00 w1[36,0] -0.00 0.00 0.00 -0.00 0.00 257.92 1.01 w1[36,1] 0.00 0.00 0.00 -0.00 0.00 560.83 1.00 w1[36,2] 0.00 0.00 0.00 -0.00 0.00 227.83 1.00 w1[36,3] 0.00 0.00 0.00 -0.00 0.00 316.23 1.01 w1[36,4] 0.00 0.00 0.00 -0.00 0.00 221.00 1.00 w1[37,0] 0.00 0.00 -0.00 -0.00 0.00 75.07 1.01 w1[37,1] 0.00 0.00 -0.00 -0.00 0.00 216.49 1.01 w1[37,2] -0.00 0.00 -0.00 -0.00 0.00 77.42 1.02 w1[37,3] 0.00 0.00 -0.00 -0.00 0.00 209.31 1.01 w1[37,4] -0.00 0.00 -0.00 -0.00 0.00 224.16 1.01 w1[38,0] 0.00 0.00 -0.00 -0.00 0.00 279.53 1.01 w1[38,1] 0.00 0.00 0.00 -0.00 0.00 814.44 1.00 w1[38,2] 0.00 0.00 0.00 -0.00 0.00 494.05 1.00 w1[38,3] -0.00 0.00 -0.00 -0.00 0.00 257.64 1.01 w1[38,4] -0.00 0.00 -0.00 -0.00 0.00 158.94 1.01 w1[39,0] 0.00 0.00 0.00 -0.00 0.00 717.01 1.01 w1[39,1] 0.00 0.00 0.00 -0.00 0.00 515.52 1.00 w1[39,2] 0.00 0.00 0.00 -0.00 0.00 245.67 1.01 w1[39,3] 0.00 0.00 0.00 -0.00 0.00 298.07 1.00 w1[39,4] 0.00 0.00 0.00 -0.00 0.00 278.73 1.00 w1[40,0] 0.00 0.00 0.00 -0.00 0.00 671.20 1.00 w1[40,1] 0.00 0.00 0.00 -0.00 0.00 779.73 1.00 w1[40,2] 0.00 0.00 0.00 -0.00 0.00 910.47 1.00 w1[40,3] 0.00 0.00 0.00 -0.00 0.00 1365.37 1.00 w1[40,4] -0.00 0.00 -0.00 -0.00 0.00 1235.55 1.00 w1[41,0] -0.00 0.00 -0.00 -0.00 0.00 475.45 1.00 w1[41,1] 0.00 0.00 -0.00 -0.00 0.00 446.77 1.00 w1[41,2] -0.00 0.00 -0.00 -0.00 0.00 407.80 1.00 w1[41,3] -0.00 0.00 -0.00 -0.00 0.00 293.79 1.01 w1[41,4] 0.00 0.00 0.00 -0.00 0.00 226.41 1.00 w1[42,0] 0.00 0.00 0.00 -0.00 0.00 897.68 1.00 w1[42,1] 0.00 0.00 0.00 -0.00 0.00 566.85 1.00 w1[42,2] 0.00 0.00 0.00 -0.00 0.00 707.74 1.00 w1[42,3] 0.00 0.00 0.00 -0.00 0.00 1260.08 1.00 w1[42,4] 0.00 0.00 0.00 -0.00 0.00 613.61 1.00 w1[43,0] 0.00 0.00 0.00 -0.00 0.00 772.18 1.00 w1[43,1] 0.00 0.00 0.00 -0.00 0.00 1495.03 1.00 w1[43,2] -0.00 0.00 0.00 -0.00 0.00 1173.89 1.00 w1[43,3] -0.00 0.00 -0.00 -0.00 0.00 1077.43 1.00 w1[43,4] 0.00 0.00 0.00 -0.00 0.00 1441.57 1.00 w1[44,0] -0.00 0.00 -0.00 -0.00 0.00 969.07 1.00 w1[44,1] -0.00 0.00 -0.00 -0.00 0.00 580.57 1.00 w1[44,2] 0.00 0.00 0.00 -0.00 0.00 821.57 1.00 w1[44,3] -0.00 0.00 0.00 -0.00 0.00 953.44 1.00 w1[44,4] -0.00 0.00 -0.00 -0.00 0.00 705.41 1.00 w1[45,0] 0.00 0.00 0.00 -0.00 0.00 127.48 1.00 w1[45,1] 0.00 0.00 0.00 -0.00 0.00 322.84 1.00 w1[45,2] 0.00 0.00 0.00 -0.00 0.00 193.77 1.00 w1[45,3] 0.00 0.00 0.00 -0.00 0.00 240.22 1.00 w1[45,4] -0.00 0.00 -0.00 -0.00 0.00 156.66 1.03 w1[46,0] -0.00 0.00 -0.00 -0.00 0.00 1937.80 1.00 w1[46,1] -0.00 0.00 -0.00 -0.00 0.00 1243.68 1.00 w1[46,2] 0.00 0.00 0.00 -0.00 0.00 920.64 1.00 w1[46,3] 0.00 0.00 0.00 -0.00 0.00 810.35 1.00 w1[46,4] -0.00 0.00 0.00 -0.00 0.00 1160.78 1.00 w1[47,0] 0.00 0.00 -0.00 -0.00 0.00 84.29 1.01 w1[47,1] 0.00 0.00 -0.00 -0.00 0.00 324.69 1.01 w1[47,2] -0.00 0.00 -0.00 -0.00 0.00 150.37 1.01 w1[47,3] 0.00 0.00 -0.00 -0.00 0.00 510.97 1.00 w1[47,4] 0.00 0.00 0.00 -0.00 0.00 294.95 1.00 w1[48,0] 0.00 0.00 0.00 -0.00 0.00 881.75 1.00 w1[48,1] 0.00 0.00 0.00 -0.00 0.00 1196.33 1.00 w1[48,2] 0.00 0.00 0.00 -0.00 0.00 1257.32 1.00 w1[48,3] 0.00 0.00 0.00 -0.00 0.00 1177.12 1.00 w1[48,4] -0.00 0.00 0.00 -0.00 0.00 1003.80 1.00 w1[49,0] 0.00 0.00 -0.00 -0.00 0.00 554.67 1.00 w1[49,1] -0.00 0.00 -0.00 -0.00 0.00 367.69 1.00 w1[49,2] -0.00 0.00 -0.00 -0.00 0.00 184.71 1.01 w1[49,3] -0.00 0.00 -0.00 -0.00 0.00 326.57 1.00 w1[49,4] -0.00 0.00 -0.00 -0.00 0.00 450.91 1.00 w1[50,0] 0.00 0.00 0.00 -0.00 0.00 1235.53 1.00 w1[50,1] 0.00 0.00 0.00 -0.00 0.00 962.07 1.00 w1[50,2] 0.00 0.00 -0.00 -0.00 0.00 973.27 1.00 w1[50,3] -0.00 0.00 0.00 -0.00 0.00 1458.43 1.00 w1[50,4] 0.00 0.00 0.00 -0.00 0.00 1639.42 1.00 w2[0,0] -0.02 1.09 -0.03 -1.72 1.87 210.13 1.00 w2[0,1] 0.08 1.05 0.09 -1.65 1.76 371.26 1.00 w2[0,2] 0.18 1.02 0.17 -1.45 1.90 222.52 1.01 w2[0,3] 0.04 1.00 0.03 -1.55 1.65 276.21 1.00 w2[0,4] 0.21 1.03 0.16 -1.48 1.90 316.38 1.01 w2[1,0] 0.01 1.00 0.03 -1.58 1.69 384.09 1.00 w2[1,1] 0.05 1.01 0.08 -1.58 1.75 450.57 1.02 w2[1,2] 0.09 1.04 0.10 -1.59 1.84 358.91 1.00 w2[1,3] 0.15 0.97 0.19 -1.32 1.84 391.44 1.00 w2[1,4] 0.06 1.05 0.07 -1.56 1.80 211.36 1.00 w2[2,0] 0.07 1.04 0.06 -1.51 1.85 332.91 1.00 w2[2,1] 0.13 1.04 0.11 -1.62 1.77 416.00 1.00 w2[2,2] 0.05 0.99 0.06 -1.58 1.65 293.51 1.00 w2[2,3] 0.13 1.05 0.11 -1.52 1.89 493.59 1.00 w2[2,4] 0.18 1.03 0.22 -1.55 1.84 379.65 1.00 w2[3,0] 0.04 1.00 0.04 -1.62 1.64 397.96 1.00 w2[3,1] 0.05 1.03 0.09 -1.66 1.67 343.36 1.00 w2[3,2] 0.12 1.04 0.12 -1.48 1.95 357.92 1.00 w2[3,3] 0.14 1.03 0.15 -1.52 1.85 365.29 1.00 w2[3,4] 0.12 1.04 0.14 -1.51 1.95 475.00 1.00 w2[4,0] 0.21 0.99 0.22 -1.37 1.86 321.87 1.00 w2[4,1] -0.04 1.04 -0.03 -1.72 1.63 254.54 1.00 w2[4,2] 0.11 1.00 0.12 -1.58 1.70 407.78 1.00 w2[4,3] 0.12 1.00 0.13 -1.54 1.69 422.59 1.00 w2[4,4] 0.10 1.03 0.12 -1.50 1.84 257.21 1.00 w3[0,0] 0.07 1.03 0.08 -1.70 1.65 352.31 1.00 w3[0,1] 0.16 1.02 0.18 -1.37 1.94 442.94 1.00 w3[0,2] 0.05 1.01 0.04 -1.69 1.65 536.94 1.00 w3[0,3] 0.02 1.01 0.01 -1.53 1.79 442.21 1.00 w3[0,4] 0.13 0.96 0.11 -1.55 1.65 430.08 1.00 w3[1,0] 0.14 1.02 0.15 -1.40 1.91 389.20 1.00 w3[1,1] 0.11 0.98 0.12 -1.42 1.80 410.43 1.00 w3[1,2] 0.01 0.97 -0.06 -1.60 1.53 513.97 1.00 w3[1,3] 0.00 1.03 0.03 -1.69 1.66 536.94 1.00 w3[1,4] 0.14 1.08 0.18 -1.68 1.80 452.11 1.00 w3[2,0] 0.08 1.00 0.05 -1.50 1.77 379.37 1.00 w3[2,1] 0.09 1.03 0.02 -1.46 1.89 379.41 1.00 w3[2,2] -0.01 1.06 -0.03 -1.70 1.71 318.26 1.00 w3[2,3] 0.13 1.03 0.14 -1.45 1.85 424.19 1.00 w3[2,4] 0.14 1.05 0.13 -1.74 1.72 373.82 1.01 w3[3,0] 0.11 0.98 0.13 -1.55 1.66 414.24 1.00 w3[3,1] 0.08 1.00 0.11 -1.52 1.73 633.13 1.00 w3[3,2] 0.04 0.99 0.04 -1.56 1.64 548.39 1.00 w3[3,3] -0.01 1.04 0.02 -1.68 1.69 310.29 1.00 w3[3,4] 0.11 1.05 0.12 -1.62 1.83 268.26 1.00 w3[4,0] 0.11 1.02 0.13 -1.52 1.81 313.32 1.00 w3[4,1] 0.13 1.02 0.10 -1.67 1.69 452.28 1.00 w3[4,2] 0.05 0.98 0.01 -1.64 1.59 550.72 1.00 w3[4,3] 0.07 1.00 0.09 -1.61 1.68 443.24 1.00 w3[4,4] 0.15 1.03 0.15 -1.53 1.79 416.71 1.00 w4[0,0] 0.08 0.99 0.04 -1.41 1.84 364.28 1.00 w4[0,1] 0.03 1.02 0.04 -1.62 1.71 559.69 1.01 w4[0,2] 0.10 0.99 0.07 -1.53 1.69 402.97 1.01 w4[0,3] 0.02 1.00 0.00 -1.68 1.58 425.19 1.00 w4[0,4] 0.08 1.03 0.06 -1.61 1.75 419.17 1.00 w4[1,0] 0.01 1.03 0.01 -1.68 1.68 553.00 1.00 w4[1,1] 0.02 1.04 0.06 -1.88 1.50 452.21 1.00 w4[1,2] 0.07 1.00 0.06 -1.40 1.84 477.69 1.00 w4[1,3] 0.13 1.07 0.14 -1.50 2.01 532.68 1.00 w4[1,4] 0.05 0.99 0.06 -1.51 1.77 486.97 1.00 w4[2,0] 0.07 1.03 0.07 -1.75 1.60 432.15 1.00 w4[2,1] 0.01 0.99 0.05 -1.66 1.51 428.88 1.01 w4[2,2] 0.12 1.02 0.10 -1.48 1.81 410.47 1.00 w4[2,3] 0.11 0.99 0.12 -1.49 1.66 501.74 1.00 w4[2,4] 0.10 1.00 0.12 -1.42 1.84 471.64 1.00 w4[3,0] 0.06 1.02 0.08 -1.70 1.65 474.47 1.02 w4[3,1] 0.00 1.02 0.01 -1.78 1.51 591.02 1.00 w4[3,2] 0.00 1.00 0.04 -1.55 1.71 563.69 1.00 w4[3,3] 0.03 1.01 0.03 -1.52 1.74 429.29 1.00 w4[3,4] 0.18 1.07 0.18 -1.48 1.99 518.86 1.00 w4[4,0] 0.06 1.03 0.02 -1.51 1.86 464.24 1.00 w4[4,1] 0.11 1.01 0.10 -1.45 1.88 561.97 1.00 w4[4,2] 0.12 1.04 0.14 -1.64 1.72 408.66 1.00 w4[4,3] 0.16 1.03 0.18 -1.59 1.77 352.54 1.01 w4[4,4] 0.16 1.05 0.13 -1.57 1.87 401.94 1.01 w5[0,0] 0.27 1.02 0.30 -1.36 1.97 278.12 1.00 w5[1,0] 0.33 1.01 0.32 -1.14 2.08 239.84 1.03 w5[2,0] 0.48 1.04 0.51 -1.09 2.23 201.16 1.00 w5[3,0] 0.53 1.06 0.56 -1.16 2.25 256.09 1.00 w5[4,0] 0.40 1.07 0.44 -1.32 2.12 245.37 1.00 Number of divergences: 87 MCMC elapsed time: 1026.5278742313385 . Notice how only s1[0] is significantly different from zero, meaning that only feature 0 is selected. . # predict Y (test) at inputs X vmap_args = (samples, random.split(rng_key_predict, args.num_samples * args.num_chains)) predictions = vmap(lambda samples, rng_key: predict(model, rng_key, samples, X_train, D_H))(*vmap_args) predictions = predictions[..., 0] # compute mean prediction and confidence interval around median mean_prediction = jnp.mean(predictions, axis=0) percentiles = np.percentile(predictions, [5.0, 95.0], axis=0) fig, ax = plt.subplots(1, 1) # plot training data ax.plot(X_train[:, 0], Y_train[:, 0], &#39;kx&#39;) # plot 90% credible interval of predictions ax.fill_between(X_train[:,0], percentiles[0, :], percentiles[1, :], color=&#39;lightblue&#39;) # plot mean prediction ax.plot(X_train[:,0], mean_prediction, &#39;blue&#39;, ls=&#39;solid&#39;, lw=2.0) ax.set(xlabel=&quot;$X_0$&quot;, ylabel=&quot;Y&quot;, title=&quot;Mean predictions on train set with 90% CI&quot;) plt.tight_layout() plt.show() . # predict Y (test) at inputs X vmap_args = (samples, random.split(rng_key_predict, args.num_samples * args.num_chains)) predictions = vmap(lambda samples, rng_key: predict(model, rng_key, samples, X_test, D_H))(*vmap_args) predictions = predictions[..., 0] # compute mean prediction and credible interval around median mean_prediction = jnp.mean(predictions, axis=0) percentiles = np.percentile(predictions, [5.0, 95.0], axis=0) fig, ax = plt.subplots(1, 1) # plot training data ax.plot(X_test[:, 0], Y_test[:, 0], &#39;kx&#39;) # plot 90% credible level of predictions ax.fill_between(X_test[:,0], percentiles[0, :], percentiles[1, :], color=&#39;lightblue&#39;) # plot mean prediction ax.plot(X_test[:,0], mean_prediction, &#39;blue&#39;, ls=&#39;solid&#39;, lw=2.0) ax.set(xlabel=&quot;$X_0$&quot;, ylabel=&quot;Y&quot;, title=&quot;Mean predictions on test set with 90% CI&quot;) plt.tight_layout() plt.show() . print(f&quot;MSE: {np.mean((mean_prediction - Y_test.squeeze())**2)}&quot;) . MSE: 0.880367636680603 . df = pd.DataFrame(predictions[:1000,:].T, index=X_test[:,0]) df.sort_index(inplace=True) df.plot(legend=False, alpha=0.01, c=&#39;b&#39;) plt.plot(X_test[:,0], mean_prediction, &#39;black&#39;, ls=&#39;solid&#39;, lw=2.0) plt.plot(X_test[:,0], Y_test.squeeze(), &#39;.&#39;, alpha=0.1, c=&#39;r&#39;) plt.xlabel(&quot;$X_0$&quot;) plt.ylabel(&quot;Y&quot;) plt.title(&quot;Samples from the posterior predictive distribution.&quot;); . Conclusion . Letting the model turn-off input-layer weights by putting an inverse-gamma prior on their variance significantly improves generalization. .",
            "url": "https://jpwoeltjen.github.io/researchBlog/bayesian/deep%20learning/feature%20selection/2020/11/09/bayesian_regularization.html",
            "relUrl": "/bayesian/deep%20learning/feature%20selection/2020/11/09/bayesian_regularization.html",
            "date": " • Nov 9, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "On the Uncertainty Quantification of Deep Neural Networks",
            "content": "import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Set matplotlib settings %matplotlib inline plt.style.use(&#39;default&#39;) %config InlineBackend.figure_format=&#39;retina&#39; %load_ext autoreload %autoreload 2 . Traditional Portfolio optimization (e.g. Markowitz (1952)) only considers aleatoric uncertainty as sources of risk. It is assumed that the vector of mean returns and the covariance matrix are known. In reality, we estimate these objects based only on finite sample sizes and thus have epistemic uncertainty about the model. To make better decisions both sources of risk need to be accounted for. Portfolio Optimization within a Bayesian Framework succinctly states the main equation and lists some advantages of this framework. Importantly, a Bayesian approach allows us to automatically recognize regime shifts or outliers and reduce our risk exposure when there is high uncertainty as a result. . In addition to uncertainty quantification, training deep neural networks within a Bayesian framework also increases performance since roughly equivalent local optima are ensembled instead of arbitrarily chosen to the exclusion of another. Bayesian model averaging was also found to eliminate the double-descent phenomenon. . The main challenge of training Bayesian neural networks is their high computational cost. . There are several approaches: . Hamiltonian MCMC | Stochastic Variational Inference | Approximate Bayesian methods Monte Carlo Drop Out | Stochastic Weight Averaging (SWA, SWAG, ) | deep ensembles | to get samples within and across basins of attraction: multi-SWAG (ensembles of SWAG) | ensemble multiple models trained via variational inference. | . | Stochastic gradient Langevin dynamics | $w_{k+1}=w_{k}- alpha_{k} nabla U(w)+ sqrt{2 alpha_{k}} epsilon, quad epsilon sim mathcal{N}(0, I)$ | . | Incomplete and noisy sinusoidal data . Let&#39;s simulate some data. . n = 1000 X = np.linspace(0,30,n) Y = np.sin(X) + 0.1*np.random.normal(0,1, n) # Y = 0.2*X + 1 plt.plot(X,Y, &#39;.&#39;) plt.xlabel(&quot;X&quot;) plt.ylabel(&quot;Y&quot;) plt.title(&quot;Whole data set&quot;) plt.show(); . Now we select some clusters from the data set to train on. We want our model to generate high uncertainty estimates for parts of the domain where there is no data. . train_indexes = np.concatenate([np.arange(int(0.1*n), int(0.2*n)), np.arange(int(0.25*n), int(0.35*n)), np.arange(int(0.6*n),int(0.7*n)), np.arange(int(0.8*n),int(0.9*n))]) X_train, Y_train = X[train_indexes, None], Y[train_indexes, None] X_test, Y_test = X[~train_indexes, None], Y[~train_indexes, None] X_train.shape, Y_train.shape . ((400, 1), (400, 1)) . plt.plot(X_train.squeeze(), Y_train.squeeze(), &#39;.&#39;) plt.xlabel(&quot;X&quot;) plt.ylabel(&quot;Y&quot;) plt.title(&quot;Train data set&quot;) plt.show(); . Deep Enembles . import torch import torch.nn as nn import torch.nn.functional as F from torch import optim from tqdm import tqdm from collections import OrderedDict class Net(nn.Module): def __init__(self, n_neurons, n_features, dropout): super().__init__() self.fc1 = nn.Linear(n_features, n_neurons) self.fc2 = nn.Linear(n_neurons, n_neurons) self.fc3 = nn.Linear(n_neurons, n_neurons) self.fc4 = nn.Linear(n_neurons, n_neurons) self.fc5 = nn.Linear(n_neurons, 1) self.fc1bn = nn.BatchNorm1d(n_neurons) self.fc2bn = nn.BatchNorm1d(n_neurons) self.fc3bn = nn.BatchNorm1d(n_neurons) self.fc4bn = nn.BatchNorm1d(n_neurons) self.drop_layer = nn.Dropout(p=dropout) def forward(self, X): X = F.relu(self.fc1bn(self.fc1(X))) X = self.drop_layer(X) X = F.relu(self.fc2bn(self.fc2(X))) X = self.drop_layer(X) X = F.relu(self.fc3bn(self.fc3(X))) X = self.drop_layer(X) X = F.relu(self.fc4bn(self.fc4(X))) X = self.drop_layer(X) X = self.fc5(X) return X def fit_model(model, epochs, X, Y, X_valid, Y_valid): optimizer = optim.Adam(model.parameters(), weight_decay=0.0, lr=1e-3) for epoch in (range(epochs)): # trainig mode model = model.train() model.zero_grad() Y_hat = model(X) loss = criterion(Y_hat, Y) loss.backward() optimizer.step() if (epoch+1) % 100 ==0: with torch.no_grad(): model.eval() Y_hat = model(X_valid) # print(f&#39;Epoch: {epoch+1} t Train loss: {loss} t Valid loss: {criterion(Y_hat, Y_valid)}&#39;) def fit_model_wr(model, epochs, X, Y, X_valid, Y_valid, batch_size=None): &quot;Fit model via SGD WITH REPLACEMENT&quot; if batch_size is None: batch_size = len(X)//2 optimizer = optim.Adam(model.parameters(), weight_decay=0.0, lr=1e-3) for epoch in (range(epochs)): # trainig mode model = model.train() # X is a torch Variable permutation = torch.randperm(X.size()[0]) optimizer.zero_grad() indices = permutation[:batch_size] batch_x, batch_y = X[indices], Y[indices] model.zero_grad() Y_hat = model(batch_x) loss = criterion(Y_hat, batch_y) loss.backward() optimizer.step() if (epoch+1) % 100 ==0: with torch.no_grad(): model.eval() Y_hat = model(X_valid) # print(f&#39;Epoch: {epoch+1} t Train loss: {loss} t Valid loss: {criterion(Y_hat, Y_valid)}&#39;) def predict(model, X): with torch.no_grad(): model.eval() Y_hat = model(X) return Y_hat . print(&quot;GPU available:&quot;, torch.cuda.is_available()) device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) . GPU available: False . N = 100 n_neurons = 20 dropout = 0. epochs = 1000 criterion = nn.MSELoss() # Train set X_train = torch.tensor(X_train).float().to(device) Y_train = torch.tensor(Y_train).float().to(device) # Validation set X_test = torch.tensor(X_test).float().to(device) Y_test = torch.tensor(Y_test).float().to(device) n_features = X_train.shape[1] models = [nn.Sequential(OrderedDict([ (&#39;lin1&#39;, nn.Linear(n_features, n_neurons)), (&#39;bn1&#39;,nn.BatchNorm1d(n_neurons)), (&#39;relu1&#39;, nn.ReLU()), (&#39;lin2&#39;, nn.Linear(n_neurons, n_neurons)), (&#39;bn2&#39;,nn.BatchNorm1d(n_neurons)), (&#39;relu2&#39;, nn.ReLU()), (&#39;lin3&#39;, nn.Linear(n_neurons, n_neurons)), (&#39;bn3&#39;,nn.BatchNorm1d(n_neurons)), (&#39;relu3&#39;, nn.ReLU()), (&#39;lin4&#39;, nn.Linear(n_neurons, n_neurons)), (&#39;bn4&#39;,nn.BatchNorm1d(n_neurons)), (&#39;relu4&#39;, nn.ReLU()), (&#39;out&#39;, nn.Linear(n_neurons, 1)), ])) for i in range(N)] for i, model in tqdm(enumerate(models)): model.to(device) fit_model_wr(model, epochs, X_train, Y_train, X_test, Y_test, int(len(X_train)//1)) . 100it [05:07, 3.07s/it] . y_hat_list = [] for model in models: y_hat_list.append(predict(model, torch.tensor(X[:, None]).float().to(device)).cpu().numpy().squeeze()) df = pd.DataFrame(np.array(y_hat_list).T, index=X) df.sort_index(inplace=True) df.plot(legend=False, alpha=0.05, c=&#39;b&#39;) plt.plot(X_train.cpu().squeeze(), Y_train.cpu().squeeze(), &#39;.&#39;, alpha=0.1, c=&#39;r&#39;) plt.xlabel(&quot;X&quot;) plt.ylabel(&quot;Y&quot;) plt.title(&quot;Samples from the posterior predictive distribution.&quot;) plt.savefig(&#39;deepensemble.png&#39;, dpi=400) plt.show() . percentiles = df.quantile([0.05, 0.95], axis=1).T df.mean(1).plot(alpha=1, c=&#39;b&#39;) plt.plot(X_train.squeeze(), Y_train.squeeze(), &#39;.&#39;, alpha=0.1, c=&#39;r&#39;) plt.fill_between(X, percentiles.iloc[:,0], percentiles.iloc[:,1], color=&#39;lightblue&#39;) plt.title(&quot;Mean predictions with 90% CI&quot;); . MC Dropout . def enable_dropout(model): &quot;&quot;&quot; Function to enable the dropout layers during test-time &quot;&quot;&quot; for m in model.modules(): if m.__class__.__name__.startswith(&#39;Dropout&#39;): m.train() def predict_mc_dropout(model, X, T): &quot;Sample T times from the approximate posterior predictive distribution.&quot; Y_hat = torch.zeros(X.shape[0], T) with torch.no_grad(): model.eval() enable_dropout(model) for i in range(T): Y_hat[:, i] = model(X).squeeze() return Y_hat . import torch import torch.nn as nn import torch.nn.functional as F from torch import optim from tqdm import tqdm from collections import OrderedDict n_neurons = 200 dropout = 0.1 epochs = 1000 criterion = nn.MSELoss() # Train set X_train = torch.tensor(X_train).float() Y_train = torch.tensor(Y_train).float() # Validation set X_test = torch.tensor(X_test).float() Y_test = torch.tensor(Y_test).float() n_features = X_train.shape[1] model = Net(n_neurons, n_features, dropout) fit_model(model, epochs, X_train, Y_train, X_test, Y_test,) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor). from ipykernel import kernelapp as app /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor). app.launch_new_instance() /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor). /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor). . Y_hat = predict_mc_dropout(model, torch.tensor(X[:, None]).float(), T=1000) Y_hat_mean = Y_hat.mean(1) df = pd.DataFrame(Y_hat.numpy(), index=X) df.sort_index(inplace=True) df.plot(legend=False, c=&#39;b&#39;, alpha=0.01) plt.plot(X_train.squeeze(), Y_train.squeeze(), &#39;.&#39;, alpha=0.1, c=&#39;r&#39;) plt.xlabel(&quot;X&quot;) plt.ylabel(&quot;Y&quot;) plt.title(&quot;Samples from the posterior predictive distribution.&quot;); . PyMC . train_indexes = np.concatenate([np.arange(int(0.1*n), int(0.2*n)), np.arange(int(0.25*n), int(0.35*n)), np.arange(int(0.6*n),int(0.7*n)), np.arange(int(0.8*n),int(0.9*n))]) X_train, Y_train = X[train_indexes, None], Y[train_indexes, None] X_test, Y_test = X[~train_indexes, None], Y[~train_indexes, None] X_train.shape, Y_train.shape . ((400, 1), (400, 1)) . import pymc3 as pm import theano.tensor as T import theano import sklearn import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns.set_style(&#39;white&#39;) from sklearn import datasets from sklearn.preprocessing import scale from pymc3 import HalfNormal, GaussianRandomWalk, Bernoulli from pymc3.math import sigmoid import theano.tensor as tt from theano.tensor.slinalg import Cholesky import arviz as az . m = X_train.mean() s = X_train.std() ann_input = theano.shared((X_train-m)/s) ann_output = theano.shared(Y_train.squeeze()) n_hidden = [20]*4 n_data = X_train.shape[1] # Initialize random weights between each layer init_1 = np.random.randn(n_data, n_hidden[0]).astype(theano.config.floatX) init_2 = np.random.randn(n_hidden[0], n_hidden[1]).astype(theano.config.floatX) init_3 = np.random.randn(n_hidden[1], n_hidden[2]).astype(theano.config.floatX) init_4 = np.random.randn(n_hidden[2], n_hidden[3]).astype(theano.config.floatX) init_5 = np.random.randn(n_hidden[3]).astype(theano.config.floatX) bias_init_1 = np.random.randn(n_hidden[0]).astype(theano.config.floatX) bias_init_2 = np.random.randn(n_hidden[1]).astype(theano.config.floatX) bias_init_3 = np.random.randn(n_hidden[2]).astype(theano.config.floatX) bias_init_4 = np.random.randn(n_hidden[3]).astype(theano.config.floatX) bias_init_5 = np.random.randn() with pm.Model() as neural_network: weights_1 = pm.Normal(&#39;w_1&#39;, 0, sd=1, shape=(n_data, n_hidden[0]), testval=init_1) bias_1 = pm.Normal(&#39;b_1&#39;, 0, sd=1, shape=(n_hidden[0],), testval=bias_init_1) weights_2 = pm.Normal(&#39;w_2&#39;, 0, sd=1, shape=(n_hidden[0], n_hidden[1]), testval=init_2) bias_2 = pm.Normal(&#39;b_2&#39;, 0, sd=1, shape=(n_hidden[1],), testval=bias_init_2) weights_3 = pm.Normal(&#39;w_3&#39;, 0, sd=1, shape=(n_hidden[1], n_hidden[2]), testval=init_3) bias_3 = pm.Normal(&#39;b_3&#39;, 0, sd=1, shape=(n_hidden[2],), testval=bias_init_3) weights_4 = pm.Normal(&#39;w_4&#39;, 0, sd=1, shape=(n_hidden[2], n_hidden[3]), testval=init_4) bias_4 = pm.Normal(&#39;b_4&#39;, 0, sd=1, shape=(n_hidden[3],), testval=bias_init_4) weights_5 = pm.Normal(&#39;w_5&#39;, 0, sd=1, shape=(n_hidden[3],), testval=init_5) bias_5 = pm.Normal(&#39;b_5&#39;, 0, sd=1, testval=bias_init_5) # Build neural-network using relu activation function a = ann_input a = tt.nnet.relu(tt.dot(a, weights_1) + bias_1) a = tt.nnet.relu(tt.dot(a, weights_2) + bias_2) a = tt.nnet.relu(tt.dot(a, weights_3) + bias_3) a = tt.nnet.relu(tt.dot(a, weights_4) + bias_4) a = tt.dot(a, weights_5) + bias_5 sigma = pm.HalfNormal(&#39;sigma&#39;, sd=1) out = pm.Normal(&#39;out&#39;, a, sigma, observed=ann_output) . Prior predictive check . with neural_network: prior_checks = pm.sample_prior_predictive(samples=50) . plt.plot(prior_checks[&#39;out&#39;].T, alpha=0.3); . with neural_network: trace = pm.sample( init=&#39;advi+adapt_diag&#39;, draws=2000, tune=1000, chains=2, n_init=200000, target_accept= 0.9) . Auto-assigning NUTS sampler... INFO:pymc3:Auto-assigning NUTS sampler... Initializing NUTS using advi+adapt_diag... INFO:pymc3:Initializing NUTS using advi+adapt_diag... . . 21.94% [43885/200000 00:41&lt;02:26 Average Loss = 1,383] Convergence achieved at 44000 INFO:pymc3.variational.inference:Convergence achieved at 44000 Interrupted at 43,999 [21%]: Average Loss = 7.9308e+05 INFO:pymc3.variational.inference:Interrupted at 43,999 [21%]: Average Loss = 7.9308e+05 Sequential sampling (2 chains in 1 job) INFO:pymc3:Sequential sampling (2 chains in 1 job) NUTS: [sigma, b_5, w_5, b_4, w_4, b_3, w_3, b_2, w_2, b_1, w_1] INFO:pymc3:NUTS: [sigma, b_5, w_5, b_4, w_4, b_3, w_3, b_2, w_2, b_1, w_1] . . 100.00% [3000/3000 30:24&lt;00:00 Sampling chain 0, 5 divergences] . 100.00% [3000/3000 30:06&lt;00:00 Sampling chain 1, 3 divergences] Sampling 2 chains for 1_000 tune and 2_000 draw iterations (2_000 + 4_000 draws total) took 3631 seconds. INFO:pymc3:Sampling 2 chains for 1_000 tune and 2_000 draw iterations (2_000 + 4_000 draws total) took 3631 seconds. There were 5 divergences after tuning. Increase `target_accept` or reparameterize. ERROR:pymc3:There were 5 divergences after tuning. Increase `target_accept` or reparameterize. The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize. WARNING:pymc3:The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize. There were 8 divergences after tuning. Increase `target_accept` or reparameterize. ERROR:pymc3:There were 8 divergences after tuning. Increase `target_accept` or reparameterize. The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize. WARNING:pymc3:The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize. The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge. ERROR:pymc3:The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge. The estimated number of effective samples is smaller than 200 for some parameters. ERROR:pymc3:The estimated number of effective samples is smaller than 200 for some parameters. . # Alternatively, fit via VI # N = 200000 # with neural_network: # inference = pm.ADVI() # approx = pm.fit(n=N, method=inference) # trace = approx.sample(draws=1000) . # az.plot_trace(trace); . Posterior predictive check . m = X.mean() s = X.std() x = (X-m)/s ann_input.set_value(x[:,None]) ann_output.set_value(Y) ppc = pm.sample_posterior_predictive(trace, model=neural_network, samples=500) . /usr/local/lib/python3.6/dist-packages/pymc3/sampling.py:1708: UserWarning: samples parameter is smaller than nchains times ndraws, some draws and/or chains may not be represented in the returned posterior predictive sample &#34;samples parameter is smaller than nchains times ndraws, some draws &#34; . . 100.00% [500/500 00:07&lt;00:00] df = pd.DataFrame(ppc[&#39;out&#39;].T, index=X.squeeze()) df.sort_index(inplace=True) df.plot(legend=False, alpha=0.01, c=&#39;b&#39;) df.mean(1).plot(legend=False, alpha=1, c=&#39;0&#39;) plt.plot(X_train.squeeze(), Y_train.squeeze(), &#39;.&#39;, alpha=0.1, c=&#39;r&#39;) plt.xlabel(&quot;X&quot;) plt.ylabel(&quot;Y&quot;) plt.ylim(bottom=-5, top=5) plt.title(&quot;Samples from the posterior predictive distribution.&quot;); . Numpyro . import os from functools import partial import torch import pyro import pyro.distributions as dist # for CI testing smoke_test = (&#39;CI&#39; in os.environ) assert pyro.__version__.startswith(&#39;1.5.0&#39;) pyro.enable_validation(True) pyro.set_rng_seed(1) pyro.enable_validation(True) . &quot;&quot;&quot; Bayesian Neural Network ======================= Adapted from https://github.com/pyro-ppl/numpyro/blob/master/examples/bnn.py &quot;&quot;&quot; import argparse import os import time import matplotlib import matplotlib.pyplot as plt import numpy as np import pandas as pd from jax import vmap import jax.numpy as jnp import jax.random as random import numpyro from numpyro import handlers import numpyro.distributions as dist from numpyro.infer import MCMC, NUTS args = pd.Series({&#39;num_samples&#39;:4000, &#39;num_warmup&#39;:1000, &#39;num_chains&#39;:2, &#39;num_hidden&#39;:5,&#39;device&#39;:&#39;cpu&#39;}) numpyro.set_platform(platform=args.device) if args.device == &#39;cpu&#39;: numpyro.set_host_device_count(args.num_chains) # the non-linearity we use in our neural network def nonlin(x): return jnp.tanh(x) # helper function for HMC inference def run_inference(model, args, rng_key, X, Y, D_H): start = time.time() kernel = NUTS(model) mcmc = MCMC(kernel, args.num_warmup, args.num_samples, num_chains=args.num_chains, progress_bar=False if &quot;NUMPYRO_SPHINXBUILD&quot; in os.environ else True) mcmc.run(rng_key, X, Y, D_H) mcmc.print_summary() print(&#39; nMCMC elapsed time:&#39;, time.time() - start) return mcmc.get_samples() # helper function for prediction def predict(model, rng_key, samples, X, D_H): model = handlers.substitute(handlers.seed(model, rng_key), samples) # note that Y will be sampled in the model because we pass Y=None here model_trace = handlers.trace(model).get_trace(X=X, Y=None, D_H=D_H) return model_trace[&#39;Y&#39;][&#39;value&#39;] . def model(X, Y, D_H): D_X, D_Y = X.shape[1], 1 # sample first layer (we put unit normal priors on all weights) w1 = numpyro.sample(&quot;w1&quot;, dist.Normal(jnp.zeros((D_X, D_H)), jnp.ones((D_X, D_H)))) # D_X D_H z1 = nonlin(jnp.matmul(X, w1)) # N D_H &lt;= first layer of activations # sample second layer w2 = numpyro.sample(&quot;w2&quot;, dist.Normal(jnp.zeros((D_H, D_H)), jnp.ones((D_H, D_H)))) # D_H D_H z2 = nonlin(jnp.matmul(z1, w2)) # N D_H &lt;= second layer of activations # sample second layer w3 = numpyro.sample(&quot;w3&quot;, dist.Normal(jnp.zeros((D_H, D_H)), jnp.ones((D_H, D_H)))) # D_H D_H z3 = nonlin(jnp.matmul(z2, w3)) # sample second layer w4 = numpyro.sample(&quot;w4&quot;, dist.Normal(jnp.zeros((D_H, D_H)), jnp.ones((D_H, D_H)))) # D_H D_H z4 = nonlin(jnp.matmul(z3, w4)) # sample final layer of weights and neural network output w5 = numpyro.sample(&quot;w5&quot;, dist.Normal(jnp.zeros((D_H, D_Y)), jnp.ones((D_H, D_Y)))) # D_H D_Y z5 = jnp.matmul(z4, w5) # N D_Y &lt;= output of the neural network # we put a prior on the observation noise prec_obs = numpyro.sample(&quot;prec_obs&quot;, dist.Gamma(3.0, 1.0)) sigma_obs = 1.0 / jnp.sqrt(prec_obs) # observe data numpyro.sample(&quot;Y&quot;, dist.Normal(z5, sigma_obs), obs=Y) . N, D_H = len(X_train), args.num_hidden # do inference rng_key, rng_key_predict = random.split(random.PRNGKey(0)) samples = run_inference(model, args, rng_key, X_train, Y_train, D_H) . mean std median 5.0% 95.0% n_eff r_hat prec_obs 74.43 6.82 73.94 62.41 85.02 12.92 1.16 w1[0,0] 0.45 0.57 0.07 -0.07 1.27 1.29 2.07 w1[0,1] 0.39 0.61 0.02 -0.04 1.29 1.91 1.48 w1[0,2] -0.20 0.39 -0.04 -0.84 -0.01 4.82 1.59 w1[0,3] 0.10 0.30 -0.01 -0.24 0.62 1.69 1.73 w1[0,4] -0.16 0.77 0.09 -1.56 1.13 6.24 1.52 w2[0,0] -0.66 1.14 -0.53 -2.91 0.85 7.54 1.14 w2[0,1] -0.48 1.12 -0.56 -2.14 1.84 3.77 1.32 w2[0,2] 1.06 1.79 1.00 -1.41 3.83 1.34 2.01 w2[0,3] -0.13 1.09 -0.16 -1.84 1.71 10.93 1.11 w2[0,4] -0.32 1.01 -0.27 -1.84 1.42 31.02 1.03 w2[1,0] -0.95 1.49 -0.89 -3.35 1.30 1.77 1.60 w2[1,1] 0.78 0.95 0.66 -0.62 2.50 5.30 1.16 w2[1,2] -0.58 2.23 -0.19 -3.84 2.54 1.17 2.65 w2[1,3] 0.12 1.26 0.00 -1.95 2.13 12.10 1.11 w2[1,4] 0.41 1.18 0.50 -1.61 2.24 5.61 1.28 w2[2,0] -0.26 1.02 -0.26 -2.03 1.30 26.67 1.04 w2[2,1] 0.16 0.94 0.27 -1.39 1.56 13.88 1.12 w2[2,2] 0.78 2.00 0.67 -1.99 3.77 1.12 2.78 w2[2,3] 0.12 1.08 0.16 -1.81 1.76 26.37 1.05 w2[2,4] -0.30 0.98 -0.31 -1.88 1.28 47.48 1.04 w2[3,0] -0.03 1.16 -0.12 -1.94 1.78 13.93 1.13 w2[3,1] 0.02 1.12 0.06 -1.90 1.74 4.27 1.26 w2[3,2] -0.31 1.52 -0.56 -2.33 2.35 8.46 1.25 w2[3,3] -0.26 1.15 -0.32 -1.89 1.90 16.88 1.15 w2[3,4] -0.37 0.91 -0.42 -1.72 1.08 39.42 1.11 w2[4,0] 0.17 1.22 0.17 -1.64 2.29 8.60 1.43 w2[4,1] 0.48 1.23 0.63 -1.35 2.61 1.93 1.47 w2[4,2] -0.98 1.41 -1.25 -3.05 1.22 3.55 1.61 w2[4,3] -0.27 1.12 -0.35 -2.12 1.43 10.77 1.07 w2[4,4] 0.58 1.22 0.58 -1.52 2.54 8.98 1.31 w3[0,0] 0.99 1.26 0.94 -1.40 2.82 3.11 1.61 w3[0,1] -0.01 1.03 -0.10 -1.55 1.76 9.43 1.49 w3[0,2] 0.37 1.14 0.52 -1.53 2.23 3.26 1.28 w3[0,3] -0.57 1.29 -0.68 -2.41 1.62 10.75 1.44 w3[0,4] -0.51 1.14 -0.52 -2.49 1.21 5.96 1.25 w3[1,0] -0.03 0.87 -0.01 -1.64 1.19 34.68 1.05 w3[1,1] -0.64 1.02 -0.64 -2.28 0.99 5.67 1.21 w3[1,2] -0.22 1.12 -0.24 -1.98 1.66 24.13 1.08 w3[1,3] 0.86 1.14 0.98 -1.28 2.54 3.77 1.30 w3[1,4] 0.48 1.09 0.50 -1.34 2.23 5.35 1.15 w3[2,0] 0.90 1.28 0.82 -1.29 3.05 5.59 1.56 w3[2,1] 0.81 1.86 1.50 -2.48 3.07 4.96 1.77 w3[2,2] 0.95 1.36 0.91 -1.26 3.26 8.82 1.48 w3[2,3] -1.57 1.04 -1.71 -3.21 -0.11 7.04 1.18 w3[2,4] 0.14 1.49 0.12 -2.36 2.37 14.23 1.08 w3[3,0] 0.11 1.17 0.26 -2.09 1.85 4.70 1.22 w3[3,1] -0.39 1.11 -0.59 -1.98 1.68 11.38 1.26 w3[3,2] 0.19 1.05 0.21 -1.37 1.97 26.60 1.04 w3[3,3] 0.60 1.01 0.67 -1.03 2.24 20.14 1.02 w3[3,4] 0.63 1.05 0.71 -1.20 2.07 5.48 1.30 w3[4,0] 0.06 1.05 0.08 -1.68 1.76 38.89 1.08 w3[4,1] -0.16 1.07 -0.07 -1.95 1.49 12.98 1.18 w3[4,2] -0.10 1.11 -0.19 -2.03 1.68 12.82 1.15 w3[4,3] -0.07 1.18 -0.12 -1.81 2.01 6.78 1.18 w3[4,4] 0.61 1.22 0.72 -1.46 2.46 6.72 1.24 w4[0,0] -1.24 1.24 -1.44 -3.36 0.62 8.77 1.33 w4[0,1] 0.14 1.07 0.18 -1.70 1.73 28.55 1.08 w4[0,2] 0.19 1.04 0.35 -1.55 1.94 27.77 1.08 w4[0,3] 0.64 1.28 0.64 -1.57 2.55 16.53 1.11 w4[0,4] 0.03 0.97 0.09 -1.53 1.60 35.62 1.07 w4[1,0] 1.10 1.88 1.49 -2.24 3.68 3.51 1.89 w4[1,1] 0.44 1.27 0.65 -2.02 2.13 6.11 1.50 w4[1,2] 0.99 1.33 1.26 -1.27 3.11 2.80 1.50 w4[1,3] 0.61 1.28 0.65 -1.18 3.14 2.46 1.38 w4[1,4] -0.26 1.01 -0.23 -1.92 1.47 21.95 1.12 w4[2,0] 0.10 1.52 0.03 -2.05 2.78 16.39 1.13 w4[2,1] 0.47 1.11 0.47 -1.41 2.23 12.29 1.16 w4[2,2] -0.06 1.53 0.14 -2.49 2.39 4.28 1.37 w4[2,3] 0.16 1.21 0.32 -1.72 2.12 1.71 1.64 w4[2,4] 0.01 1.00 0.04 -1.57 1.65 33.52 1.07 w4[3,0] 0.17 1.81 0.65 -3.02 2.51 2.59 1.98 w4[3,1] -0.59 1.04 -0.61 -2.24 1.18 9.67 1.13 w4[3,2] 0.09 1.85 -0.19 -2.71 3.26 3.11 1.35 w4[3,3] -0.79 1.25 -0.76 -2.93 0.94 2.13 1.48 w4[3,4] 0.05 1.22 -0.03 -1.79 2.17 14.88 1.14 w4[4,0] -1.09 1.63 -1.05 -4.00 1.28 3.61 1.62 w4[4,1] -0.28 0.94 -0.30 -1.84 1.23 50.87 1.01 w4[4,2] -0.57 1.05 -0.53 -2.41 0.83 2.90 1.32 w4[4,3] -0.48 1.05 -0.48 -2.04 1.35 8.80 1.17 w4[4,4] 0.00 0.95 0.01 -1.53 1.57 27.00 1.18 w5[0,0] 1.90 0.39 1.86 1.28 2.47 21.71 1.13 w5[1,0] 0.45 1.36 0.51 -1.53 2.69 7.08 1.42 w5[2,0] -0.48 2.05 -1.50 -2.82 2.66 1.37 2.38 w5[3,0] 0.51 1.67 0.93 -2.09 2.87 1.24 2.34 w5[4,0] -0.66 1.09 -0.60 -2.48 1.11 17.29 1.22 Number of divergences: 537 MCMC elapsed time: 369.78590750694275 . # predict Y (train AND test) at inputs X vmap_args = (samples, random.split(rng_key_predict, args.num_samples * args.num_chains)) predictions = vmap(lambda samples, rng_key: predict(model, rng_key, samples, X[:,None], D_H))(*vmap_args) predictions = predictions[..., 0] . df = pd.DataFrame(predictions[:1000,:].T, index=X.squeeze()) df.sort_index(inplace=True) df.plot(legend=False, alpha=0.01, c=&#39;b&#39;) df.mean(1).plot(legend=False, alpha=1, c=&#39;0&#39;) plt.plot(X_train.squeeze(), Y_train.squeeze(), &#39;.&#39;, alpha=0.1, c=&#39;r&#39;) plt.xlabel(&quot;X&quot;) plt.ylabel(&quot;Y&quot;) plt.ylim(bottom=-5, top=5) plt.title(&quot;Samples from the posterior predictive distribution.&quot;); . # compute mean prediction and confidence interval around median mean_prediction = jnp.mean(predictions, axis=0) percentiles = np.percentile(predictions, [5.0, 95.0], axis=0) fig, ax = plt.subplots(1, 1) # plot training data ax.plot(X_train[:, 0], Y_train[:, 0], &#39;kx&#39;) # plot 90% confidence level of predictions ax.fill_between(X, percentiles[0, :], percentiles[1, :], color=&#39;lightblue&#39;) # plot mean prediction ax.plot(X, mean_prediction, &#39;blue&#39;, ls=&#39;solid&#39;, lw=2.0) ax.set(xlabel=&quot;X&quot;, ylabel=&quot;Y&quot;, title=&quot;Mean predictions with 90% CI&quot;) plt.tight_layout() plt.show() . Conlusion . Deep ensembles generate impressive uncertainty estimates and are computational feasible. They can be run in parallel at train and test time. Furthermore, they can be combined with other techniques like variational inference or SWAG, which tend to only sample from the posterior predictive distribution within one basin of attraction. .",
            "url": "https://jpwoeltjen.github.io/researchBlog/bayesian/deep%20learning/2020/11/04/bayesian_uncertainty.html",
            "relUrl": "/bayesian/deep%20learning/2020/11/04/bayesian_uncertainty.html",
            "date": " • Nov 4, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Portfolio Optimization within a Bayesian Framework",
            "content": "The Bayesian framework for portfolio optimization offers very important advantages compared to the classical approach. Among them are the following: . The Bayesian framework accounts for full uncertainty: . Aleatoric uncertainty: inherent variance of returns. | Epistemic uncertainty: parameter uncertainty due to limited data (at every step). Natural adjustment for parameter uncertainty of different assets (due to unequal amounts of data or otherwise weaker evidence). | Natural downscaling of allocation when new data point is far away from training data (data anomaly, financial crises, regime shift). | . | . | The specification of complex models with constraints is natural and estimation via MCMC straightforward. . | . The optimization problem . The general idea is to (numerically) maximize the expected utility based on a large number of sample draws from the posterior predictive distribution of returns: . begin{equation} begin{aligned} max _{ boldsymbol{ omega}} mathrm{E} left[U left( boldsymbol{ omega}^{ prime} mathbf{r}_{T+1} right) right]=&amp; max _{ boldsymbol{ omega}} int U left( boldsymbol{ omega}^{ prime} mathbf{r}_{T+1} right) p left( mathbf{r}_{T+1} mid mathbf{ mathcal{D}} right) mathrm{d} mathbf{r}_{T+1} text { subject to } &amp; boldsymbol{ omega}^{ prime} mathbf{1}=1, end{aligned} end{equation}where: . $ mathbf{r}_{T+1}$ : vector of returns at $T+1$. | $ mathbf{ mathcal{D}}$ : predictive and observable features. | $U( cdot)$ : utility function (quadratic utility is unrealistic; utility function should punish large losses but not large gains). | $ boldsymbol{ omega}$ : vector of portfolio weights. | $ mathbf{ Sigma}$ : covariance matrix of returns. | $ lambda$ : risk aversion parameter. | . If the utility function is quadratic: begin{equation} mathrm{E}[U( boldsymbol{ omega})]= boldsymbol{ omega}^{ prime} mathrm{E}( mathbf{r})- lambda boldsymbol{ omega}^{ prime} mathbf{ Sigma} boldsymbol{ omega}. end{equation} . Quadratic utility is often deemed inappropriate when returns aren&#39;t normal. For a skewed and heavy-tailed distribution, the variance is not an appropriate risk measure. In particular, investors dislike large losses but like large gains. . Quadratic utility may not be a bad choice, however, if portfolio positions are -- or can be made -- independent of each other, even if idividual stock returns aren&#39;t normal. If the variance minimizing weights perfectly hedge out risk factors (and all the dependence of stock returns are due to linear loadings on the identified risk factors only), the residual returns are independent. Then by a CLT, the portfolio returns are normal and the variance is the correct risk measure. . Implementation . To model high-dimensional returns with covariance, coskewness, and cokurtosis it is convenient (and also plausible) to assume a factor model. For each $t, t=1, ldots, T,$ estimate the cross-sectional regression and obtain the time series of factor return estimates $F,$ and specific return estimates $ widehat{E}$ $$ widehat{ mathbf{F}}= left( begin{array}{llll} widehat{f}_{1,1} &amp; widehat{f}_{1,2} &amp; cdots &amp; widehat{f}_{1, K} cdots &amp; cdots &amp; cdots &amp; cdots widehat{f}_{t, 1} &amp; widehat{f}_{t, 2} &amp; cdots &amp; widehat{f}_{t, K} cdots &amp; cdots &amp; cdots &amp; hat{f}_{T, K} end{array} right) $$ and $$ widehat{ mathbf{E}}= left( begin{array}{llll} widehat{e}_{1,1} &amp; widehat{e}_{1,2} &amp; cdots &amp; widehat{e}_{1, N} cdots &amp; cdots &amp; cdots &amp; cdots widehat{e}_{t, 1} &amp; widehat{e}_{t, 2} &amp; cdots &amp; widehat{e}_{t, N} cdots &amp; cdots &amp; cdots &amp; widehat{e}_{T, N} end{array} right) $$ respectively. . Predict (simulate) the expected returns, factor returns, and stock-specific returns and use them as drivers to compute scenarios of the stock returns according to $$ tilde{ mathbf{r}}= tilde{ boldsymbol{ mu}} +{ mathbf{B} tilde{ mathbf{f}}}+ tilde{ mathbf{e}} $$ where $ tilde{ boldsymbol{ mu}}$ is the matrix of simulated mean returns for $N$ assets, generated from the posterior predictive distribution of the conditional expectation model, and $ tilde{ mathbf{f}}$ and $ tilde{ mathbf{e}}$ are the predicted (simulated) factor and stock-specific returns, respectively. . Use high-frequency (HF) data to extact factors (including industries). | Use HF data to compute loadings. | Use Bayesian (realized) MV-GARCH to forecast factor returns. | Use Bayesian (realized) GARCH to forecast stock specific returns (if all factors are accounted for these can be assumed independent) | Estimate the posterior predictive distribution of the conditional expectation $ boldsymbol{ mu}$ (for example via a Bayesian neural network) and sample from it. | Simulate 10,000 MC draws according to $ tilde{ mathbf{r}}= tilde{ boldsymbol{ mu}} +{ mathbf{B} tilde{ mathbf{f}}}+ tilde{ mathbf{e}} $. . | Optimize (empirical; based on the sample draws) expected utility. . | . There are GARCH-type models that account for coskewness (GRJ-GARCH) and cokurtosis (BEKK). GARCH based on Student&#39;s t innovations are better able to capture heaviness of tails. Minimizing the expected shortfall subject to an expected return constraint is a valid portfolio optimization procedure in a non-Gaussian setting (where the standard deviation is not an appropriate risk measure). .",
            "url": "https://jpwoeltjen.github.io/researchBlog/probabilistic%20programming/bayesian/portfolio%20optimization/2020/10/01/bayesian_porfolio_optimization.html",
            "relUrl": "/probabilistic%20programming/bayesian/portfolio%20optimization/2020/10/01/bayesian_porfolio_optimization.html",
            "date": " • Oct 1, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Adaptive Bayesian Neural Network Based on Random Walk Priors",
            "content": "Nonstationary dynamics . Imagine the following dynamics of financial asset returns: . There exists a (nonlinear) predictive signal. | Market participants find it in the data. | Their efforts to profit from it causes the relationship to get arbitraged away. | Historic data still show evidence of the signal. | Slowly adapting traders still try to trade based on it. | This causes the signal to be predictive in the opposite direction. | . The nonlinear relationship at each time can be modeled by a deep neural network. Retraining the whole model for each time step independently is inefficient, though. Traditionally, transfer learning is used to increase data efficiency by training the lower layers of a neural network on some different but related data. The layers closer to the output layer are trained on the actual data of interest, while the layers closer to the input are either completely frozen or trained with a progressively smaller learning rate. . In a Bayesian framework, the step size of each layer can be modeled as a learnable parameter. . Low-level weights are constant; use all data to encode features. | But layers further towards the output layer need to smoothly adapt. | A principled way to update layers enables one to be one step ahead. | . Panel model: . $p$ assets. | $n$ time steps. | the optimal weights of the last layer change smoothly over time. | . Consider the neural network . $$ mathbf{y}_t = f( mathbf{ Theta}, mathbf{X}_t), $$where $f$ is a composition of functions and $ mathbf{ Theta}$ are the learnable parameters. We assume that our data is not iid and that $ mathbf{ Theta}$ is changing over time. We thus need a different $ mathbf{ Theta}_t$ for every $t$: . $$ mathbf{y}_t = f( mathbf{ Theta_t}, mathbf{X}_t) $$In order to avoid overfitting, $ mathbf{ Theta}_t$ needs to be constrained. We will assume that while $ mathbf{ Theta}_t$ is changing over time, it will do so rather gradually by placing a random-walk prior on it: . $$ mathbf{ Theta}_t sim mathcal{N}( mathbf{ Theta}_{t-1}, mathbf{ Sigma}) $$So $ mathbf{ Theta}_t$ is allowed to only deviate a little bit (determined by the step size $ mathbf{ Sigma}$) from its previous value $ mathbf{ Theta}_{t-1}$. $ mathbf{ Sigma}$ can be thought of as a stability parameter. Larger values imply a quickly changing relationship, whereas smaller values imply a slowly changing relationship. . This idea is based on an adaptive classifier proposed by Thomas Wiecki. For more details, refer to his excellent blog post: https://github.com/twiecki/WhileMyMCMCGentlySamples/blob/master/content/downloads/notebooks/random_walk_deep_net.ipynb . . Later we will implement an approximate Bayesian solution via a custom PyTorch layer that scales to large data sets. This solution uses two tricks. First, we use the fact that a Gaussian random walk can be written as the cumulative sum of Gaussian random variables. Second, we use the result that an $ ell_2$ regularization term corresponds to a Gaussian prior. Lastly, we compute a deep ensemble to approximate a Bayesian model average. . Unfortunately, the step size must then be chosen ad hoc via tuning of the $ ell_2$ penalaty factor. . Data Generation . The following snipped generates a nonlinear panel data set with time-varying ground truth parameter. . import matplotlib.animation as animation import numpy as np import matplotlib.pyplot as plt from mpl_toolkits import mplot3d # %matplotlib notebook # number of time steps (e.g. days) n = 100 # number of assets p = 100 #features # x = np.random.normal(0, 1, size=(n, p, 2)) x = np.random.uniform(-5, 5, size=(n, p, 2)) x1 = x[:, :, 0] x2 = x[:, :, 1] # coefficient beta = np.linspace(-0.5, 0.5, n) # ground truth model def f(x1, x2, beta): y = beta[:,None]*x1*x2 return y # dependent variable (log-returns) y = f(x1, x2, beta) + np.random.normal(0, 1/10, size=(n, p)) . Without a notion of time, there is no way to make profitable predictions. Plotting the dependent variable as a surface over the feature space through time results in the following animation: . Your browser does not support the video tag. From this animation it is clear that when time is respected, there is a way to make profitable predictions. . Definition of the model . The following code defines a Bayesian neural network with two hidden layers and random walk priors, which we hope is capable of modeling the time dependent surface. For each time step, we have a different matrix connecting the 2nd layer and the output layer. Since the regression equation is stacked, we have to repeat it $p$ times. . np.random.seed(123) X = x.reshape(n*p, 2) Y = y.reshape(n*p) ann_input = theano.shared(X) ann_output = theano.shared(Y) n_hidden = [10, 10] . init_1 = np.random.randn(X.shape[1], n_hidden[0]).astype(theano.config.floatX) init_2 = np.random.randn(n_hidden[0], n_hidden[1]).astype(theano.config.floatX) init_out = np.random.randn(n_hidden[1]).astype(theano.config.floatX) with pm.Model() as neural_network: # stability param of random walk step_size3 = pm.HalfNormal(&#39;step_size3&#39;, sd=10) # std of Gaussian likelihood sigma = pm.HalfNormal(&#39;sigma&#39;, sd=10) ##################### Input layer to hidden 1 ############################## weights_in_1 = pm.Normal(&#39;w1&#39;, mu=0, sd=1., shape=(1, X.shape[1], n_hidden[0]), testval=init_1) # repetition is needed to have consistent dimensions with the random walk # weights (one matrix for n time steps times p assets) of the last layer. weights_in_1_rep = tt.repeat(weights_in_1, ann_input.shape[0], axis=0) ##################### Hidden layer 1 to 2################################### weights_1_2 = pm.Normal(&#39;w2&#39;, mu=0, sd=1., shape=(1, n_hidden[0], n_hidden[1]), testval=init_2) weights_1_2_rep = tt.repeat(weights_1_2, ann_input.shape[0], axis=0) ##################### Hidden layer 2 to output layer ######################## weights_2_out = pm.GaussianRandomWalk(&#39;w3&#39;, sd=step_size3, shape=(n, n_hidden[1]), testval=np.tile(init_out, (n, 1)) ) weights_2_out_rep = tt.repeat(weights_2_out, p, axis=0) # Build neural-network using relu activation function act_1 = tt.nnet.relu(tt.batched_dot(ann_input, weights_in_1_rep)) act_2 = tt.nnet.relu(tt.batched_dot(act_1, weights_1_2_rep)) # linear output layer intercept = pm.Normal(&#39;intercept&#39;, mu=0, sd=10) act_out = tt.batched_dot(act_2, weights_2_out_rep) + intercept out = pm.Normal(&#39;out&#39;, act_out, sigma, observed=ann_output) . WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library. . Let&#39;s fit it via automatic differentiation variational inference. . N = 200000 with neural_network: inference = pm.ADVI() approx = pm.fit(n=N, method=inference) . Average Loss = -36,547: 86%|████████▌ | 172028/200000 [1:11:46&lt;11:48, 39.48it/s] . Buffered data was truncated after reaching the output size limit. . with neural_network: trace = approx.sample(draws=1000) . Evaluation . The plot below confirms that the weights are indeed changing over time. . plt.plot(trace[&#39;w3&#39;][200:, :, 0].T, alpha=.05, color=&#39;r&#39;); plt.plot(trace[&#39;w3&#39;][200:, :, 1].T, alpha=.05, color=&#39;b&#39;); plt.plot(trace[&#39;w3&#39;][200:, :, 2].T, alpha=.05, color=&#39;g&#39;); plt.plot(trace[&#39;w3&#39;][200:, :, 3].T, alpha=.05, color=&#39;c&#39;); plt.plot(trace[&#39;w3&#39;][200:, :, 4].T, alpha=.05, color=&#39;y&#39;); plt.xlabel(&#39;time&#39;); plt.ylabel(&#39;weights&#39;); plt.title(&#39;Optimal weights change over time&#39;); sns.despine(); . Now, let&#39;s sample from the posterior predictive distribution to generate predictions. . ppc = pm.sample_posterior_predictive(trace, model=neural_network, samples=1000) . 100%|██████████| 1000/1000 [00:27&lt;00:00, 36.45it/s] . y_pred = ppc[&#39;out&#39;].mean(0).reshape(n,p) . def mse(y_hat, y): return ((y_hat - y)**2).mean() mse(y_pred, y) . 0.2869693253919095 . Plotting the predictions as a surface and the targets as points, both evolving through time, we obtain the following animation: . Your browser does not support the video tag. It worked! The surface of predictions follows the data cloud of true outcomes closely through time. . Approximate Bayesian implementation via Pytorch . import math import torch import torch.nn as nn class RandomWalk(nn.Module): &quot;&quot;&quot; This class implements a random walk layer. When it is optimized with an l2 penalty, it is a Gaussian random walk, where the variance of the iid Gaussian increments is contolled via the l2 penalty factor. &quot;&quot;&quot; def __init__(self, size_in, size_out, n, p): super().__init__() self.size_in, self.size_out, self.n, self.p = size_in, size_out, n, p weights = torch.Tensor(n, size_out, size_in) self.weights = nn.Parameter(weights) bias = torch.Tensor(n, size_out) self.bias = nn.Parameter(bias) # initialize weights and biases nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights) bound = 1 / math.sqrt(fan_in) nn.init.uniform_(self.bias, -bound, bound) def forward(self, x): &quot;&quot;&quot;&quot; Dimensions: X : (np x 1 x size_in) W : (n x size_out x size_in) &quot;&quot;&quot; X = x.view(self.n, self.p, 1, self.size_in) W = torch.cumsum(self.weights, dim=0) XW = (X @ W.view(self.n,1,self.size_out,self.size_in).permute(0,1,3,2) ).view(self.n,self.p,self.size_out) XW = XW + torch.cumsum(self.bias[:, None, :], dim=0) return XW.view(self.n*self.p, self.size_out) . class AdaptiveNet(nn.Module): def __init__(self, n_neurons, n_features, dropout, n, p): super().__init__() self.fc1 = nn.Linear(n_features, n_neurons) self.fc2 = nn.Linear(n_neurons, n_neurons) self.fc3 = nn.Linear(n_neurons, n_neurons) self.fc4 = RandomWalk(n_neurons, 1, n, p) self.fc1bn = nn.BatchNorm1d(n_neurons) self.fc2bn = nn.BatchNorm1d(n_neurons) self.fc3bn = nn.BatchNorm1d(n_neurons) self.drop_layer = nn.Dropout(p=dropout) def forward(self, X): X = F.relu(self.fc1bn(self.fc1(X))) X = self.drop_layer(X) X = F.relu(self.fc2bn(self.fc2(X))) X = self.drop_layer(X) X = F.relu(self.fc3bn(self.fc3(X))) X = self.drop_layer(X) X = self.fc4(X) return X def fit_model(model, epochs, X, Y, X_valid, Y_valid): optimizer = optim.Adam(model.parameters(), weight_decay=0.0, lr=1e-3) for epoch in (range(epochs)): # trainig mode model = model.train() model.zero_grad() Y_hat = model(X) loss = criterion(Y_hat, Y) # Gaussian prior of random walk differences l2 = torch.sqrt(model.fc4.weights.pow(2).sum() + model.fc4.bias.pow(2).sum()) factor = 0.01 loss += factor * l2 loss.backward() optimizer.step() if (epoch+1) % 1000 ==0: with torch.no_grad(): model.eval() Y_hat = model(X_valid) print(f&#39;Epoch: {epoch+1} t Train loss: {loss} t Valid loss: {criterion(Y_hat, Y_valid)}&#39;) def predict(model, X): with torch.no_grad(): model.eval() Y_hat = model(X) return Y_hat N = 5 n_neurons = 5 dropout = 0. epochs = 10000 criterion = nn.MSELoss() # Train set X = x.reshape(n*p, 2) Y = y.reshape(n*p)[:,None] X_train = torch.tensor(X).float() Y_train = torch.tensor(Y).float() # Validation set # Here we actually test on the training set. # Ideally train/test split should be online. X_test = torch.tensor(X).float() Y_test = torch.tensor(Y).float() n_features = X_train.shape[1] models = [AdaptiveNet(n_neurons, n_features, dropout, n=n, p=p) for i in range(N)] for i, model in enumerate(models): print(f&#39;Fitting model {i} ...&#39;) fit_model(model, epochs, X_train, Y_train, X_test, Y_test) . Fitting model 0 ... Epoch: 1000 Train loss: 0.3664185106754303 Valid loss: 0.3102811872959137 Epoch: 2000 Train loss: 0.15169930458068848 Valid loss: 0.10498853772878647 Epoch: 3000 Train loss: 0.0856589525938034 Valid loss: 0.0457574762403965 Epoch: 4000 Train loss: 0.06329291313886642 Valid loss: 0.02981697954237461 Epoch: 5000 Train loss: 0.04943346977233887 Valid loss: 0.020191699266433716 Epoch: 6000 Train loss: 0.03954114019870758 Valid loss: 0.021761026233434677 Epoch: 7000 Train loss: 0.03460316359996796 Valid loss: 0.01445063017308712 Epoch: 8000 Train loss: 0.03226261958479881 Valid loss: 0.01735026016831398 Epoch: 9000 Train loss: 0.03013009764254093 Valid loss: 0.014183185063302517 Epoch: 10000 Train loss: 0.02876121550798416 Valid loss: 0.021244391798973083 Fitting model 1 ... Epoch: 1000 Train loss: 0.5104950666427612 Valid loss: 0.4489796757698059 Epoch: 2000 Train loss: 0.2661382555961609 Valid loss: 0.20915427803993225 Epoch: 3000 Train loss: 0.1631760150194168 Valid loss: 0.11135413497686386 Epoch: 4000 Train loss: 0.10610751807689667 Valid loss: 0.059995386749506 Epoch: 5000 Train loss: 0.07790811359882355 Valid loss: 0.037965673953294754 Epoch: 6000 Train loss: 0.06104784831404686 Valid loss: 0.028306856751441956 Epoch: 7000 Train loss: 0.0492624007165432 Valid loss: 0.022823812440037727 Epoch: 8000 Train loss: 0.04007012024521828 Valid loss: 0.01837974600493908 Epoch: 9000 Train loss: 0.036449819803237915 Valid loss: 0.016985943540930748 Epoch: 10000 Train loss: 0.03362405672669411 Valid loss: 0.017896689474582672 Fitting model 2 ... Epoch: 1000 Train loss: 0.9739128947257996 Valid loss: 0.916059672832489 Epoch: 2000 Train loss: 0.45656928420066833 Valid loss: 0.4400976598262787 Epoch: 3000 Train loss: 0.30080828070640564 Valid loss: 0.24016711115837097 Epoch: 4000 Train loss: 0.22962310910224915 Valid loss: 0.16848084330558777 Epoch: 5000 Train loss: 0.1840335726737976 Valid loss: 0.12302099913358688 Epoch: 6000 Train loss: 0.14510247111320496 Valid loss: 0.08211981505155563 Epoch: 7000 Train loss: 0.12599343061447144 Valid loss: 0.06273797899484634 Epoch: 8000 Train loss: 0.11399130523204803 Valid loss: 0.05010528117418289 Epoch: 9000 Train loss: 0.10665198415517807 Valid loss: 0.04497310519218445 Epoch: 10000 Train loss: 0.09907227009534836 Valid loss: 0.0343887098133564 Fitting model 3 ... Epoch: 1000 Train loss: 0.5274525284767151 Valid loss: 0.46602508425712585 Epoch: 2000 Train loss: 0.29005444049835205 Valid loss: 0.2332426756620407 Epoch: 3000 Train loss: 0.192952960729599 Valid loss: 0.14019721746444702 Epoch: 4000 Train loss: 0.13883166015148163 Valid loss: 0.10573847591876984 Epoch: 5000 Train loss: 0.10494367778301239 Valid loss: 0.06143864244222641 Epoch: 6000 Train loss: 0.08329556882381439 Valid loss: 0.04145120084285736 Epoch: 7000 Train loss: 0.06947709619998932 Valid loss: 0.03530387580394745 Epoch: 8000 Train loss: 0.06028853356838226 Valid loss: 0.03136395663022995 Epoch: 9000 Train loss: 0.05302805453538895 Valid loss: 0.030534470453858376 Epoch: 10000 Train loss: 0.047490864992141724 Valid loss: 0.02806430123746395 Fitting model 4 ... Epoch: 1000 Train loss: 0.45621055364608765 Valid loss: 0.3979698419570923 Epoch: 2000 Train loss: 0.2321588546037674 Valid loss: 0.1791723221540451 Epoch: 3000 Train loss: 0.14385493099689484 Valid loss: 0.09565398097038269 Epoch: 4000 Train loss: 0.09529533982276917 Valid loss: 0.05243904888629913 Epoch: 5000 Train loss: 0.0702160894870758 Valid loss: 0.03319921717047691 Epoch: 6000 Train loss: 0.055079638957977295 Valid loss: 0.02398517169058323 Epoch: 7000 Train loss: 0.04642944782972336 Valid loss: 0.020928673446178436 Epoch: 8000 Train loss: 0.041729748249053955 Valid loss: 0.021932894363999367 Epoch: 9000 Train loss: 0.037777334451675415 Valid loss: 0.01956726238131523 Epoch: 10000 Train loss: 0.03581549972295761 Valid loss: 0.020492559298872948 . y_hat_list = [] for model in models: y_hat_list.append((predict(model, X_train)).numpy().squeeze()) y_hat = np.mean(y_hat_list, 0) . mse = ((y_hat.squeeze() - Y.squeeze())**2).mean() print(&quot;MSE:&quot;, mse) . MSE: 0.014255865492199475 . plt.plot(Y.squeeze(), y_hat, &#39;.&#39;) plt.title(&quot;Predictions vs. targets&quot;) plt.ylabel(&#39;$ hat{y}$&#39;) plt.xlabel(&#39;$y$&#39;) . Text(0.5, 0, &#39;$y$&#39;) . for model in models: rw = model.fc4.weights.detach().numpy()[:,:,:].cumsum(0) plt.plot(rw.reshape(n, rw.shape[1]*rw.shape[2]), alpha=.5,) plt.gca().set_prop_cycle(None) #break after 1 model as weights in randomly initialized models # will not perform the same job break . for model in models: rw_bias = model.fc4.bias.detach().numpy()[:].cumsum(0) plt.plot(rw_bias, alpha=.5,) break plt.plot() . [] . Your browser does not support the video tag.",
            "url": "https://jpwoeltjen.github.io/researchBlog/efficiency/time%20series/deep%20learning/2020/08/28/random_walk_deep_net_reg.html",
            "relUrl": "/efficiency/time%20series/deep%20learning/2020/08/28/random_walk_deep_net_reg.html",
            "date": " • Aug 28, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Hierarchical Bayesian Neural Network for Efficient High-dimensional Asset Return Prediction.",
            "content": "Financial assets exhibit hierarchical structure. They belong to classes such as stocks or bonds, are country specific, belong to sectors and industries, and load on other common risk factors. . Stocks may have different conditional expectations given their sector, for example. To build performant models, these differences need to be modeled. . In the following, a panel data set of asset returns is simulated. By construction, the unconditional expectation (i.e., the expectation ignoring the dependence on sectors) is equal to zero. Hence, a pooled model is unable to fit the data. . There are different ways to fit the model: . fit pooled data (here: impossible to achieve a good fit.) | fit data independently per sector (inefficient) | encode sector as feature and fit pooled data | use hierarchical model | . Why is there a need for hierarchical models when neural networks are very good at learning asset encodings internally given relevant features? Consider what happens if each asset has a different and independent data generating function. Then you would need an one-hot encoding matrix with dimensions $np times p$. If $n$ is not very large relative to $p$, the asset encoding might be hard to learn. A Bayesian approach allows each asset specific neural network to deviate from the group model if and only if there is sufficient evidence to conclude that it follows a different data generating process. . In a Bayesian framework we could also model the multiple levels of the hierarchical structure explicitly. The overall net may serve as a prior for sector nets, which, in turn, may serve as priors for asset specific nets. . def mse(y_hat, y): return ((y_hat - y)**2).mean() . def get_ir(log_rets, Y_hat, inv_cov_tensor): w = torch.einsum(&#39;np, npj -&gt; nj&#39;, Y_hat, inv_cov_tensor).detach().numpy() opt_weights = w / np.abs(w).sum(axis=1)[:, None] portfolio_ret = np.einsum(&#39;ij, ij-&gt;i&#39;, opt_weights, log_rets) ir = np.mean(portfolio_ret) / np.std(portfolio_ret) * np.sqrt(252) return ir, portfolio_ret . Data generation . The following snipped generates a panel data set with sector dependent nonlinear conditional expectation. You may want to open it in Colab and play with different valuess for n_stocks, n_days, n_ind. . def add_daily_trend_to_log_price(log_price, feature_list, beta_list): &quot;&quot;&quot; This function adds a daily trend to high-frequency simulations. &quot;&quot;&quot; for beta, feature in zip(beta_list, feature_list): signal = pd.DataFrame(log_price).copy() signal.iloc[:, :] = np.nan for i in set(signal.index.date): for c in signal.columns: signal.loc[signal.index.date == i, c] = feature[feature.index == i][c].values * beta / n_periods signal_cumsum = signal.cumsum() log_price += signal_cumsum # Hyperparams n_stocks = 50 n_days = 100 n_ind = n_stocks//2 pct_train = 0.5 factor_loadings = np.linspace(1, 3, n_stocks)[:, None] npi = int(n_stocks/n_ind) industry_loadings = np.zeros((n_stocks, n_ind)) for i in range(n_ind): industry_loadings[i*npi:(i+1)*npi, i] = np.ones(npi)[:] # beta feature_beta = industry_loadings @ np.linspace(-1,1, n_ind) # np.random.shuffle(feature_beta) # Use only intraday returns for cov estimation d = 1 # High-frequency prices are irregularly sampled # proportion of sampled prices liq = 1#0.8 # Prices are contaminated with micro structure noise # 10ct per $100 ms noise gamma = 0#0.1/100 # mins per day n_periods = 6.5*60 # GARCH(1,1) spec sigma_sq_0 = 0.1**2/250/n_periods garch_alpha = 0#0.019 garch_beta = 0#0.98 omega = sigma_sq_0*(1-garch_alpha-garch_beta) # preaveraging parameters k = int(0.5*(n_periods)**(0.5)) pairwise = True # NERIVE parameters L = 4 stp = hd._get_partitions(L) # condition number targeting parameters max_cond_target = 800 max_iter = 100 step = 0.05 estimators = { # &#39;realized_cov&#39;: partial(hf.mrc, pairwise=False, theta=0), # &#39;msrc&#39;: partial(hf.msrc, pairwise=pairwise, M=M, N=N), &#39;mrc&#39;: partial(hf.mrc, pairwise=pairwise, theta=None, k=k), # &#39;hy&#39;: partial(hf.hayashi_yoshida, theta=None, k=k), # &#39;krvm&#39;: partial(hf.krvm, H=H, pairwise=pairwise, # kernel=hf.parzen_kernel), # &#39;epic&#39;: partial(hf.ensemble, var_weights=np.ones(4)/4, # cov_weights=np.ones(4)/4), } u = sim.Universe(0, [sigma_sq_0, 0, garch_alpha, garch_beta, omega], [sigma_sq_0, 0, garch_alpha, garch_beta, omega], [sigma_sq_0, 0, garch_alpha, garch_beta, omega], factor_loadings, industry_loadings, liq, gamma, &#39;m&#39;, ) u.simulate(n_days) price = u.price feature_daily = np.random.normal(0, 0.1, n_days*n_stocks).reshape(n_days, n_stocks) feature2_daily = np.random.normal(0, 0.1, n_days*n_stocks).reshape(n_days, n_stocks) # add a daily predictable mean to high-frequency data signal = feature_beta * feature_daily * feature2_daily signal_df = pd.DataFrame(signal, index=pd.Series(list(set(price.index.date))).sort_values(), columns=price.columns) log_price = np.log(price) add_daily_trend_to_log_price(log_price, [signal_df], [1.]) price = np.exp(log_price) price_daily_close = price.resample(&#39;1d&#39;).last() price_daily_open = price.resample(&#39;1d&#39;).first() log_rets_daily = np.log(price_daily_close) - np.log(price_daily_open) X = torch.tensor([feature_daily, feature2_daily]).permute(1, 0, 2).float() Y = torch.tensor(log_rets_daily.values).float() n_time_steps_train = int(n_days*pct_train) n_time_steps_valid = n_days - n_time_steps_train covs_nerive = {key: [] for key in estimators.keys()} # Compute the integrated covariance matrices. for i in log_rets_daily.index.date[:n_time_steps_train]: p = price[(price.index.date &lt; i+timedelta(days=d))&amp;(price.index.date &gt; i-timedelta(days=d))] l = [hf.get_cumu_demeaned_resid(p.iloc[:, j]) for j in range(price.shape[1])] for name, estimator in estimators.items(): cov = estimator(l) # covs[name].append(cov) if max_cond_target is None: covs_nerive[name].append(hd.nerive(l, estimator=estimator, stp=stp)) else: covs_nerive[name].append(hd.linear_shrink_target( hd.nerive(l, estimator=estimator, stp=stp), max_cond_target, step, max_iter)) # split into train and valid set Y_train = Y[:n_time_steps_train, :] X_train = X[:n_time_steps_train, :, :] Y_valid = Y[n_time_steps_train:, :] X_valid = X[n_time_steps_train:, :, :] # normalize train_mean = X_train.mean(keepdim=True, dim=(0, 2)) train_std = X_train.std(keepdim=True, dim=(0, 2)) X_train = (X_train - train_mean) / train_std X_valid = (X_valid - train_mean) / train_std . /usr/local/lib/python3.6/dist-packages/hfhd/hf.py:1204: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 1d, C), array(float64, 1d, A)) cov[i, i] = _mrc(data, theta, g, bias_correction, k)[0, 0] /usr/local/lib/python3.6/dist-packages/numba/np/ufunc/parallel.py:363: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION &gt;= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled. warnings.warn(problem) . inv_cov_tensor = torch.tensor([np.linalg.inv(c) for c in u.cond_cov()])[n_time_steps_train:] . The following covariance matrix and dendrogram show the hierarchical structure of asset returns. . cov_avg = np.array(covs_nerive[&#39;mrc&#39;])[:n_time_steps_train].mean(0) corr = hd.to_corr(cov_avg) sns.clustermap(corr) . &lt;seaborn.matrix.ClusterGrid at 0x7fba4f204780&gt; . from scipy.cluster.hierarchy import ward, dendrogram linkage_matrix = ward(corr) fig, ax = plt.subplots(figsize=(10, 10)) ax = dendrogram(linkage_matrix, orientation=&quot;right&quot;); plt.tick_params( axis= &#39;x&#39;, which=&#39;both&#39;, bottom=&#39;off&#39;, top=&#39;off&#39;, labelbottom=&#39;off&#39; ) . By construction the returns are not predictable without conditioning on the sector. The figure below illustrates this. . n = n_time_steps_train p = n_stocks x1 = X_train[:,0,:] x2 = X_train[:,1,:] y = Y_train fig = plt.figure() ax = plt.axes(projection=&#39;3d&#39;) ax.set_xlabel(&#39;$x_1$&#39;) ax.set_ylabel(&#39;$x_2$&#39;) ax.set_zlabel(&#39;y&#39;); ax.scatter(x1, x2, y, c=y, cmap=&#39;plasma&#39;, linewidth=0.0); plt.title(&quot;Log-returns without conditioning on sectors.&quot;) plt.show() . x1_train = np.empty((n_ind, X_train.shape[0], npi)) x2_train = np.empty_like(x1_train) y_train = np.empty_like(x1_train) x1_valid = np.empty((n_ind, X_valid.shape[0], npi)) x2_valid = np.empty_like(x1_valid) y_valid = np.empty_like(x1_valid) for i in range(n_ind): x1_train[i,:,:] = X_train[:,0,i*npi:(i+1)*npi] x2_train[i,:,:] = X_train[:,1,i*npi:(i+1)*npi] y_train[i,:,:] = Y_train[:, i*npi:(i+1)*npi] x1_valid[i,:,:] = X_valid[:,0,i*npi:(i+1)*npi] x2_valid[i,:,:] = X_valid[:,1,i*npi:(i+1)*npi] y_valid[i,:,:] = Y_valid[:, i*npi:(i+1)*npi] . The following figure shows the train set data points per sector. There are not enough observations to fit a seperate neural network to each sector without informative priors. . fig = plt.figure(figsize=(13,13)) fig.suptitle(&#39;Train set per sector.&#39;, fontsize=20) for i in range(n_ind): ax = fig.add_subplot(np.ceil(np.sqrt(n_ind)), np.ceil(np.sqrt(n_ind)), i+1, projection=&#39;3d&#39;) ax.set_xlabel(&#39;$x_1$&#39;, fontsize=8) ax.set_ylabel(&#39;$x_2$&#39;, fontsize=8) ax.set_zlabel(&#39;y&#39;, fontsize=8); ax.tick_params(labelsize=8); ax.set_xlim(-2, 2) ax.set_ylim(-2, 2) ax.set_zlim(-0.01, 0.01) ax.scatter(x1_train[i,:,:], x2_train[i,:,:], y_train[i,:,:], c=y_train[i,:,:], cmap=&#39;viridis&#39;, linewidth=0.0); plt.show() . The next figure shows the test set data points per sector. . fig = plt.figure(figsize=(13,13)) fig.suptitle(&#39;Test set per sector.&#39;, fontsize=20) for i in range(n_ind): ax = fig.add_subplot(np.ceil(np.sqrt(n_ind)), np.ceil(np.sqrt(n_ind)), i+1, projection=&#39;3d&#39;) ax.set_xlabel(&#39;$x_1$&#39;, fontsize=8) ax.set_ylabel(&#39;$x_2$&#39;, fontsize=8) ax.set_zlabel(&#39;y&#39;, fontsize=8); ax.tick_params(labelsize=8); ax.set_xlim(-2, 2) ax.set_ylim(-2, 2) ax.set_zlim(-0.01, 0.01) ax.scatter(x1_valid[i,:,:], x2_valid[i,:,:], y_valid[i,:,:], c=y_valid[i,:,:], cmap=&#39;viridis&#39;, linewidth=0.0); plt.show() . Hierarchical model . The following code defines a hierarchical Bayesian neural network with two hidden layers. . # This idea comes from: # https://twiecki.io/blog/2018/08/13/hierarchical_bayesian_neural_network/. # Non-centered specification of hierarchical model: # see https://twiecki.github.io/blog/2017/02/08/bayesian-hierchical-non-centered/ # Why?: # https://www.youtube.com/watch?v=gSd1msFFZTw X_train = np.stack((x1_train.reshape(n_ind, npi*n_time_steps_train), x2_train.reshape(n_ind, npi*n_time_steps_train)), 2) Y_train = y_train.reshape(n_ind, npi*n_time_steps_train) ann_input = theano.shared(X_train) ann_output = theano.shared(Y_train) n_hidden = [10, 10] n_data = X_train.shape[2] n_grps = n_ind # Initialize random weights between each layer init_1 = np.random.randn(n_data, n_hidden[0]).astype(theano.config.floatX) init_2 = np.random.randn(n_hidden[0], n_hidden[1]).astype(theano.config.floatX) init_out = np.random.randn(n_hidden[1]).astype(theano.config.floatX) with pm.Model() as neural_network: # Group mean distribution for input to hidden layer weights_in_1_grp = pm.Normal(&#39;w_in_1_grp&#39;, 0, sd=1, shape=(n_data, n_hidden[0]), testval=init_1) # Group standard-deviation weights_in_1_grp_sd = pm.HalfNormal(&#39;w_in_1_grp_sd&#39;, sd=1.) # Group mean distribution for weights from 1st to 2nd layer weights_1_2_grp = pm.Normal(&#39;w_1_2_grp&#39;, 0, sd=1, shape=(n_hidden[0], n_hidden[1]), testval=init_2) weights_1_2_grp_sd = pm.HalfNormal(&#39;w_1_2_grp_sd&#39;, sd=1.) # Group mean distribution from hidden layer to output weights_2_out_grp = pm.Normal(&#39;w_2_out_grp&#39;, 0, sd=1, shape=(n_hidden[1],), testval=init_out) weights_2_out_grp_sd = pm.HalfNormal(&#39;w_2_out_grp_sd&#39;, sd=1.) # Separate weights for each different model, just add a 3rd dimension # of weights weights_in_1_raw = pm.Normal(&#39;w_in_1&#39;, shape=(n_grps, n_data, n_hidden[0])) weights_in_1 = weights_in_1_raw * weights_in_1_grp_sd + weights_in_1_grp weights_1_2_raw = pm.Normal(&#39;w_1_2&#39;, shape=(n_grps, n_hidden[0], n_hidden[1])) weights_1_2 = weights_1_2_raw * weights_1_2_grp_sd + weights_1_2_grp weights_2_out_raw = pm.Normal(&#39;w_2_out&#39;, shape=(n_grps, n_hidden[1])) weights_2_out = weights_2_out_raw * weights_2_out_grp_sd + weights_2_out_grp # Build neural-network using relu activation function act_1 = tt.nnet.relu(tt.batched_dot(ann_input, weights_in_1)) act_2 = tt.nnet.relu(tt.batched_dot(act_1, weights_1_2)) # linear output layer intercept = pm.Normal(&#39;intercept&#39;, mu=0, sd=10) act_out = tt.batched_dot(act_2, weights_2_out) + intercept sigma = pm.HalfNormal(&#39;sigma&#39;, sd=10) out = pm.Normal(&#39;out&#39;, act_out, sigma, observed=ann_output) . WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library. . with neural_network: trace = pm.sample(init=&#39;advi+adapt_diag&#39;, n_init=200000, tune=50, chains=1, nuts_kwargs={&#39;target_accept&#39;: 0.9},) . Auto-assigning NUTS sampler... Initializing NUTS using advi+adapt_diag... Average Loss = -6,472.8: 100%|██████████| 200000/200000 [07:55&lt;00:00, 420.34it/s] Finished [100%]: Average Loss = -6,472.8 Sequential sampling (1 chains in 1 job) NUTS: [sigma, intercept, w_2_out, w_1_2, w_in_1, w_2_out_grp_sd, w_2_out_grp, w_1_2_grp_sd, w_1_2_grp, w_in_1_grp_sd, w_in_1_grp] 100%|██████████| 550/550 [09:03&lt;00:00, 1.01it/s] There were 9 divergences after tuning. Increase `target_accept` or reparameterize. The acceptance probability does not match the target. It is 0.4623924185795791, but should be close to 0.9. Try to increase the number of tuning steps. Only one chain was sampled, this makes it impossible to run some convergence checks . Train set performance . ppc = pm.sample_posterior_predictive(trace, model=neural_network, samples=1000) y_pred = ppc[&#39;out&#39;].mean(0) . 100%|██████████| 1000/1000 [00:18&lt;00:00, 53.97it/s] . mse(y_pred, Y_train) . 0.00024471051017955266 . Test set performance . X_valid = np.stack((x1_valid.reshape(n_ind, npi*n_time_steps_valid), x2_valid.reshape(n_ind, npi*n_time_steps_valid)), 2) Y_valid = y_valid.reshape(n_ind, npi*n_time_steps_valid) ann_input.set_value(X_valid) ann_output.set_value(Y_valid) ppc = pm.sample_posterior_predictive(trace, model=neural_network, samples=1000) y_hat_valid = ppc[&#39;out&#39;].mean(0) . 100%|██████████| 1000/1000 [00:15&lt;00:00, 66.46it/s] . mse(y_hat_valid, Y_valid) . 0.00035460431075137717 . The figure below shows the predictions (green) and targets (blue) per sector for the hierarchical model. . fig = plt.figure(figsize=(13,13)) fig.suptitle(&#39;Predictions per sector.&#39;, fontsize=20) for i in range(n_ind): ax = fig.add_subplot(np.ceil(np.sqrt(n_ind)), np.ceil(np.sqrt(n_ind)), i+1, projection=&#39;3d&#39;) ax.set_xlabel(&#39;$x_1$&#39;, fontsize=8) ax.set_ylabel(&#39;$x_2$&#39;, fontsize=8) ax.set_zlabel(&#39;y&#39;, fontsize=8); ax.tick_params(labelsize=8); ax.set_xlim(-2, 2) ax.set_ylim(-2, 2) ax.set_zlim(-0.01, 0.01) ax.scatter(x1_valid[i,:,:], x2_valid[i,:,:], y_hat_valid[i,:], #c=y_pred[i,:], #cmap=&#39;Greens&#39;, c=&#39;green&#39;, linewidth=0.0); ax.scatter(x1_valid[i,:,:], x2_valid[i,:,:], y_valid[i,:,:], #c=y_valid[i,:,:], #cmap=&#39;gray&#39;, c=&#39;blue&#39;, linewidth=0.0); plt.show() . ir, rets = get_ir(log_rets=torch.tensor(Y_valid.reshape(n_stocks, n_time_steps_valid)).T, Y_hat=torch.tensor(y_hat_valid.reshape(n_stocks, n_time_steps_valid)).T, inv_cov_tensor=inv_cov_tensor) . print(&#39;Test set information ratio:&#39;, ir) plt.plot(np.cumsum(rets)) plt.title(&#39;Test set cumulative portfolio log-returns&#39;) plt.xlabel(&#39;days&#39;) plt.ylabel(&#39;cumulative portfolio log-returns&#39;) . Test set information ratio: 23.54194763228627 . Text(0, 0.5, &#39;cumulative portfolio log-returns&#39;) . Industry one-hot encoding as feature . X_train = np.stack((x1_train.reshape(n_ind*npi*n_time_steps_train), x2_train.reshape(n_ind*npi*n_time_steps_train)), 1) # Comment the following line fit pooled model without industry feature X_train = np.column_stack((X_train, np.repeat(industry_loadings, n_time_steps_train, 0))) Y_train = y_train.reshape(n_ind*npi*n_time_steps_train) ann_input = theano.shared(X_train) ann_output = theano.shared(Y_train) # n_hidden = [10, 5] n_data = X_train.shape[1] # Initialize random weights between each layer init_1 = np.random.randn(n_data, n_hidden[0]).astype(theano.config.floatX) init_2 = np.random.randn(n_hidden[0], n_hidden[1]).astype(theano.config.floatX) init_out = np.random.randn(n_hidden[1]).astype(theano.config.floatX) with pm.Model() as neural_network: weights_in_1 = pm.Normal(&#39;w_in_1_grp&#39;, 0, sd=1, shape=(n_data, n_hidden[0]), testval=init_1) weights_1_2 = pm.Normal(&#39;w_1_2_grp&#39;, 0, sd=1, shape=(n_hidden[0], n_hidden[1]), testval=init_2) weights_2_out = pm.Normal(&#39;w_2_out_grp&#39;, 0, sd=1, shape=(n_hidden[1],), testval=init_out) # Build neural-network using relu activation function act_1 = tt.nnet.relu(tt.dot(ann_input, weights_in_1)) act_2 = tt.nnet.relu(tt.dot(act_1, weights_1_2)) # linear output layer intercept = pm.Normal(&#39;intercept&#39;, mu=0, sd=10) act_out = tt.dot(act_2, weights_2_out) + intercept sigma = pm.HalfNormal(&#39;sigma&#39;, sd=10) out = pm.Normal(&#39;out&#39;, act_out, sigma, observed=ann_output) . with neural_network: trace = pm.sample( init=&#39;advi+adapt_diag&#39;, tune=50, chains=1, n_init=200000, nuts_kwargs={&#39;target_accept&#39;: 0.9},) . Auto-assigning NUTS sampler... Initializing NUTS using advi+adapt_diag... WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library. Average Loss = -5,807.8: 21%|██▏ | 42666/200000 [02:02&lt;07:32, 348.00it/s] Convergence achieved at 42700 Interrupted at 42,699 [21%]: Average Loss = -824.93 Sequential sampling (1 chains in 1 job) NUTS: [sigma, intercept, w_2_out_grp, w_1_2_grp, w_in_1_grp] 100%|██████████| 550/550 [11:10&lt;00:00, 1.22s/it] There were 77 divergences after tuning. Increase `target_accept` or reparameterize. The acceptance probability does not match the target. It is 0.5934816874673563, but should be close to 0.9. Try to increase the number of tuning steps. The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize. Only one chain was sampled, this makes it impossible to run some convergence checks . Train set performance . ppc = pm.sample_posterior_predictive(trace, model=neural_network, samples=1000) y_hat = ppc[&#39;out&#39;].mean(0).reshape(n_ind, npi*n_time_steps_train) mse(y_hat, Y_train.reshape(n_ind, npi*n_time_steps_train)) . 100%|██████████| 1000/1000 [00:24&lt;00:00, 40.70it/s] . 0.00023132680868411578 . Test set performance . X_valid = np.stack((x1_valid.reshape(n_ind*npi*n_time_steps_valid), x2_valid.reshape(n_ind*npi*n_time_steps_valid)), 1) X_valid = np.column_stack((X_valid, np.repeat(industry_loadings, n_time_steps_valid, 0))) Y_valid = y_valid.reshape(n_ind*npi*n_time_steps_valid) . ann_input.set_value(X_valid) ann_output.set_value(Y_valid) ppc = pm.sample_posterior_predictive(trace, model=neural_network, samples=5000) . 100%|██████████| 5000/5000 [01:48&lt;00:00, 46.05it/s] . y_hat_valid = ppc[&#39;out&#39;].mean(0).reshape(n_ind, npi*n_time_steps_valid) mse(y_hat_valid, Y_valid.reshape(n_ind, npi*n_time_steps_valid)) . 0.00036157395132278554 . The figure below shows the predictions (green) and targets (blue) per sector for the standard model. . fig = plt.figure(figsize=(13,13)) fig.suptitle(&#39;Predictions per sector.&#39;, fontsize=20) for i in range(n_ind): ax = fig.add_subplot(np.ceil(np.sqrt(n_ind)), np.ceil(np.sqrt(n_ind)), i+1, projection=&#39;3d&#39;) ax.set_xlabel(&#39;$x_1$&#39;, fontsize=8) ax.set_ylabel(&#39;$x_2$&#39;, fontsize=8) ax.set_zlabel(&#39;y&#39;, fontsize=8); ax.tick_params(labelsize=8); ax.set_xlim(-2, 2) ax.set_ylim(-2, 2) ax.set_zlim(-0.01, 0.01) ax.scatter(x1_valid[i,:,:], x2_valid[i,:,:], y_hat_valid[i,:], #c=y_pred[i,:], #cmap=&#39;Greens&#39;, c=&#39;green&#39;, linewidth=0.0); ax.scatter(x1_valid[i,:,:], x2_valid[i,:,:], y_valid[i,:,:], #c=y_valid[i,:,:], #cmap=&#39;gray&#39;, c=&#39;blue&#39;, linewidth=0.0); plt.show() . ir, rets = get_ir(log_rets=torch.tensor(Y_valid.reshape(n_stocks, n_time_steps_valid)).T, Y_hat=torch.tensor(y_hat_valid.reshape(n_stocks, n_time_steps_valid)).T, inv_cov_tensor=inv_cov_tensor) . print(&#39;Test set information ratio:&#39;, ir) plt.plot(np.cumsum(rets)) plt.title(&#39;Test set cumulative portfolio log-returns&#39;) plt.xlabel(&#39;days&#39;) plt.ylabel(&#39;cumulative portfolio log-returns&#39;) . Test set information ratio: 22.910427663560622 . Text(0, 0.5, &#39;cumulative portfolio log-returns&#39;) . Eigenvectors of the covariance matrix as encodings . Here we use the eigenvectors of the high-frequency estimates of the integrated covariance matrix averaged over all days of the training set as asset encodings. The true covariance matrix has a low-rank plus block-diagonal plus diagonal structure. . X_train = np.stack((x1_train.reshape(n_ind*npi*n_time_steps_train), x2_train.reshape(n_ind*npi*n_time_steps_train)), 1) # number of principal components, 0 if all. n_pc = 0 evectors = np.linalg.eigh(cov_avg)[1][:, -n_pc:] # evectors = np.linalg.eigh(u.uncond_cov())[1][:, -n_pc:] X_train = np.column_stack((X_train, np.repeat(evectors, n_time_steps_train, 0))) Y_train = y_train.reshape(n_ind*npi*n_time_steps_train) ann_input = theano.shared(X_train) ann_output = theano.shared(Y_train) # n_hidden = [10, 5] n_data = X_train.shape[1] # Initialize random weights between each layer init_1 = np.random.randn(n_data, n_hidden[0]).astype(theano.config.floatX) init_2 = np.random.randn(n_hidden[0], n_hidden[1]).astype(theano.config.floatX) init_out = np.random.randn(n_hidden[1]).astype(theano.config.floatX) with pm.Model() as neural_network: weights_in_1 = pm.Normal(&#39;w_in_1_grp&#39;, 0, sd=1, shape=(n_data, n_hidden[0]), testval=init_1) weights_1_2 = pm.Normal(&#39;w_1_2_grp&#39;, 0, sd=1, shape=(n_hidden[0], n_hidden[1]), testval=init_2) weights_2_out = pm.Normal(&#39;w_2_out_grp&#39;, 0, sd=1, shape=(n_hidden[1],), testval=init_out) # Build neural-network using relu activation function act_1 = tt.nnet.relu(tt.dot(ann_input, weights_in_1)) act_2 = tt.nnet.relu(tt.dot(act_1, weights_1_2)) # linear output layer intercept = pm.Normal(&#39;intercept&#39;, mu=0, sd=10) act_out = tt.dot(act_2, weights_2_out) + intercept sigma = pm.HalfNormal(&#39;sigma&#39;, sd=10) out = pm.Normal(&#39;out&#39;, act_out, sigma, observed=ann_output) . with neural_network: trace = pm.sample( init=&#39;advi+adapt_diag&#39;, tune=50, chains=1, n_init=200000, nuts_kwargs={&#39;target_accept&#39;: 0.9},) . Auto-assigning NUTS sampler... Initializing NUTS using advi+adapt_diag... Average Loss = -6,392.4: 28%|██▊ | 56287/200000 [03:28&lt;08:53, 269.35it/s] Convergence achieved at 56300 Interrupted at 56,299 [28%]: Average Loss = -1,968.1 Sequential sampling (1 chains in 1 job) NUTS: [sigma, intercept, w_2_out_grp, w_1_2_grp, w_in_1_grp] 100%|██████████| 550/550 [15:27&lt;00:00, 1.69s/it] There was 1 divergence after tuning. Increase `target_accept` or reparameterize. The acceptance probability does not match the target. It is 0.6732817678810623, but should be close to 0.9. Try to increase the number of tuning steps. The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize. Only one chain was sampled, this makes it impossible to run some convergence checks . Train set performance . ppc = pm.sample_posterior_predictive(trace, model=neural_network, samples=1000) y_hat = ppc[&#39;out&#39;].mean(0).reshape(n_ind, npi*n_time_steps_train) mse(y_hat, Y_train.reshape(n_ind, npi*n_time_steps_train)) . 100%|██████████| 1000/1000 [00:22&lt;00:00, 44.43it/s] . 0.0002160451662617625 . Test set performance . X_valid = np.stack((x1_valid.reshape(n_ind*npi*n_time_steps_valid), x2_valid.reshape(n_ind*npi*n_time_steps_valid)), 1) X_valid = np.column_stack((X_valid, np.repeat(evectors, n_time_steps_valid, 0))) Y_valid = y_valid.reshape(n_ind*npi*n_time_steps_valid) . ann_input.set_value(X_valid) ann_output.set_value(Y_valid) ppc = pm.sample_posterior_predictive(trace, model=neural_network, samples=1000) . 100%|██████████| 1000/1000 [00:22&lt;00:00, 44.84it/s] . y_hat_valid = ppc[&#39;out&#39;].mean(0).reshape(n_ind, npi*n_time_steps_valid) mse(y_hat_valid, Y_valid.reshape(n_ind, npi*n_time_steps_valid)) . 0.00036303203719532354 . ir, rets = get_ir(log_rets=torch.tensor(Y_valid.reshape(n_stocks, n_time_steps_valid)).T, Y_hat=torch.tensor(y_hat_valid.reshape(n_stocks, n_time_steps_valid)).T, inv_cov_tensor=inv_cov_tensor) . print(&#39;Test set information ratio:&#39;, ir) plt.plot(np.cumsum(rets)) plt.title(&#39;Test set cumulative portfolio log-returns&#39;) plt.xlabel(&#39;days&#39;) plt.ylabel(&#39;cumulative portfolio log-returns&#39;) . Test set information ratio: 20.179354474730747 . Text(0, 0.5, &#39;cumulative portfolio log-returns&#39;) . PyTorch deep ensemble . import torch import torch.nn as nn import torch.nn.functional as F from torch import optim class Net(nn.Module): def __init__(self, n_neurons, n_features, dropout): super().__init__() self.fc1 = nn.Linear(n_features, n_neurons) self.fc2 = nn.Linear(n_neurons, n_neurons) self.fc4 = nn.Linear(n_neurons, n_neurons) self.fc5 = nn.Linear(n_neurons, n_neurons) self.fc3 = nn.Linear(n_neurons, 1) self.fc1bn = nn.BatchNorm1d(n_neurons) self.fc2bn = nn.BatchNorm1d(n_neurons) self.fc4bn = nn.BatchNorm1d(n_neurons) self.fc5bn = nn.BatchNorm1d(n_neurons) self.drop_layer = nn.Dropout(p=dropout) def forward(self, X): X = F.relu(self.fc1bn(self.fc1(X))) X = self.drop_layer(X) X = F.relu(self.fc2bn(self.fc2(X))) X = self.drop_layer(X) # X = F.relu(self.fc4bn(self.fc4(X))) # X = self.drop_layer(X) # X = F.relu(self.fc5bn(self.fc5(X))) # X = self.drop_layer(X) X = self.fc3(X) return X . from tqdm import tqdm def fit_model(model, epochs, X, Y, X_valid, Y_valid): optimizer = optim.Adam(model.parameters(), weight_decay=0., lr=1e-2) for epoch in (range(epochs)): # trainig mode model = model.train() model.zero_grad() Y_hat = model(X) loss = criterion(Y_hat, Y) loss.backward() optimizer.step() if (epoch+1) % 1000 ==0: with torch.no_grad(): model.eval() Y_hat = model(X_valid) print(f&#39;Epoch: {epoch+1} t Train loss: {loss} t Valid loss: {criterion(Y_hat, Y_valid)}&#39;) def predict(model, X): with torch.no_grad(): model.eval() Y_hat = model(X) return Y_hat . N = 30 n_neurons = 10 dropout = 0. epochs = 3000 criterion = nn.MSELoss() # Train set X_train = np.stack((x1_train.reshape(n_ind*npi*n_time_steps_train), x2_train.reshape(n_ind*npi*n_time_steps_train)), 1) X_train = np.column_stack((X_train, np.repeat(industry_loadings, n_time_steps_train, 0))) Y_train = y_train.reshape(n_ind*npi*n_time_steps_train, 1) X_train = torch.tensor(X_train).float() Y_train = torch.tensor(Y_train).float() # Validation set X_valid = np.stack((x1_valid.reshape(n_ind*npi*n_time_steps_valid), x2_valid.reshape(n_ind*npi*n_time_steps_valid)), 1) X_valid = np.column_stack((X_valid, np.repeat(industry_loadings, n_time_steps_valid, 0))) Y_valid = y_valid.reshape(n_ind*npi*n_time_steps_valid, 1) X_valid = torch.tensor(X_valid).float() Y_valid = torch.tensor(Y_valid).float() n_features = X_train.shape[1] models = [Net(n_neurons, n_features, dropout) for i in range(N)] for i, model in enumerate(models): print(f&#39;Fitting model {i} ...&#39;) fit_model(model, epochs, X_train, Y_train, X_valid, Y_valid,) . Fitting model 0 ... Epoch: 1000 Train loss: 0.0002508650650270283 Valid loss: 0.0004071235307492316 Epoch: 2000 Train loss: 0.00023996594245545566 Valid loss: 0.00040422912570647895 Epoch: 3000 Train loss: 0.00023474314366467297 Valid loss: 0.0004130271845497191 Fitting model 1 ... Epoch: 1000 Train loss: 0.0002466226869728416 Valid loss: 0.00040071021066978574 Epoch: 2000 Train loss: 0.0002341988729313016 Valid loss: 0.00039889742038212717 Epoch: 3000 Train loss: 0.00023318800958804786 Valid loss: 0.0003919414011761546 Fitting model 2 ... Epoch: 1000 Train loss: 0.0002498300455044955 Valid loss: 0.0003950036771129817 Epoch: 2000 Train loss: 0.00023865590628702193 Valid loss: 0.00039687982643954456 Epoch: 3000 Train loss: 0.0002277261664858088 Valid loss: 0.0004184243152849376 Fitting model 3 ... Epoch: 1000 Train loss: 0.00024302199017256498 Valid loss: 0.0004039230407215655 Epoch: 2000 Train loss: 0.00023296280414797366 Valid loss: 0.00041294313268736005 Epoch: 3000 Train loss: 0.00023424768005497754 Valid loss: 0.0004649430338758975 Fitting model 4 ... Epoch: 1000 Train loss: 0.00025030976394191384 Valid loss: 0.0003966281365137547 Epoch: 2000 Train loss: 0.00023791301646269858 Valid loss: 0.0004042745567858219 Epoch: 3000 Train loss: 0.00023029415751807392 Valid loss: 0.00041665148455649614 Fitting model 5 ... Epoch: 1000 Train loss: 0.0002436818613205105 Valid loss: 0.0004103767278138548 Epoch: 2000 Train loss: 0.0002334853634238243 Valid loss: 0.0004120630619581789 Epoch: 3000 Train loss: 0.00023018076899461448 Valid loss: 0.0004126749117858708 Fitting model 6 ... Epoch: 1000 Train loss: 0.0002556033432483673 Valid loss: 0.00040123239159584045 Epoch: 2000 Train loss: 0.0002425243437755853 Valid loss: 0.0003935922868549824 Epoch: 3000 Train loss: 0.00023646869522053748 Valid loss: 0.00039417625521309674 Fitting model 7 ... Epoch: 1000 Train loss: 0.0002523782313801348 Valid loss: 0.0004040569765493274 Epoch: 2000 Train loss: 0.0002402125537628308 Valid loss: 0.0004080838116351515 Epoch: 3000 Train loss: 0.0002350450085941702 Valid loss: 0.00040917948354035616 Fitting model 8 ... Epoch: 1000 Train loss: 0.0002448004961479455 Valid loss: 0.00041951227467507124 Epoch: 2000 Train loss: 0.000229840210522525 Valid loss: 0.00041069864528253675 Epoch: 3000 Train loss: 0.00022234210337046534 Valid loss: 0.0004027922113891691 Fitting model 9 ... Epoch: 1000 Train loss: 0.0002454790810588747 Valid loss: 0.0004230051126796752 Epoch: 2000 Train loss: 0.00023656564007978886 Valid loss: 0.00041839986806735396 Epoch: 3000 Train loss: 0.00023000931832939386 Valid loss: 0.0004211821942590177 Fitting model 10 ... Epoch: 1000 Train loss: 0.0002483158023096621 Valid loss: 0.0003964881761930883 Epoch: 2000 Train loss: 0.000234818464377895 Valid loss: 0.0004061934887431562 Epoch: 3000 Train loss: 0.000222624687012285 Valid loss: 0.0004086146946065128 Fitting model 11 ... Epoch: 1000 Train loss: 0.000249355478445068 Valid loss: 0.0004024350200779736 Epoch: 2000 Train loss: 0.0002366793341934681 Valid loss: 0.0004024521913379431 Epoch: 3000 Train loss: 0.00022948744299355894 Valid loss: 0.0004035316815134138 Fitting model 12 ... Epoch: 1000 Train loss: 0.00024561697500757873 Valid loss: 0.00040083681233227253 Epoch: 2000 Train loss: 0.00022838980657979846 Valid loss: 0.0004054875753354281 Epoch: 3000 Train loss: 0.00022083648946136236 Valid loss: 0.00040789719787426293 Fitting model 13 ... Epoch: 1000 Train loss: 0.0002446445287205279 Valid loss: 0.0004010601551271975 Epoch: 2000 Train loss: 0.00022973056184127927 Valid loss: 0.00040554016595706344 Epoch: 3000 Train loss: 0.00022411468671634793 Valid loss: 0.0003935765125788748 Fitting model 14 ... Epoch: 1000 Train loss: 0.00025146466214209795 Valid loss: 0.00040369381895288825 Epoch: 2000 Train loss: 0.00024030216445680708 Valid loss: 0.0004035647725686431 Epoch: 3000 Train loss: 0.00023493324988521636 Valid loss: 0.0004059392085764557 Fitting model 15 ... Epoch: 1000 Train loss: 0.00024309262516908348 Valid loss: 0.00040126906242221594 Epoch: 2000 Train loss: 0.00023285775387194008 Valid loss: 0.00039379208465106785 Epoch: 3000 Train loss: 0.0002281484194099903 Valid loss: 0.0003939683083444834 Fitting model 16 ... Epoch: 1000 Train loss: 0.00024127463984768838 Valid loss: 0.00041748175863176584 Epoch: 2000 Train loss: 0.00023192162916529924 Valid loss: 0.0004089689755346626 Epoch: 3000 Train loss: 0.00022744224406778812 Valid loss: 0.0004062617663294077 Fitting model 17 ... Epoch: 1000 Train loss: 0.0002443670528009534 Valid loss: 0.000404148711822927 Epoch: 2000 Train loss: 0.0002300344203831628 Valid loss: 0.00040540017653256655 Epoch: 3000 Train loss: 0.00022617366630584002 Valid loss: 0.0004077464691363275 Fitting model 18 ... Epoch: 1000 Train loss: 0.00023845398391131312 Valid loss: 0.00040168772102333605 Epoch: 2000 Train loss: 0.0002224644849775359 Valid loss: 0.0004150435561314225 Epoch: 3000 Train loss: 0.00021296142949722707 Valid loss: 0.0004222855204716325 Fitting model 19 ... Epoch: 1000 Train loss: 0.0002498266112525016 Valid loss: 0.0003959094174206257 Epoch: 2000 Train loss: 0.00023983922437764704 Valid loss: 0.00039493574877269566 Epoch: 3000 Train loss: 0.00022892268316354603 Valid loss: 0.00039925871533341706 Fitting model 20 ... Epoch: 1000 Train loss: 0.00022800166334491223 Valid loss: 0.00041681560105644166 Epoch: 2000 Train loss: 0.00022075066226534545 Valid loss: 0.00041216512909159064 Epoch: 3000 Train loss: 0.00021413722424767911 Valid loss: 0.00041918197530321777 Fitting model 21 ... Epoch: 1000 Train loss: 0.0002461010590195656 Valid loss: 0.0004059797211084515 Epoch: 2000 Train loss: 0.00023865287948865443 Valid loss: 0.00040679622907191515 Epoch: 3000 Train loss: 0.00023225756012834609 Valid loss: 0.00041148404125124216 Fitting model 22 ... Epoch: 1000 Train loss: 0.00023736321600154042 Valid loss: 0.0003993347054347396 Epoch: 2000 Train loss: 0.0002264958166051656 Valid loss: 0.0004273262165952474 Epoch: 3000 Train loss: 0.00022050924599170685 Valid loss: 0.0004051178402733058 Fitting model 23 ... Epoch: 1000 Train loss: 0.0002564026799518615 Valid loss: 0.00041236868128180504 Epoch: 2000 Train loss: 0.00024284643586724997 Valid loss: 0.00040239820373244584 Epoch: 3000 Train loss: 0.00023604398302268237 Valid loss: 0.0004029260599054396 Fitting model 24 ... Epoch: 1000 Train loss: 0.0002551157376728952 Valid loss: 0.0004068294074386358 Epoch: 2000 Train loss: 0.00024311809102073312 Valid loss: 0.00040164098027162254 Epoch: 3000 Train loss: 0.00023520956165157259 Valid loss: 0.00040673359762877226 Fitting model 25 ... Epoch: 1000 Train loss: 0.0002518740948289633 Valid loss: 0.00042671780101954937 Epoch: 2000 Train loss: 0.000242152382270433 Valid loss: 0.0004163305275142193 Epoch: 3000 Train loss: 0.0002383065439062193 Valid loss: 0.00041217298712581396 Fitting model 26 ... Epoch: 1000 Train loss: 0.0002392594760749489 Valid loss: 0.00042069697519764304 Epoch: 2000 Train loss: 0.0002272079000249505 Valid loss: 0.0004235675442032516 Epoch: 3000 Train loss: 0.00022260515834204853 Valid loss: 0.00044391441042535007 Fitting model 27 ... Epoch: 1000 Train loss: 0.00023777727619744837 Valid loss: 0.0004073498130310327 Epoch: 2000 Train loss: 0.00022612173052038997 Valid loss: 0.00040711450856179 Epoch: 3000 Train loss: 0.00021765446581412107 Valid loss: 0.00041556329233571887 Fitting model 28 ... Epoch: 1000 Train loss: 0.00025968998670578003 Valid loss: 0.00038338295416906476 Epoch: 2000 Train loss: 0.00024011768982745707 Valid loss: 0.0003876080154441297 Epoch: 3000 Train loss: 0.00022824975894764066 Valid loss: 0.0003926944627892226 Fitting model 29 ... Epoch: 1000 Train loss: 0.0002526867319829762 Valid loss: 0.0004468766273930669 Epoch: 2000 Train loss: 0.00022869124950375408 Valid loss: 0.0004210940969642252 Epoch: 3000 Train loss: 0.0002248857490485534 Valid loss: 0.0004204651922918856 . Test set performance . # X_valid = np.stack((x1_valid.reshape(n_ind*npi*n_time_steps_valid), # x2_valid.reshape(n_ind*npi*n_time_steps_valid)), 1) # X_valid = np.column_stack((X_valid, np.repeat(industry_loadings, n_time_steps_valid, 0))) # Y_valid = y_valid.reshape(n_ind*npi*n_time_steps_valid, 1) # X_valid = torch.tensor(X_valid).float() # Y_valid = torch.tensor(Y_valid).float() y_hat_valid_list = [] for model in models: y_hat_valid_list.append(predict(model, X_valid).numpy()) y_hat_valid_mean = np.array(y_hat_valid_list).mean(0) y_hat_valid = y_hat_valid_mean.reshape(n_ind, npi*n_time_steps_valid) mse(y_hat_valid, Y_valid.numpy().reshape(n_ind, npi*n_time_steps_valid)) . 0.0003699056 . ir, rets = get_ir(log_rets=torch.tensor(Y_valid.reshape(n_stocks, n_time_steps_valid)).T, Y_hat=torch.tensor(y_hat_valid.reshape(n_stocks, n_time_steps_valid)).T, inv_cov_tensor=inv_cov_tensor.float()) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor). &#34;&#34;&#34;Entry point for launching an IPython kernel. . print(&#39;Test set information ratio:&#39;, ir) plt.plot(np.cumsum(rets)) plt.title(&#39;Test set cumulative portfolio log-returns&#39;) plt.xlabel(&#39;days&#39;) plt.ylabel(&#39;cumulative portfolio log-returns&#39;) . Test set information ratio: 20.05791717754258 . Text(0, 0.5, &#39;cumulative portfolio log-returns&#39;) . Conclusion . Hierarchical neural networks can successfully model panel data of asset returns with sector dependent data generating functions by sharing information via informative priors. Still, standard neural networks are surprisingly good at capturing the sector differences as well if the sector is given as a one-hot encoded matrix. . CPython 3.6.9 IPython 5.5.0 numpy 1.18.5 numba 0.48.0 hfhd 0.1.4 scipy 1.4.1 theano 1.0.5 pymc3 3.7 matplotlib 3.2.2 compiler : GCC 8.4.0 system : Linux release : 4.19.112+ machine : x86_64 processor : x86_64 CPU cores : 2 interpreter: 64bit .",
            "url": "https://jpwoeltjen.github.io/researchBlog/efficiency/hierarchical/deep%20learning/probabilistic%20programming/bayesian/panel/2020/08/28/hierarchical_bayesian_neural_network.html",
            "relUrl": "/efficiency/hierarchical/deep%20learning/probabilistic%20programming/bayesian/panel/2020/08/28/hierarchical_bayesian_neural_network.html",
            "date": " • Aug 28, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Efficient Estimation of Predictive Models using High-frequency High-dimensional Data",
            "content": "My Master’s thesis, submitted to the Institute for Statistics and Econometrics of the Christian-Albrechts-Universität zu Kiel, Germany, proposes a method to increase the data efficiency of neural networks for asset return prediction. . Abstract . In this thesis, the data efficiency of linear and nonlinear regression models of asset return panel data is enhanced by accounting for cross-sectional correlations and longitudinal volatility clusters of residuals. The procedure is motivated by the infeasible generalized least squares estimator. In an extension, a generalized least squares loss function is proposed to efficiently fit nonlinear relationships via deep neural networks. Feasibility is achieved by estimating the unobserved covariance matrix of residuals with a nonparametrically eigenvalue-regularized ensembled pairwise integrated covariance (NER EPIC) matrix estimator applied to high-frequency returns in high dimensions. Monte Carlo evidence confirms efficiency gains for linear and nonlinear conditional expectation models in finite- samples. A study of historical stock market data for the 100 largest US-based stocks shows substantially improved portfolio return characteristics of general- ized models compared to their standard counterparts. A trading strategy based on the predictions of a neural network, minimizing the proposed generalized ob- jective function, generates an out-of-sample information ratio of 2.59. Compared to a model with the same hyperparameters but minimizing the conventional MSE loss function, this represents an improvement of close to 150%. . Get the thesis here. .",
            "url": "https://jpwoeltjen.github.io/researchBlog/efficiency/deep%20learning/time%20series/panel/2020/08/10/Efficient_estimation_of_predictive_models-using_high-frequency_high-dimensional_data.html",
            "relUrl": "/efficiency/deep%20learning/time%20series/panel/2020/08/10/Efficient_estimation_of_predictive_models-using_high-frequency_high-dimensional_data.html",
            "date": " • Aug 10, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Challenges in Financial Machine Learning",
            "content": "Why is machine learning applied to financial data hard? What are the main differences of financial data compared to datasets where machine learning techniques work well. . Panel model . $p$ assets. | $n$ time steps. | . Nonlinearity of conditional expectation Deep neural network | . | Nonstationarity of conditional expectation Financial markets are dynamic; old patterns are arbitraged away and new patterns emerge. | The optimal weights of neural networks change smoothly over time (there might also be sudden shocks). | Layers close to the input are more stable while layers closer to the output must adapt quickly to new market dynamics. | https://jpwoeltjen.github.io/researchBlog/efficiency/time%20series/deep%20learning/2020/08/28/random_walk_deep_net_reg.html | . | Hierarchical structure Each asset/sector can have different weights (esp. in the last layer). | But there is dependence. | Even though the absolute amount of data is big when pooled. Data is limited relative to model complexity. | Use covariance matrix as measure of similarity. | Dirichlet process Allows for infinite clusters but promotes sparsity. | . | Alternatively, use asset type encoding (loadings on sector, industry, risk factors) as feature in a standard net. | . | High noise component with cross-sectional correlation and longitudinal volatility clusters. Assets are correlated. | Volatility is stochastic and autoregressive. | Improve data efficiency of conditional expectation models by accounting for it. https://jpwoeltjen.github.io/researchBlog/efficiency/deep%20learning/time%20series/panel/2020/08/10/Efficient_estimation_of_predictive_models-using_high-frequency_high-dimensional_data.html | . | In a Bayesian setting, compute the daily integrated covariance matrix with high-frequency data and use the Kalman smoother to get a better estimate (this is more principled and Bayesian than the adhoc moving average I used in the thesis). Then use this estimator as the covariance matrix model of the multivariate (Gaussian) likelihood function of the p dimensional asset returns. (Remember: The covariance matrix estimate may use future data in the training phase.) | For forecasting: Use stochastic volatility models or multivariate GARCH models. Condition on events (earnings announcements, news, etc.) | . | . | The dimensionality of the covariance matrix is high. The number of assets is large relative to the sample size. | Hence sample eigenvalues are over-dispersed and the sample covariance matrix is ill-conditioned. | . | Prices are observed irregularly and with noise at high frequency. Intraday prices contain microstructure noise and are observed non-synchronously across assets. | https://hfhd.readthedocs.io/ | . | Estimates of uncertainty is important. Bayesian framework allows for uncertainty estimates. | A decision is always better if one accounts for uncertainty. | Main portfolio optimization result is Markowitz: optimal weights depend on mean and variance of returns. | assumes first two moments are known. | when estimates are used instead, need to account for epistemic uncertainty of estimates as well Aleatoric uncertainty: noise inherent in the observations. (i.e. variance of returns) | Epistemic uncertainty accounts for uncertainty in the model – uncertainty which can be explained away given enough data. (i.e. variance of mean and variance estimates) | https://jpwoeltjen.github.io/researchBlog/probabilistic%20programming/bayesian/portfolio%20optimization/2020/10/01/bayesian_porfolio_optimization.html | . | . | . | Time series features. Recurrent neural networks (LSTM, GRU) | Transformers adapted to time series. | . | 2 and 3: Each point in asset-time has different weights, but there is dependence between them. Weights change smoothly over time and weights of similar stocks are themself similar. A stock is similar to another, e.g., if it operates in the same industry. A company can have several arms, though. A stock can be correlated to many other stocks, the covariance matrix is the right object to encode this similarity. .",
            "url": "https://jpwoeltjen.github.io/researchBlog/efficiency/deep%20learning/time%20series/panel/nonstationarity/heteroskedastisity/hierarchical/bayesian/2020/01/01/Financial_machine_learning.html",
            "relUrl": "/efficiency/deep%20learning/time%20series/panel/nonstationarity/heteroskedastisity/hierarchical/bayesian/2020/01/01/Financial_machine_learning.html",
            "date": " • Jan 1, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Natural Language Processing of Financial Reports for Stock Clustering",
            "content": "import pandas as pd import numpy as np from matplotlib import pyplot as plt from statsmodels.tsa.stattools import adfuller import seaborn as sns import statsmodels.api as sm from tqdm import tqdm, tnrange, tqdm_notebook from sklearn.metrics.pairwise import cosine_similarity from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer from sklearn import decomposition from IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = &quot;all&quot; %load_ext autoreload %autoreload 2 %config InlineBackend.figure_format = &#39;retina&#39; . Clustering stocks according to Item1 (Business) . Loading documents . #collapse import os import re dr = &#39;/Users/jan/Desktop/SEC-EDGAR-text-copy/data/batch_0002/&#39; data = [] tickers = [] for batch in os.listdir(dr): if batch == &#39;.DS_Store&#39;: continue for file in os.listdir(os.path.join(dr, batch)): if &#39;Item1_excerpt.txt&#39; in file : print(file) ticker = file.split(&#39;_&#39;)[0] if ticker in tickers: print(&#39;Skipping&#39;, ticker, &#39;Reason: ticker duplicated&#39;) continue with open(os.path.join(dr, batch, file)) as f: d = f.read() d = d.replace(&#39; n&#39;,&#39; &#39;) d = re.sub(&quot; d&quot;, &quot;&quot;, d) if len(d) &lt; 2000: print(&#39;Skipping&#39;, ticker, &#39;Reason: len too small&#39;) continue data.append(&#39; n&#39; + d + &#39; n&#39;) tickers.append(ticker) . . SCG_0000091882_10K_20190228_Item1_excerpt.txt WLTW_0001140536_10K_20190227_Item1_excerpt.txt HRL_0000048465_10K_20181207_Item1_excerpt.txt FDX_0001048911_10K_20190716_Item1_excerpt.txt WYNN_0001174922_10K_20190228_Item1_excerpt.txt NDAQ_0001120193_10K_20190222_Item1_excerpt.txt AEE_0000018654_10K_20190226_Item1_excerpt.txt MMC_0000062709_10K_20190221_Item1_excerpt.txt XLNX_0000743988_10K_20190510_Item1_excerpt.txt PNR_0000077360_10K_20190219_Item1_excerpt.txt GD_0000040533_10K_20190213_Item1_excerpt.txt HD_0000354950_10K_20190328_Item1_excerpt.txt LOW_0000060667_10K_20190402_Item1_excerpt.txt OMC_0000029989_10K_20190212_Item1_excerpt.txt BRKB_0001067983_10K_20190225_Item1_excerpt.txt BAX_0000010456_10K_20190221_Item1_excerpt.txt AAL_0000004515_10K_20190225_Item1_excerpt.txt TXT_0000217346_10K_20190214_Item1_excerpt.txt KR_0000056873_10K_20190402_Item1_excerpt.txt CCI_0001051470_10K_20190225_Item1_excerpt.txt MTB_0000036270_10K_20190220_Item1_excerpt.txt ALL_0000899051_10K_20190215_Item1_excerpt.txt TXN_0000097476_10K_20190222_Item1_excerpt.txt PSA_0001393311_10K_20190227_Item1_excerpt.txt UAL_0000100517_10K_20190228_Item1_excerpt.txt CAT_0000018230_10K_20190214_Item1_excerpt.txt AME_0001037868_10K_20190221_Item1_excerpt.txt SHW_0000089800_10K_20190222_Item1_excerpt.txt HOLX_0000859737_10K_20181120_Item1_excerpt.txt WBA_0001618921_10K_20181011_Item1_excerpt.txt LMT_0000936468_10K_20190208_Item1_excerpt.txt HBI_0001359841_10K_20190211_Item1_excerpt.txt JEC_0000052988_10K_20181121_Item1_excerpt.txt RF_0001281761_10K_20190222_Item1_excerpt.txt HON_0000773840_10K_20190208_Item1_excerpt.txt FL_0000850209_10K_20190402_Item1_excerpt.txt NWL_0000814453_10K_20190304_Item1_excerpt.txt AXP_0000004962_10K_20190213_Item1_excerpt.txt CSX_0000277948_10K_20190206_Item1_excerpt.txt JWN_0000072333_10K_20190318_Item1_excerpt.txt PCG_0000075488_10K_20190228_Item1_excerpt.txt FLIR_0000354908_10K_20190228_Item1_excerpt.txt BK_0001390777_10K_20190227_Item1_excerpt.txt CINF_0000020286_10K_20190222_Item1_excerpt.txt ITW_0000049826_10K_20190215_Item1_excerpt.txt ARNC_0000004281_10K_20190221_Item1_excerpt.txt WYN_0001361658_10K_20190226_Item1_excerpt.txt JNPR_0001043604_10K_20190222_Item1_excerpt.txt ROK_0001024478_10K_20181109_Item1_excerpt.txt BBT_0000092230_10K_20190226_Item1_excerpt.txt WM_0000823768_10K_20190214_Item1_excerpt.txt AVY_0000008818_10K_20190227_Item1_excerpt.txt HAS_0000046080_10K_20190226_Item1_excerpt.txt IPG_0000051644_10K_20190225_Item1_excerpt.txt KMB_0000055785_10K_20190207_Item1_excerpt.txt PKI_0000031791_10K_20190226_Item1_excerpt.txt TDG_0001260221_10K_20181109_Item1_excerpt.txt GS_0000886982_10K_20190226_Item1_excerpt.txt ZBH_0001136869_10K_20190226_Item1_excerpt.txt SPG_0001022344_10K_20190222_Item1_excerpt.txt PEP_0000077476_10K_20190215_Item1_excerpt.txt FITB_0000035527_10K_20190301_Item1_excerpt.txt CSCO_0000858877_10K_20190905_Item1_excerpt.txt MYL_0001623613_10K_20190227_Item1_excerpt.txt EQR_0000906107_10K_20190221_Item1_excerpt.txt TSN_0000100493_10K_20181113_Item1_excerpt.txt DHR_0000313616_10K_20190221_Item1_excerpt.txt LH_0000920148_10K_20190228_Item1_excerpt.txt GT_0000042582_10K_20190208_Item1_excerpt.txt VNO_0000899689_10K_20190211_Item1_excerpt.txt IR_0001466258_10K_20190212_Item1_excerpt.txt ORCL_0001341439_10K_20190621_Item1_excerpt.txt MAT_0000063276_10K_20190222_Item1_excerpt.txt LLL_0001039101_10K_20190221_Item1_excerpt.txt UNP_0000100885_10K_20190208_Item1_excerpt.txt KIM_0000879101_10K_20190215_Item1_excerpt.txt CL_0000021665_10K_20190221_Item1_excerpt.txt AMAT_0000006951_10K_20181213_Item1_excerpt.txt SEE_0001012100_10K_20190219_Item1_excerpt.txt NLSN_0001492633_10K_20190228_Item1_excerpt.txt MKC_0000063754_10K_20190125_Item1_excerpt.txt AWK_0001410636_10K_20190219_Item1_excerpt.txt CHD_0000313927_10K_20190221_Item1_excerpt.txt LVLT_0000794323_10K_20190319_Item1_excerpt.txt AON_0000315293_10K_20190219_Item1_excerpt.txt TMK_0000320335_10K_20190301_Item1_excerpt.txt PVH_0000078239_10K_20190329_Item1_excerpt.txt PPG_0000079879_10K_20190221_Item1_excerpt.txt JNJ_0000200406_10K_20190220_Item1_excerpt.txt DIS_0001001039_10K_20181121_Item1_excerpt.txt MRK_0000310158_10K_20190227_Item1_excerpt.txt FAST_0000815556_10K_20190206_Item1_excerpt.txt DUK_0000017797_10K_20190228_Item1_excerpt.txt ADS_0001101215_10K_20190226_Item1_excerpt.txt TGNA_0000039899_10K_20190301_Item1_excerpt.txt UNH_0000731766_10K_20190212_Item1_excerpt.txt KO_0000021344_10K_20190221_Item1_excerpt.txt CMG_0001058090_10K_20190208_Item1_excerpt.txt MSFT_0000789019_10K_20190801_Item1_excerpt.txt NOV_0001021860_10K_20190214_Item1_excerpt.txt VMC_0001396009_10K_20190226_Item1_excerpt.txt DHI_0000882184_10K_20181116_Item1_excerpt.txt AVB_0000915912_10K_20190222_Item1_excerpt.txt HPE_0001645590_10K_20181212_Item1_excerpt.txt NI_0001111711_10K_20190220_Item1_excerpt.txt GPN_0001123360_10K_20190221_Item1_excerpt.txt PGR_0000080661_10K_20190227_Item1_excerpt.txt ABBV_0001551152_10K_20190227_Item1_excerpt.txt ALK_0000766421_10K_20190215_Item1_excerpt.txt EXPE_0001324424_10K_20190208_Item1_excerpt.txt JBHT_0000728535_10K_20190222_Item1_excerpt.txt PCAR_0000075362_10K_20190221_Item1_excerpt.txt EQIX_0001101239_10K_20190222_Item1_excerpt.txt TRIP_0001526520_10K_20190222_Item1_excerpt.txt HP_0000046765_10K_20181116_Item1_excerpt.txt XRX_0000108772_10K_20190225_Item1_excerpt.txt AMG_0001004434_10K_20190222_Item1_excerpt.txt SBUX_0000829224_10K_20181116_Item1_excerpt.txt MNST_0000865752_10K_20190228_Item1_excerpt.txt FB_0001326801_10K_20190131_Item1_excerpt.txt NFLX_0001065280_10K_20190129_Item1_excerpt.txt NEM_0001164727_10K_20190221_Item1_excerpt.txt ADP_0000008670_10K_20190809_Item1_excerpt.txt GILD_0000882095_10K_20190226_Item1_excerpt.txt DG_0000029534_10K_20190322_Item1_excerpt.txt CELG_0000816284_10K_20190226_Item1_excerpt.txt DOW_0000029915_10K_20190211_Item1_excerpt.txt ULTA_0001403568_10K_20190402_Item1_excerpt.txt MAA_0000912595_10K_20190221_Item1_excerpt.txt ADBE_0000796343_10K_20190125_Item1_excerpt.txt QRVO_0001604778_10K_20190517_Item1_excerpt.txt TEL_0001385157_10K_20181113_Item1_excerpt.txt DD_0000030554_10K_20190211_Item1_excerpt.txt CNC_0001071739_10K_20190219_Item1_excerpt.txt CHK_0000895126_10K_20190227_Item1_excerpt.txt SO_0000003153_10K_20190220_Item1_excerpt.txt SLB_0000087347_10K_20190123_Item1_excerpt.txt NVDA_0001045810_10K_20190221_Item1_excerpt.txt CXO_0001358071_10K_20190220_Item1_excerpt.txt ALXN_0000899866_10K_20190206_Item1_excerpt.txt ILMN_0001110803_10K_20190212_Item1_excerpt.txt SCHW_0000316709_10K_20190222_Item1_excerpt.txt IVZ_0000914208_10K_20190222_Item1_excerpt.txt BLK_0001364742_10K_20190228_Item1_excerpt.txt PXD_0001038357_10K_20190226_Item1_excerpt.txt MAR_0001048286_10K_20190301_Item1_excerpt.txt AJG_0000354190_10K_20190208_Item1_excerpt.txt CVX_0000093410_10K_20190222_Item1_excerpt.txt EQT_0000033213_10K_20190214_Item1_excerpt.txt COL_0001137411_10K_20181126_Item1_excerpt.txt AKAM_0001086222_10K_20190228_Item1_excerpt.txt GOOGL_0001652044_10K_20190205_Item1_excerpt.txt CTSH_0001058290_10K_20190219_Item1_excerpt.txt PYPL_0001633917_10K_20190207_Item1_excerpt.txt MA_0001141391_10K_20190213_Item1_excerpt.txt OKE_0001039684_10K_20190226_Item1_excerpt.txt IBM_0000051143_10K_20190226_Item1_excerpt.txt V_0001403161_10K_20181116_Item1_excerpt.txt CERN_0000804753_10K_20190208_Item1_excerpt.txt AMZN_0001018724_10K_20190201_Item1_excerpt.txt NAVI_0001593538_10K_20190226_Item1_excerpt.txt MLM_0000916076_10K_20190225_Item1_excerpt.txt IRM_0001020569_10K_20190214_Item1_excerpt.txt IDXX_0000874716_10K_20190215_Item1_excerpt.txt VRTX_0000875320_10K_20190213_Item1_excerpt.txt EOG_0000821189_10K_20190226_Item1_excerpt.txt XOM_0000034088_10K_20190227_Item1_excerpt.txt BHI_0000808362_10K_20190219_Item1_excerpt.txt EW_0001099800_10K_20190215_Item1_excerpt.txt MRO_0000101778_10K_20190221_Item1_excerpt.txt ISRG_0001035267_10K_20190204_Item1_excerpt.txt CRM_0001108524_10K_20190308_Item1_excerpt.txt PCLN_0001075531_10K_20190227_Item1_excerpt.txt NUE_0000073309_10K_20190228_Item1_excerpt.txt D_0000103682_10K_20190228_Item1_excerpt.txt SWK_0000093556_10K_20190226_Item1_excerpt.txt UDR_0000074208_10K_20190219_Item1_excerpt.txt RCL_0000884887_10K_20190222_Item1_excerpt.txt DLTR_0000935703_10K_20190327_Item1_excerpt.txt ORLY_0000898173_10K_20190227_Item1_excerpt.txt LB_0000701985_10K_20190322_Item1_excerpt.txt MTD_0001037646_10K_20190208_Item1_excerpt.txt MCO_0001059556_10K_20190225_Item1_excerpt.txt IFF_0000051253_10K_20190226_Item1_excerpt.txt ROST_0000745732_10K_20190402_Item1_excerpt.txt SNA_0000091440_10K_20190214_Item1_excerpt.txt EXPD_0000746515_10K_20190222_Item1_excerpt.txt BIIB_0000875045_10K_20190206_Item1_excerpt.txt FMC_0000037785_10K_20190228_Item1_excerpt.txt BXP_0001037540_10K_20190228_Item1_excerpt.txt CF_0001324404_10K_20190222_Item1_excerpt.txt KMX_0001170010_10K_20190419_Item1_excerpt.txt EL_0001001250_10K_20190823_Item1_excerpt.txt AMT_0001053507_10K_20190227_Item1_excerpt.txt FFIV_0001048695_10K_20181121_Item1_excerpt.txt PAYX_0000723531_10K_20190724_Item1_excerpt.txt ABC_0001140859_10K_20181120_Item1_excerpt.txt KSU_0000054480_10K_20190125_Item1_excerpt.txt CME_0001156375_10K_20190228_Item1_excerpt.txt COO_0000711404_10K_20181221_Item1_excerpt.txt ACN_0001467373_10K_20181024_Item1_excerpt.txt TSCO_0000916365_10K_20190221_Item1_excerpt.txt TROW_0001113169_10K_20190213_Item1_excerpt.txt UTX_0000101829_10K_20190207_Item1_excerpt.txt EA_0000712515_10K_20190524_Item1_excerpt.txt MOS_0001285785_10K_20190313_Item1_excerpt.txt APH_0000820313_10K_20190213_Item1_excerpt.txt R_0000085961_10K_20190221_Item1_excerpt.txt ZTS_0001555280_10K_20190214_Item1_excerpt.txt WAT_0001000697_10K_20190226_Item1_excerpt.txt MCK_0000927653_10K_20190515_Item1_excerpt.txt ANTM_0001156039_10K_20190220_Item1_excerpt.txt DISCK_0001437107_10K_20190301_Item1_excerpt.txt FBHS_0001519751_10K_20190225_Item1_excerpt.txt TJX_0000109198_10K_20190403_Item1_excerpt.txt FISV_0000798354_10K_20190221_Item1_excerpt.txt ESS_0000920522_10K_20190221_Item1_excerpt.txt AGN_0001578845_10K_20190215_Item1_excerpt.txt DE_0000315189_10K_20181217_Item1_excerpt.txt CHRW_0001043277_10K_20190225_Item1_excerpt.txt TSS_0000721683_10K_20190221_Item1_excerpt.txt ZION_0000109380_10K_20190226_Item1_excerpt.txt BSX_0000885725_10K_20190219_Item1_excerpt.txt NTRS_0000073124_10K_20190226_Item1_excerpt.txt DLR_0001297996_10K_20190225_Item1_excerpt.txt PBCT_0001378946_10K_20190301_Item1_excerpt.txt WAT_0001000697_10KA_20190301_Item1_excerpt.txt Skipping WAT Reason: ticker duplicated STZ_0000016918_10K_20190423_Item1_excerpt.txt SYK_0000310764_10K_20190207_Item1_excerpt.txt CMA_0000028412_10K_20190212_Item1_excerpt.txt COST_0000909832_10K_20181026_Item1_excerpt.txt ROP_0000882835_10K_20190225_Item1_excerpt.txt HBAN_0000049196_10K_20190215_Item1_excerpt.txt ABT_0000001800_10K_20190222_Item1_excerpt.txt VRSK_0001442145_10K_20190219_Item1_excerpt.txt USB_0000036104_10K_20190222_Item1_excerpt.txt SJM_0000091419_10K_20190617_Item1_excerpt.txt MHK_0000851968_10K_20190228_Item1_excerpt.txt NEE_0000037634_10K_20190215_Item1_excerpt.txt LKQ_0001065696_10K_20190301_Item1_excerpt.txt HSIC_0001000228_10K_20190220_Item1_excerpt.txt HUM_0000049071_10K_20190221_Item1_excerpt.txt CFG_0000759944_10K_20190221_Item1_excerpt.txt LUV_0000092380_10K_20190205_Item1_excerpt.txt UHS_0000352915_10K_20190227_Item1_excerpt.txt TMO_0000097745_10K_20190227_Item1_excerpt.txt CHTR_0001091667_10K_20190131_Item1_excerpt.txt LUK_0000096223_10KT_20190129_Item1_excerpt.txt CCL_0000815097_10K_20190128_Item1_excerpt.txt SWKS_0000004127_10K_20181115_Item1_excerpt.txt BFB_0000014693_10K_20190613_Item1_excerpt.txt ALB_0000915913_10K_20190227_Item1_excerpt.txt QCOM_0000804328_10K_20181107_Item1_excerpt.txt MUR_0000717423_10K_20190227_Item1_excerpt.txt CBG_0001138118_10K_20190301_Item1_excerpt.txt FRT_0000034903_10K_20190213_Item1_excerpt.txt MAC_0000912242_10K_20190225_Item1_excerpt.txt CMCSA_0000902739_10K_20190131_Item1_excerpt.txt DLPH_0001521332_10K_20190204_Item1_excerpt.txt EFX_0000033185_10K_20190221_Item1_excerpt.txt AZO_0000866787_10K_20181024_Item1_excerpt.txt BMY_0000014272_10K_20190225_Item1_excerpt.txt PM_0001413329_10K_20190207_Item1_excerpt.txt SPGI_0000064040_10K_20190213_Item1_excerpt.txt ATVI_0000718877_10K_20190228_Item1_excerpt.txt VAR_0000203527_10K_20181126_Item1_excerpt.txt NOC_0001133421_10K_20190131_Item1_excerpt.txt RHI_0000315213_10K_20190215_Item1_excerpt.txt CB_0000896159_10K_20190228_Item1_excerpt.txt DFS_0001393612_10K_20190220_Item1_excerpt.txt O_0000726728_10K_20190222_Item1_excerpt.txt HCA_0000860730_10K_20190221_Item1_excerpt.txt STT_0000093751_10K_20190221_Item1_excerpt.txt TIF_0000098246_10K_20190322_Item1_excerpt.txt BDX_0000010795_10K_20181121_Item1_excerpt.txt UPS_0001090727_10K_20190221_Item1_excerpt.txt NSC_0000702165_10K_20190208_Item1_excerpt.txt ECL_0000031462_10K_20190301_Item1_excerpt.txt VFC_0000103379_10K_20190524_Item1_excerpt.txt WFC_0000072971_10K_20190227_Item1_excerpt.txt O_0000726728_10KA_20190301_Item1_excerpt.txt Skipping O Reason: ticker duplicated MAS_0000062996_10K_20190207_Item1_excerpt.txt URBN_0000912615_10K_20190401_Item1_excerpt.txt ICE_0001571949_10K_20190207_Item1_excerpt.txt A_0001090872_10K_20181220_Item1_excerpt.txt BWA_0000908255_10KA_20180928_Item1_excerpt.txt SLG_0001040971_10K_20190227_Item1_excerpt.txt BWA_0000908255_10K_20190219_Item1_excerpt.txt Skipping BWA Reason: ticker duplicated FIS_0001136893_10K_20190221_Item1_excerpt.txt PNC_0000713676_10K_20190301_Item1_excerpt.txt PFG_0001126328_10K_20190213_Item1_excerpt.txt LNT_0000052485_10K_20190222_Item1_excerpt.txt LEG_0000058492_10K_20190227_Item1_excerpt.txt VRSN_0001014473_10K_20190215_Item1_excerpt.txt GWW_0000277135_10K_20190228_Item1_excerpt.txt ALLE_0001579241_10K_20190219_Item1_excerpt.txt HOG_0000793952_10K_20190228_Item1_excerpt.txt PFE_0000078003_10K_20190228_Item1_excerpt.txt DGX_0001022079_10K_20190221_Item1_excerpt.txt TAP_0000024545_10K_20190212_Item1_excerpt.txt WU_0001365135_10K_20190221_Item1_excerpt.txt GIS_0000040704_10K_20190628_Item1_excerpt.txt EMR_0000032604_10K_20181119_Item1_excerpt.txt KHC_0001637459_10K_20190607_Item1_excerpt.txt ADM_0000007084_10K_20190219_Item1_excerpt.txt EXC_0000008192_10K_20190208_Item1_excerpt.txt MU_0000723125_10K_20181015_Item1_excerpt.txt TDC_0000816761_10K_20190226_Item1_excerpt.txt WMT_0000104169_10K_20190328_Item1_excerpt.txt WMB_0000107263_10K_20190221_Item1_excerpt.txt CAG_0000023217_10K_20190719_Item1_excerpt.txt SIG_0000832988_10K_20190403_Item1_excerpt.txt MMM_0000066740_10K_20190207_Item1_excerpt.txt CNDT_0001677703_10K_20190228_Item1_excerpt.txt ES_0000013372_10K_20190226_Item1_excerpt.txt IP_0000051434_10K_20190220_Item1_excerpt.txt AMGN_0000318154_10K_20190213_Item1_excerpt.txt HPQ_0000047217_10K_20181213_Item1_excerpt.txt ED_0000023632_10K_20190221_Item1_excerpt.txt CTL_0000018926_10K_20190311_Item1_excerpt.txt GLW_0000024741_10K_20190212_Item1_excerpt.txt AAPL_0000320193_10K_20181105_Item1_excerpt.txt NRG_0001013871_10K_20190228_Item1_excerpt.txt AIV_0000922864_10K_20190220_Item1_excerpt.txt HCP_0000765880_10K_20190214_Item1_excerpt.txt NWSA_0001564708_10K_20190813_Item1_excerpt.txt K_0000055067_10K_20190225_Item1_excerpt.txt CNP_0000048732_10K_20190228_Item1_excerpt.txt T_0000732717_10K_20190220_Item1_excerpt.txt LNC_0000059558_10K_20190220_Item1_excerpt.txt KLAC_0000319201_10K_20190816_Item1_excerpt.txt BEN_0000038777_10K_20181109_Item1_excerpt.txt FTR_0000020520_10K_20190301_Item1_excerpt.txt XEL_0000072903_10K_20190222_Item1_excerpt.txt AAP_0001158449_10K_20190219_Item1_excerpt.txt SRCL_0000861878_10K_20190228_Item1_excerpt.txt SYMC_0000849399_10K_20181026_Item1_excerpt.txt WEC_0000783325_10K_20190226_Item1_excerpt.txt SWN_0000007332_10K_20190228_Item1_excerpt.txt AEP_0000004904_10K_20190221_Item1_excerpt.txt NTAP_0001002047_10K_20190618_Item1_excerpt.txt BBBY_0000886158_10K_20190430_Item1_excerpt.txt TDC_0000816761_10KA_20190301_Item1_excerpt.txt Skipping TDC Reason: ticker duplicated M_0000794367_10K_20190403_Item1_excerpt.txt GRMN_0001121788_10K_20190220_Item1_excerpt.txt TRV_0000086312_10K_20190214_Item1_excerpt.txt RL_0001037038_10K_20190516_Item1_excerpt.txt ENDP_0001593034_10K_20190228_Item1_excerpt.txt PBI_0000078814_10K_20190220_Item1_excerpt.txt L_0000060086_10K_20190213_Item1_excerpt.txt UNM_0000005513_10K_20190219_Item1_excerpt.txt GPS_0000039911_10K_20190319_Item1_excerpt.txt STX_0001137789_10K_20190802_Item1_excerpt.txt CPB_0000016732_10K_20180927_Item1_excerpt.txt EXR_0001289490_10K_20190226_Item1_excerpt.txt PLD_0001045609_10K_20190213_Item1_excerpt.txt LLY_0000059478_10K_20190219_Item1_excerpt.txt MET_0001099219_10K_20190222_Item1_excerpt.txt MPC_0001510295_10K_20190228_Item1_excerpt.txt CTAS_0000723254_10K_20190726_Item1_excerpt.txt AYI_0001144215_10K_20181025_Item1_excerpt.txt FLR_0001124198_10K_20190221_Item1_excerpt.txt ADI_0000006281_10K_20181127_Item1_excerpt.txt VIAB_0001339947_10K_20181116_Item1_excerpt.txt PPL_0000055387_10K_20190214_Item1_excerpt.txt BLL_0000009389_10K_20190222_Item1_excerpt.txt HST_0001061937_10K_20190226_Item1_excerpt.txt VZ_0000732712_10K_20190215_Item1_excerpt.txt TGT_0000027419_10K_20190313_Item1_excerpt.txt JCI_0000833444_10K_20181120_Item1_excerpt.txt CTXS_0000877890_10K_20190215_Item1_excerpt.txt KORS_0001530721_10K_20190529_Item1_excerpt.txt KSS_0000885639_10K_20190322_Item1_excerpt.txt GPC_0000040987_10K_20190225_Item1_excerpt.txt FSLR_0001274494_10K_20190222_Item1_excerpt.txt HRB_0000012659_10K_20190614_Item1_excerpt.txt MSI_0000068505_10K_20190215_Item1_excerpt.txt RIG_0001451505_10K_20190219_Item1_excerpt.txt GM_0001467858_10K_20190206_Item1_excerpt.txt NWS_0001564708_10K_20190813_Item1_excerpt.txt ETN_0001551182_10K_20190227_Item1_excerpt.txt PRU_0001137774_10K_20190215_Item1_excerpt.txt FLS_0000030625_10K_20190220_Item1_excerpt.txt CMS_0000201533_10K_20190205_Item1_excerpt.txt EMN_0000915389_10K_20190227_Item1_excerpt.txt AIG_0000005272_10K_20190215_Item1_excerpt.txt DPS_0001418135_10K_20190228_Item1_excerpt.txt FE_0001031296_10K_20190219_Item1_excerpt.txt SYMC_0000849399_10K_20190524_Item1_excerpt.txt Skipping SYMC Reason: ticker duplicated F_0000037996_10K_20190221_Item1_excerpt.txt BBY_0000764478_10K_20190328_Item1_excerpt.txt PEG_0000081033_10K_20190228_Item1_excerpt.txt . Vectorize . vectorizer = CountVectorizer(stop_words=&#39;english&#39;) X = vectorizer.fit_transform(data).todense() vocab = np.array(vectorizer.get_feature_names()) vectorizer_tfidf = TfidfVectorizer(stop_words=&#39;english&#39;, max_df = 1.,) X_tfidf = vectorizer_tfidf.fit_transform(data) . Dendrogram . from scipy.cluster.hierarchy import ward, dendrogram raw_cosine = pd.DataFrame(cosine_similarity(X_tfidf,X_tfidf)) linkage_matrix = ward(raw_cosine) fig, ax = plt.subplots(figsize=(20, 50)) # set size ax = dendrogram(linkage_matrix, orientation=&quot;right&quot;, labels=tickers); plt.tick_params( axis= &#39;x&#39;, which=&#39;both&#39;, bottom=&#39;off&#39;, top=&#39;off&#39;, labelbottom=&#39;off&#39; ) plt.savefig(&#39;ward_clusters.png&#39;, dpi=200) #save figure as ward_clusters . Singular Value Decomposition . u, s, v = decomposition.randomized_svd(X_tfidf, 20) . def show_topics(a, num_top_words): top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]] topic_words = ([top_words(t) for t in a]) return [&#39; &#39;.join(t) for t in topic_words] . show_topics(v, 10) . [&#39;products company services business customers including financial solutions information product&#39;, &#39;bank banking capital insurance reserve frb federal institution fdic financial&#39;, &#39;gas natural energy company oil electric ferc drilling utility transmission&#39;, &#39;gas fda clinical drug health products natural pharmaceutical patients care&#39;, &#39;stores merchandise company store brands bank products retail apparel retailers&#39;, &#39;insurance apartment properties tenants estate reit communities care health real&#39;, &#39;president vice mr stores served gas care health services merchandise&#39;, &#39;insurance president company vice mr reinsurance clients served chief officer&#39;, &#39;president apartment vice properties mr tenants estate reit communities real&#39;, &#39;company television programming content cable advertising fcc news video broadcast&#39;, &#39;aircraft airlines company care air health carriers medicare airline aviation&#39;, &#39;company clients solutions drilling oil services care merchandise gas health&#39;, &#39;drilling oil reserves wells insurance production proved rigs exploration rig&#39;, &#39;care health medicare company insurance customers medical wireless bank cms&#39;, &#39;care brands food beverage health products segment consumer packaging brand&#39;, &#39;cloud travel hotel security hotels airlines expedia data reservation guests&#39;, &#39;rail freight transportation railroad carriers customers network csx railroads intermodal&#39;, &#39;vehicle vehicles parts automotive travel rail gm cruise transportation carmax&#39;, &#39;payment card payments vehicle visa merchants credit vehicles paypal debit&#39;, &#39;apartment communities aimco residents redevelopment homes community multifamily partnership resident&#39;] . Latent Dirichlet Allocation . import pyLDAvis import pyLDAvis.sklearn pyLDAvis.enable_notebook() from sklearn.decomposition import LatentDirichletAllocation . lda_tf = LatentDirichletAllocation(n_components=10) lda_tf.fit(X) . LatentDirichletAllocation(batch_size=128, doc_topic_prior=None, evaluate_every=-1, learning_decay=0.7, learning_method=&#39;batch&#39;, learning_offset=10.0, max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001, n_components=10, n_jobs=None, perp_tol=0.1, random_state=None, topic_word_prior=None, total_samples=1000000.0, verbose=0) . /Users/jan/anaconda/lib/python3.6/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version of pandas will change to not sort by default. To accept the future behavior, pass &#39;sort=False&#39;. To retain the current behavior and silence the warning, pass &#39;sort=True&#39;. return pd.concat([default_term_info] + list(topic_dfs)) . Entropy heatmap . from scipy.stats import entropy def jensen_shannon(query, matrix): &quot;&quot;&quot; This function implements a Jensen-Shannon similarity between the input query (an LDA topic distribution for a document) and the entire corpus of topic distributions. It returns an array of length M where M is the number of documents in the corpus &quot;&quot;&quot; # lets keep with the p,q notation above p = query[None,:].T # take transpose q = matrix.T # transpose matrix m = 0.5*(p + q) return np.sqrt(0.5*(entropy(p,m) + entropy(q,m))) . lda_matrix = lda_tf.transform(X) js_df = pd.DataFrame() for i, ticker in enumerate(tickers): js_df[ticker] = jensen_shannon(lda_matrix[i], lda_matrix) js_df.index=tickers . js_df[&#39;AMT&#39;].sort_values() . AMT 0.000000 CCI 0.060150 IRM 0.176408 DLR 0.225501 WLTW 0.265463 ... LLY 0.832111 MRK 0.832179 AMGN 0.832195 VRTX 0.832203 BIIB 0.832239 Name: AMT, Length: 387, dtype: float64 . AMT is very similar to CCI but not to other stocks. This makes sense. I leave it to the reader compare their business descriptions. . &lt;Figure size 2160x1440 with 0 Axes&gt; . /Users/jan/anaconda/lib/python3.6/site-packages/seaborn/matrix.py:143: DeprecationWarning: elementwise comparison failed; this will raise an error in the future. if xticklabels == []: /Users/jan/anaconda/lib/python3.6/site-packages/seaborn/matrix.py:151: DeprecationWarning: elementwise comparison failed; this will raise an error in the future. if yticklabels == []: . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c30eaee48&gt; . Clustering stocks according to Item1A (Risk factors) . Loading risk factor documents . #collapse import os import re dr = &#39;/Users/jan/Desktop/SEC-EDGAR-text-copy/data/batch_0002/&#39; data = [] tickers = [] for batch in os.listdir(dr): if batch == &#39;.DS_Store&#39;: continue for file in os.listdir(os.path.join(dr, batch)): if &#39;Item1A_excerpt.txt&#39; in file : print(file) ticker = file.split(&#39;_&#39;)[0] if ticker in tickers: print(&#39;Skipping&#39;, ticker, &#39;Reason: ticker duplicated&#39;) continue with open(os.path.join(dr, batch, file)) as f: d = f.read() d = d.replace(&#39; n&#39;,&#39; &#39;) d = re.sub(&quot; d&quot;, &quot;&quot;, d) if len(d) &lt; 2000: print(&#39;Skipping&#39;, ticker, &#39;Reason: len too small&#39;) continue data.append(&#39; n&#39; + d + &#39; n&#39;) tickers.append(ticker) . . &lt;&gt;:19: DeprecationWarning: invalid escape sequence d &lt;&gt;:19: DeprecationWarning: invalid escape sequence d &lt;&gt;:19: DeprecationWarning: invalid escape sequence d &lt;ipython-input-15-1c00bd898fb3&gt;:19: DeprecationWarning: invalid escape sequence d d = re.sub(&#34; d&#34;, &#34;&#34;, d) . LH_0000920148_10K_20190228_Item1A_excerpt.txt FDX_0001048911_10K_20190716_Item1A_excerpt.txt Skipping FDX Reason: len too small SCG_0000091882_10K_20190228_Item1A_excerpt.txt AAL_0000004515_10K_20190225_Item1A_excerpt.txt EQR_0000906107_10K_20190221_Item1A_excerpt.txt DUK_0000017797_10K_20190228_Item1A_excerpt.txt AME_0001037868_10K_20190221_Item1A_excerpt.txt NLSN_0001492633_10K_20190228_Item1A_excerpt.txt ROK_0001024478_10K_20181109_Item1A_excerpt.txt AXP_0000004962_10K_20190213_Item1A_excerpt.txt CSX_0000277948_10K_20190206_Item1A_excerpt.txt FLIR_0000354908_10K_20190228_Item1A_excerpt.txt VNO_0000899689_10K_20190211_Item1A_excerpt.txt CHD_0000313927_10K_20190221_Item1A_excerpt.txt KIM_0000879101_10K_20190215_Item1A_excerpt.txt PSA_0001393311_10K_20190227_Item1A_excerpt.txt CCI_0001051470_10K_20190225_Item1A_excerpt.txt TXN_0000097476_10K_20190222_Item1A_excerpt.txt ORCL_0001341439_10K_20190621_Item1A_excerpt.txt GS_0000886982_10K_20190226_Item1A_excerpt.txt HD_0000354950_10K_20190328_Item1A_excerpt.txt ARNC_0000004281_10K_20190221_Item1A_excerpt.txt TXT_0000217346_10K_20190214_Item1A_excerpt.txt MRK_0000310158_10K_20190227_Item1A_excerpt.txt DIS_0001001039_10K_20181121_Item1A_excerpt.txt PKI_0000031791_10K_20190226_Item1A_excerpt.txt GD_0000040533_10K_20190213_Item1A_excerpt.txt OMC_0000029989_10K_20190212_Item1A_excerpt.txt MTB_0000036270_10K_20190220_Item1A_excerpt.txt LVLT_0000794323_10K_20190319_Item1A_excerpt.txt HOLX_0000859737_10K_20181120_Item1A_excerpt.txt ALL_0000899051_10K_20190215_Item1A_excerpt.txt MKC_0000063754_10K_20190125_Item1A_excerpt.txt CSCO_0000858877_10K_20190905_Item1A_excerpt.txt ETR_0000007323_10K_20190226_Item1A_excerpt.txt NWL_0000814453_10K_20190304_Item1A_excerpt.txt BAX_0000010456_10K_20190221_Item1A_excerpt.txt FAST_0000815556_10K_20190206_Item1A_excerpt.txt HBI_0001359841_10K_20190211_Item1A_excerpt.txt WLTW_0001140536_10K_20190227_Item1A_excerpt.txt BBT_0000092230_10K_20190226_Item1A_excerpt.txt HON_0000773840_10K_20190208_Item1A_excerpt.txt IR_0001466258_10K_20190212_Item1A_excerpt.txt PCG_0000075488_10K_20190228_Item1A_excerpt.txt NDAQ_0001120193_10K_20190222_Item1A_excerpt.txt BRKB_0001067983_10K_20190225_Item1A_excerpt.txt LOW_0000060667_10K_20190402_Item1A_excerpt.txt SHW_0000089800_10K_20190222_Item1A_excerpt.txt ZBH_0001136869_10K_20190226_Item1A_excerpt.txt WYNN_0001174922_10K_20190228_Item1A_excerpt.txt RF_0001281761_10K_20190222_Item1A_excerpt.txt HAS_0000046080_10K_20190226_Item1A_excerpt.txt TMK_0000320335_10K_20190301_Item1A_excerpt.txt KR_0000056873_10K_20190402_Item1A_excerpt.txt PEP_0000077476_10K_20190215_Item1A_excerpt.txt CINF_0000020286_10K_20190222_Item1A_excerpt.txt JNPR_0001043604_10K_20190222_Item1A_excerpt.txt FITB_0000035527_10K_20190301_Item1A_excerpt.txt AEE_0000018654_10K_20190226_Item1A_excerpt.txt AON_0000315293_10K_20190219_Item1A_excerpt.txt IPG_0000051644_10K_20190225_Item1A_excerpt.txt AVY_0000008818_10K_20190227_Item1A_excerpt.txt JEC_0000052988_10K_20181121_Item1A_excerpt.txt SPG_0001022344_10K_20190222_Item1A_excerpt.txt BK_0001390777_10K_20190227_Item1A_excerpt.txt Skipping BK Reason: len too small TSN_0000100493_10K_20181113_Item1A_excerpt.txt CAT_0000018230_10K_20190214_Item1A_excerpt.txt DHR_0000313616_10K_20190221_Item1A_excerpt.txt MMC_0000062709_10K_20190221_Item1A_excerpt.txt KMB_0000055785_10K_20190207_Item1A_excerpt.txt PNR_0000077360_10K_20190219_Item1A_excerpt.txt UAL_0000100517_10K_20190228_Item1A_excerpt.txt FL_0000850209_10K_20190402_Item1A_excerpt.txt PPG_0000079879_10K_20190221_Item1A_excerpt.txt AWK_0001410636_10K_20190219_Item1A_excerpt.txt WBA_0001618921_10K_20181011_Item1A_excerpt.txt UNP_0000100885_10K_20190208_Item1A_excerpt.txt XLNX_0000743988_10K_20190510_Item1A_excerpt.txt JWN_0000072333_10K_20190318_Item1A_excerpt.txt GT_0000042582_10K_20190208_Item1A_excerpt.txt JNJ_0000200406_10K_20190220_Item1A_excerpt.txt ITW_0000049826_10K_20190215_Item1A_excerpt.txt WYN_0001361658_10K_20190226_Item1A_excerpt.txt LYB_0001489393_10K_20190221_Item1A_excerpt.txt LMT_0000936468_10K_20190208_Item1A_excerpt.txt MYL_0001623613_10K_20190227_Item1A_excerpt.txt CL_0000021665_10K_20190221_Item1A_excerpt.txt TDG_0001260221_10K_20181109_Item1A_excerpt.txt PVH_0000078239_10K_20190329_Item1A_excerpt.txt SEE_0001012100_10K_20190219_Item1A_excerpt.txt LLL_0001039101_10K_20190221_Item1A_excerpt.txt WM_0000823768_10K_20190214_Item1A_excerpt.txt HRL_0000048465_10K_20181207_Item1A_excerpt.txt AMAT_0000006951_10K_20181213_Item1A_excerpt.txt MAT_0000063276_10K_20190222_Item1A_excerpt.txt GOOGL_0001652044_10K_20190205_Item1A_excerpt.txt PXD_0001038357_10K_20190226_Item1A_excerpt.txt VMC_0001396009_10K_20190226_Item1A_excerpt.txt CHK_0000895126_10K_20190227_Item1A_excerpt.txt OXY_0000797468_10K_20190221_Item1A_excerpt.txt UNH_0000731766_10K_20190212_Item1A_excerpt.txt DOW_0000029915_10K_20190211_Item1A_excerpt.txt AKAM_0001086222_10K_20190228_Item1A_excerpt.txt DD_0000030554_10K_20190211_Item1A_excerpt.txt HPE_0001645590_10K_20181212_Item1A_excerpt.txt TGNA_0000039899_10K_20190301_Item1A_excerpt.txt CNC_0001071739_10K_20190219_Item1A_excerpt.txt FB_0001326801_10K_20190131_Item1A_excerpt.txt SCHW_0000316709_10K_20190222_Item1A_excerpt.txt JBHT_0000728535_10K_20190222_Item1A_excerpt.txt GPN_0001123360_10K_20190221_Item1A_excerpt.txt APA_0000006769_10K_20190301_Item1A_excerpt.txt SO_0000003153_10K_20190220_Item1A_excerpt.txt CRM_0001108524_10K_20190308_Item1A_excerpt.txt DHI_0000882184_10K_20181116_Item1A_excerpt.txt ADBE_0000796343_10K_20190125_Item1A_excerpt.txt SBUX_0000829224_10K_20181116_Item1A_excerpt.txt MSFT_0000789019_10K_20190801_Item1A_excerpt.txt ADS_0001101215_10K_20190226_Item1A_excerpt.txt NKE_0000320187_10K_20190723_Item1A_excerpt.txt ADP_0000008670_10K_20190809_Item1A_excerpt.txt SLB_0000087347_10K_20190123_Item1A_excerpt.txt MA_0001141391_10K_20190213_Item1A_excerpt.txt ALXN_0000899866_10K_20190206_Item1A_excerpt.txt XRX_0000108772_10K_20190225_Item1A_excerpt.txt EW_0001099800_10K_20190215_Item1A_excerpt.txt ALK_0000766421_10K_20190215_Item1A_excerpt.txt IDXX_0000874716_10K_20190215_Item1A_excerpt.txt TRIP_0001526520_10K_20190222_Item1A_excerpt.txt BLK_0001364742_10K_20190228_Item1A_excerpt.txt RRC_0000315852_10K_20190225_Item1A_excerpt.txt AMZN_0001018724_10K_20190201_Item1A_excerpt.txt CELG_0000816284_10K_20190226_Item1A_excerpt.txt PCLN_0001075531_10K_20190227_Item1A_excerpt.txt EOG_0000821189_10K_20190226_Item1A_excerpt.txt BHI_0000808362_10K_20190219_Item1A_excerpt.txt V_0001403161_10K_20181116_Item1A_excerpt.txt MAR_0001048286_10K_20190301_Item1A_excerpt.txt CXO_0001358071_10K_20190220_Item1A_excerpt.txt XEC_0001168054_10K_20190220_Item1A_excerpt.txt NI_0001111711_10K_20190220_Item1A_excerpt.txt XOM_0000034088_10K_20190227_Item1A_excerpt.txt QRVO_0001604778_10K_20190517_Item1A_excerpt.txt COL_0001137411_10K_20181126_Item1A_excerpt.txt HES_0000004447_10K_20190221_Item1A_excerpt.txt REGN_0000872589_10K_20190207_Item1A_excerpt.txt ILMN_0001110803_10K_20190212_Item1A_excerpt.txt EXPE_0001324424_10K_20190208_Item1A_excerpt.txt TEL_0001385157_10K_20181113_Item1A_excerpt.txt MRO_0000101778_10K_20190221_Item1A_excerpt.txt ABBV_0001551152_10K_20190227_Item1A_excerpt.txt AJG_0000354190_10K_20190208_Item1A_excerpt.txt IRM_0001020569_10K_20190214_Item1A_excerpt.txt NEM_0001164727_10K_20190221_Item1A_excerpt.txt COG_0000858470_10K_20190226_Item1A_excerpt.txt NFLX_0001065280_10K_20190129_Item1A_excerpt.txt EQIX_0001101239_10K_20190222_Item1A_excerpt.txt CTSH_0001058290_10K_20190219_Item1A_excerpt.txt IBM_0000051143_10K_20190226_Item1A_excerpt.txt Skipping IBM Reason: len too small ISRG_0001035267_10K_20190204_Item1A_excerpt.txt PCAR_0000075362_10K_20190221_Item1A_excerpt.txt NOV_0001021860_10K_20190214_Item1A_excerpt.txt NVDA_0001045810_10K_20190221_Item1A_excerpt.txt APC_0000773910_10K_20190214_Item1A_excerpt.txt NAVI_0001593538_10K_20190226_Item1A_excerpt.txt GILD_0000882095_10K_20190226_Item1A_excerpt.txt MLM_0000916076_10K_20190225_Item1A_excerpt.txt EQT_0000033213_10K_20190214_Item1A_excerpt.txt PYPL_0001633917_10K_20190207_Item1A_excerpt.txt VRTX_0000875320_10K_20190213_Item1A_excerpt.txt CVX_0000093410_10K_20190222_Item1A_excerpt.txt DG_0000029534_10K_20190322_Item1A_excerpt.txt HP_0000046765_10K_20181116_Item1A_excerpt.txt AMG_0001004434_10K_20190222_Item1A_excerpt.txt IVZ_0000914208_10K_20190222_Item1A_excerpt.txt CERN_0000804753_10K_20190208_Item1A_excerpt.txt MAA_0000912595_10K_20190221_Item1A_excerpt.txt AVB_0000915912_10K_20190222_Item1A_excerpt.txt KO_0000021344_10K_20190221_Item1A_excerpt.txt MNST_0000865752_10K_20190228_Item1A_excerpt.txt OKE_0001039684_10K_20190226_Item1A_excerpt.txt FCX_0000831259_10K_20190215_Item1A_excerpt.txt ULTA_0001403568_10K_20190402_Item1A_excerpt.txt CMG_0001058090_10K_20190208_Item1A_excerpt.txt PGR_0000080661_10K_20190227_Item1A_excerpt.txt TSCO_0000916365_10K_20190221_Item1A_excerpt.txt MHK_0000851968_10K_20190228_Item1A_excerpt.txt WHR_0000106640_10K_20190212_Item1A_excerpt.txt ESS_0000920522_10K_20190221_Item1A_excerpt.txt CMA_0000028412_10K_20190212_Item1A_excerpt.txt ABC_0001140859_10K_20181120_Item1A_excerpt.txt CFG_0000759944_10K_20190221_Item1A_excerpt.txt DE_0000315189_10K_20181217_Item1A_excerpt.txt SYK_0000310764_10K_20190207_Item1A_excerpt.txt HUM_0000049071_10K_20190221_Item1A_excerpt.txt TSS_0000721683_10K_20190221_Item1A_excerpt.txt NEE_0000037634_10K_20190215_Item1A_excerpt.txt CCL_0000815097_10K_20190128_Item1A_excerpt.txt EA_0000712515_10K_20190524_Item1A_excerpt.txt ABT_0000001800_10K_20190222_Item1A_excerpt.txt MUR_0000717423_10K_20190227_Item1A_excerpt.txt CBG_0001138118_10K_20190301_Item1A_excerpt.txt LB_0000701985_10K_20190322_Item1A_excerpt.txt AMT_0001053507_10K_20190227_Item1A_excerpt.txt BFB_0000014693_10K_20190613_Item1A_excerpt.txt BXP_0001037540_10K_20190228_Item1A_excerpt.txt SNA_0000091440_10K_20190214_Item1A_excerpt.txt WAT_0001000697_10K_20190226_Item1A_excerpt.txt SWKS_0000004127_10K_20181115_Item1A_excerpt.txt ZTS_0001555280_10K_20190214_Item1A_excerpt.txt VAR_0000203527_10K_20181126_Item1A_excerpt.txt ACN_0001467373_10K_20181024_Item1A_excerpt.txt NUE_0000073309_10K_20190228_Item1A_excerpt.txt BSX_0000885725_10K_20190219_Item1A_excerpt.txt ATVI_0000718877_10K_20190228_Item1A_excerpt.txt SWK_0000093556_10K_20190226_Item1A_excerpt.txt STZ_0000016918_10K_20190423_Item1A_excerpt.txt EL_0001001250_10K_20190823_Item1A_excerpt.txt HBAN_0000049196_10K_20190215_Item1A_excerpt.txt NTRS_0000073124_10K_20190226_Item1A_excerpt.txt PBCT_0001378946_10K_20190301_Item1A_excerpt.txt BIIB_0000875045_10K_20190206_Item1A_excerpt.txt MTD_0001037646_10K_20190208_Item1A_excerpt.txt ROST_0000745732_10K_20190402_Item1A_excerpt.txt MCO_0001059556_10K_20190225_Item1A_excerpt.txt SJM_0000091419_10K_20190617_Item1A_excerpt.txt CHTR_0001091667_10K_20190131_Item1A_excerpt.txt PM_0001413329_10K_20190207_Item1A_excerpt.txt DISCK_0001437107_10K_20190301_Item1A_excerpt.txt TROW_0001113169_10K_20190213_Item1A_excerpt.txt R_0000085961_10K_20190221_Item1A_excerpt.txt ORLY_0000898173_10K_20190227_Item1A_excerpt.txt ROP_0000882835_10K_20190225_Item1A_excerpt.txt FRT_0000034903_10K_20190213_Item1A_excerpt.txt ZION_0000109380_10K_20190226_Item1A_excerpt.txt MOS_0001285785_10K_20190313_Item1A_excerpt.txt CHRW_0001043277_10K_20190225_Item1A_excerpt.txt WAT_0001000697_10KA_20190301_Item1A_excerpt.txt Skipping WAT Reason: ticker duplicated FFIV_0001048695_10K_20181121_Item1A_excerpt.txt CME_0001156375_10K_20190228_Item1A_excerpt.txt TMO_0000097745_10K_20190227_Item1A_excerpt.txt ALB_0000915913_10K_20190227_Item1A_excerpt.txt LUK_0000096223_10KT_20190129_Item1A_excerpt.txt COO_0000711404_10K_20181221_Item1A_excerpt.txt NOC_0001133421_10K_20190131_Item1A_excerpt.txt EXPD_0000746515_10K_20190222_Item1A_excerpt.txt BMY_0000014272_10K_20190225_Item1A_excerpt.txt UTX_0000101829_10K_20190207_Item1A_excerpt.txt IFF_0000051253_10K_20190226_Item1A_excerpt.txt FBHS_0001519751_10K_20190225_Item1A_excerpt.txt DLTR_0000935703_10K_20190327_Item1A_excerpt.txt SPGI_0000064040_10K_20190213_Item1A_excerpt.txt KMX_0001170010_10K_20190419_Item1A_excerpt.txt AGN_0001578845_10K_20190215_Item1A_excerpt.txt VRSK_0001442145_10K_20190219_Item1A_excerpt.txt UDR_0000074208_10K_20190219_Item1A_excerpt.txt FISV_0000798354_10K_20190221_Item1A_excerpt.txt COST_0000909832_10K_20181026_Item1A_excerpt.txt CMCSA_0000902739_10K_20190131_Item1A_excerpt.txt TJX_0000109198_10K_20190403_Item1A_excerpt.txt LKQ_0001065696_10K_20190301_Item1A_excerpt.txt EFX_0000033185_10K_20190221_Item1A_excerpt.txt AZO_0000866787_10K_20181024_Item1A_excerpt.txt UHS_0000352915_10K_20190227_Item1A_excerpt.txt APH_0000820313_10K_20190213_Item1A_excerpt.txt COP_0001163165_10K_20190219_Item1A_excerpt.txt DLR_0001297996_10K_20190225_Item1A_excerpt.txt D_0000103682_10K_20190228_Item1A_excerpt.txt ANTM_0001156039_10K_20190220_Item1A_excerpt.txt CVS_0000064803_10K_20190228_Item1A_excerpt.txt KSU_0000054480_10K_20190125_Item1A_excerpt.txt HSIC_0001000228_10K_20190220_Item1A_excerpt.txt CF_0001324404_10K_20190222_Item1A_excerpt.txt QCOM_0000804328_10K_20181107_Item1A_excerpt.txt MCK_0000927653_10K_20190515_Item1A_excerpt.txt USB_0000036104_10K_20190222_Item1A_excerpt.txt Skipping USB Reason: len too small MAC_0000912242_10K_20190225_Item1A_excerpt.txt RCL_0000884887_10K_20190222_Item1A_excerpt.txt PAYX_0000723531_10K_20190724_Item1A_excerpt.txt LUV_0000092380_10K_20190205_Item1A_excerpt.txt FMC_0000037785_10K_20190228_Item1A_excerpt.txt DLPH_0001521332_10K_20190204_Item1A_excerpt.txt GWW_0000277135_10K_20190228_Item1A_excerpt.txt BWA_0000908255_10K_20190219_Item1A_excerpt.txt STT_0000093751_10K_20190221_Item1A_excerpt.txt TIF_0000098246_10K_20190322_Item1A_excerpt.txt O_0000726728_10K_20190222_Item1A_excerpt.txt A_0001090872_10K_20181220_Item1A_excerpt.txt MAS_0000062996_10K_20190207_Item1A_excerpt.txt UPS_0001090727_10K_20190221_Item1A_excerpt.txt NSC_0000702165_10K_20190208_Item1A_excerpt.txt BDX_0000010795_10K_20181121_Item1A_excerpt.txt SLG_0001040971_10K_20190227_Item1A_excerpt.txt LNT_0000052485_10K_20190222_Item1A_excerpt.txt CB_0000896159_10K_20190228_Item1A_excerpt.txt DFS_0001393612_10K_20190220_Item1A_excerpt.txt URBN_0000912615_10K_20190401_Item1A_excerpt.txt ICE_0001571949_10K_20190207_Item1A_excerpt.txt HCA_0000860730_10K_20190221_Item1A_excerpt.txt VFC_0000103379_10K_20190524_Item1A_excerpt.txt ECL_0000031462_10K_20190301_Item1A_excerpt.txt ALLE_0001579241_10K_20190219_Item1A_excerpt.txt VRSN_0001014473_10K_20190215_Item1A_excerpt.txt O_0000726728_10KA_20190301_Item1A_excerpt.txt Skipping O Reason: ticker duplicated RHI_0000315213_10K_20190215_Item1A_excerpt.txt PFG_0001126328_10K_20190213_Item1A_excerpt.txt LEG_0000058492_10K_20190227_Item1A_excerpt.txt BWA_0000908255_10KA_20180928_Item1A_excerpt.txt Skipping BWA Reason: ticker duplicated FIS_0001136893_10K_20190221_Item1A_excerpt.txt WFC_0000072971_10K_20190227_Item1A_excerpt.txt Skipping WFC Reason: len too small HOG_0000793952_10K_20190228_Item1A_excerpt.txt PNC_0000713676_10K_20190301_Item1A_excerpt.txt ED_0000023632_10K_20190221_Item1A_excerpt.txt AAPL_0000320193_10K_20181105_Item1A_excerpt.txt HRB_0000012659_10K_20190614_Item1A_excerpt.txt GIS_0000040704_10K_20190628_Item1A_excerpt.txt KMI_0001506307_10K_20190208_Item1A_excerpt.txt VZ_0000732712_10K_20190215_Item1A_excerpt.txt AIG_0000005272_10K_20190215_Item1A_excerpt.txt L_0000060086_10K_20190213_Item1A_excerpt.txt BBBY_0000886158_10K_20190430_Item1A_excerpt.txt PRU_0001137774_10K_20190215_Item1A_excerpt.txt HCP_0000765880_10K_20190214_Item1A_excerpt.txt FLR_0001124198_10K_20190221_Item1A_excerpt.txt ES_0000013372_10K_20190226_Item1A_excerpt.txt K_0000055067_10K_20190225_Item1A_excerpt.txt VIAB_0001339947_10K_20181116_Item1A_excerpt.txt EMR_0000032604_10K_20181119_Item1A_excerpt.txt SRCL_0000861878_10K_20190228_Item1A_excerpt.txt CAG_0000023217_10K_20190719_Item1A_excerpt.txt UNM_0000005513_10K_20190219_Item1A_excerpt.txt WMT_0000104169_10K_20190328_Item1A_excerpt.txt CTXS_0000877890_10K_20190215_Item1A_excerpt.txt HPQ_0000047217_10K_20181213_Item1A_excerpt.txt ADI_0000006281_10K_20181127_Item1A_excerpt.txt FTR_0000020520_10K_20190301_Item1A_excerpt.txt AEP_0000004904_10K_20190221_Item1A_excerpt.txt NRG_0001013871_10K_20190228_Item1A_excerpt.txt JCI_0000833444_10K_20181120_Item1A_excerpt.txt BLL_0000009389_10K_20190222_Item1A_excerpt.txt SYMC_0000849399_10K_20190524_Item1A_excerpt.txt CPB_0000016732_10K_20180927_Item1A_excerpt.txt MET_0001099219_10K_20190222_Item1A_excerpt.txt EXR_0001289490_10K_20190226_Item1A_excerpt.txt NWSA_0001564708_10K_20190813_Item1A_excerpt.txt FSLR_0001274494_10K_20190222_Item1A_excerpt.txt SIG_0000832988_10K_20190403_Item1A_excerpt.txt RIG_0001451505_10K_20190219_Item1A_excerpt.txt TGT_0000027419_10K_20190313_Item1A_excerpt.txt GPC_0000040987_10K_20190225_Item1A_excerpt.txt KHC_0001637459_10K_20190607_Item1A_excerpt.txt M_0000794367_10K_20190403_Item1A_excerpt.txt MU_0000723125_10K_20181015_Item1A_excerpt.txt PEG_0000081033_10K_20190228_Item1A_excerpt.txt RL_0001037038_10K_20190516_Item1A_excerpt.txt MMM_0000066740_10K_20190207_Item1A_excerpt.txt DPS_0001418135_10K_20190228_Item1A_excerpt.txt FLS_0000030625_10K_20190220_Item1A_excerpt.txt BBY_0000764478_10K_20190328_Item1A_excerpt.txt PBI_0000078814_10K_20190220_Item1A_excerpt.txt TDC_0000816761_10KA_20190301_Item1A_excerpt.txt CNDT_0001677703_10K_20190228_Item1A_excerpt.txt MPC_0001510295_10K_20190228_Item1A_excerpt.txt SYMC_0000849399_10K_20181026_Item1A_excerpt.txt Skipping SYMC Reason: ticker duplicated KORS_0001530721_10K_20190529_Item1A_excerpt.txt ETN_0001551182_10K_20190227_Item1A_excerpt.txt STX_0001137789_10K_20190802_Item1A_excerpt.txt EMN_0000915389_10K_20190227_Item1A_excerpt.txt CMS_0000201533_10K_20190205_Item1A_excerpt.txt TAP_0000024545_10K_20190212_Item1A_excerpt.txt FE_0001031296_10K_20190219_Item1A_excerpt.txt CTAS_0000723254_10K_20190726_Item1A_excerpt.txt AYI_0001144215_10K_20181025_Item1A_excerpt.txt T_0000732717_10K_20190220_Item1A_excerpt.txt F_0000037996_10K_20190221_Item1A_excerpt.txt WMB_0000107263_10K_20190221_Item1A_excerpt.txt AIV_0000922864_10K_20190220_Item1A_excerpt.txt NWS_0001564708_10K_20190813_Item1A_excerpt.txt PLD_0001045609_10K_20190213_Item1A_excerpt.txt TRV_0000086312_10K_20190214_Item1A_excerpt.txt DGX_0001022079_10K_20190221_Item1A_excerpt.txt GM_0001467858_10K_20190206_Item1A_excerpt.txt LNC_0000059558_10K_20190220_Item1A_excerpt.txt AAP_0001158449_10K_20190219_Item1A_excerpt.txt NTAP_0001002047_10K_20190618_Item1A_excerpt.txt ADM_0000007084_10K_20190219_Item1A_excerpt.txt PPL_0000055387_10K_20190214_Item1A_excerpt.txt XEL_0000072903_10K_20190222_Item1A_excerpt.txt KLAC_0000319201_10K_20190816_Item1A_excerpt.txt DTE_0000028385_10K_20190207_Item1A_excerpt.txt KSS_0000885639_10K_20190322_Item1A_excerpt.txt PFE_0000078003_10K_20190228_Item1A_excerpt.txt MO_0000764180_10K_20190226_Item1A_excerpt.txt AMGN_0000318154_10K_20190213_Item1A_excerpt.txt PSX_0001534701_10K_20190222_Item1A_excerpt.txt IP_0000051434_10K_20190220_Item1A_excerpt.txt HST_0001061937_10K_20190226_Item1A_excerpt.txt EXC_0000008192_10K_20190208_Item1A_excerpt.txt LLY_0000059478_10K_20190219_Item1A_excerpt.txt CTL_0000018926_10K_20190311_Item1A_excerpt.txt BEN_0000038777_10K_20181109_Item1A_excerpt.txt WU_0001365135_10K_20190221_Item1A_excerpt.txt MSI_0000068505_10K_20190215_Item1A_excerpt.txt ENDP_0001593034_10K_20190228_Item1A_excerpt.txt GPS_0000039911_10K_20190319_Item1A_excerpt.txt TDC_0000816761_10K_20190226_Item1A_excerpt.txt Skipping TDC Reason: ticker duplicated CNP_0000048732_10K_20190228_Item1A_excerpt.txt SWN_0000007332_10K_20190228_Item1A_excerpt.txt GRMN_0001121788_10K_20190220_Item1A_excerpt.txt GLW_0000024741_10K_20190212_Item1A_excerpt.txt WEC_0000783325_10K_20190226_Item1A_excerpt.txt . Vectorize . vectorizer = CountVectorizer(stop_words=&#39;english&#39;) X = vectorizer.fit_transform(data).todense() vocab = np.array(vectorizer.get_feature_names()) vectorizer_tfidf = TfidfVectorizer(stop_words=&#39;english&#39;, max_df = 1.,) X_tfidf = vectorizer_tfidf.fit_transform(data) . Dendrogram . from scipy.cluster.hierarchy import ward, dendrogram raw_cosine = pd.DataFrame(cosine_similarity(X_tfidf,X_tfidf)) linkage_matrix = ward(raw_cosine) fig, ax = plt.subplots(figsize=(20, 50)) ax = dendrogram(linkage_matrix, orientation=&quot;right&quot;, labels=tickers); plt.tick_params( axis= &#39;x&#39;, which=&#39;both&#39;, bottom=&#39;off&#39;, top=&#39;off&#39;, labelbottom=&#39;off&#39; ) plt.savefig(&#39;ward_clusters_rf.png&#39;, dpi=200) . Singular Value Decomposition . u, s, v = decomposition.randomized_svd(X_tfidf, 40) . show_topics(v, 5) . [&#39;business operations financial products results&#39;, &#39;gas oil drilling natural crude&#39;, &#39;company products product operates manufacturing&#39;, &#39;reit tenants clients company estate&#39;, &#39;reit products tenants clinical properties&#39;, &#39;merchandise stores sales customers consumer&#39;, &#39;services oil content solutions products&#39;, &#39;merchandise stores medicare care oil&#39;, &#39;products reinsurance financial oil risk&#39;, &#39;clients client operations reit government&#39;, &#39;clients registrants energy electric utility&#39;, &#39;content programming advertising brands clients&#39;, &#39;aircraft airline airlines travel fuel&#39;, &#39;bank financial merchants card merchant&#39;, &#39;programming vehicles vehicle merchandise clinical&#39;, &#39;rail railroads tenants ptc fuel&#39;, &#39;contracts government merger utc contract&#39;, &#39;vehicles vehicle automotive registrants business&#39;, &#39;solutions business results drug clinical&#39;, &#39;aircraft products airline airlines condition&#39;, &#39;rail merchants registrants card merger&#39;, &#39;rail bank stock shares mr&#39;, &#39;registrants duke dte exelon drilling&#39;, &#39;ships cruise stock merger guests&#39;, &#39;crude utc business vehicles rail&#39;, &#39;registrants apartment aimco adversely maa&#39;, &#39;tobacco dow business cruise tenants&#39;, &#39;drilling cruise ships guests reit&#39;, &#39;tobacco solutions tenants altria bank&#39;, &#39;tobacco altria adult apartment reit&#39;, &#39;tobacco hotels altria hotel services&#39;, &#39;merger dow dupont utc drilling&#39;, &#39;hotels reit hotel mr customers&#39;, &#39;clearing mining pharmaceutical trading generic&#39;, &#39;business dow games dupont services&#39;, &#39;mr vice eastman utc president&#39;, &#39;wireless cms mining semiconductor crude&#39;, &#39;hotels grainger solutions hotel dow&#39;, &#39;vice mr eastman mining water&#39;, &#39;blackrock dow comerica aum dupont&#39;] . Latent Dirichlet Allocation . import pyLDAvis import pyLDAvis.sklearn pyLDAvis.enable_notebook() from sklearn.decomposition import LatentDirichletAllocation . lda_tf = LatentDirichletAllocation(n_components=10) lda_tf.fit(X) . LatentDirichletAllocation(batch_size=128, doc_topic_prior=None, evaluate_every=-1, learning_decay=0.7, learning_method=&#39;batch&#39;, learning_offset=10.0, max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001, n_components=10, n_jobs=None, perp_tol=0.1, random_state=None, topic_word_prior=None, total_samples=1000000.0, verbose=0) . /Users/jan/anaconda/lib/python3.6/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version of pandas will change to not sort by default. To accept the future behavior, pass &#39;sort=False&#39;. To retain the current behavior and silence the warning, pass &#39;sort=True&#39;. return pd.concat([default_term_info] + list(topic_dfs)) . Entropy heatmap . lda_matrix = lda_tf.transform(X) js_df = pd.DataFrame() for i, ticker in enumerate(tickers): js_df[ticker] = jensen_shannon(lda_matrix[i], lda_matrix) js_df.index=tickers . js_df[&#39;AMT&#39;].sort_values() . AMT 0.000000 DLR 0.140596 IRM 0.174905 EQIX 0.240909 CCI 0.249484 ... AMGN 0.829769 ALXN 0.831655 TGT 0.831767 CMG 0.832220 VRTX 0.832243 Name: AMT, Length: 401, dtype: float64 . &lt;Figure size 2160x1440 with 0 Axes&gt; . /Users/jan/anaconda/lib/python3.6/site-packages/seaborn/matrix.py:143: DeprecationWarning: elementwise comparison failed; this will raise an error in the future. if xticklabels == []: /Users/jan/anaconda/lib/python3.6/site-packages/seaborn/matrix.py:151: DeprecationWarning: elementwise comparison failed; this will raise an error in the future. if yticklabels == []: . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c25e68c50&gt; . Conclusion . NLP of documents can help clustering stocks. Clusters can be used in statistical arbitrage strategies. Exposure to risk factors is a valuable input to portfolio construction methods. It can be used either directly or as input to a shrinkage target for covariance matrix estimation regularization. .",
            "url": "https://jpwoeltjen.github.io/researchBlog/nlp/clustering/svd/lda/2019/12/20/nlp_10-k.html",
            "relUrl": "/nlp/clustering/svd/lda/2019/12/20/nlp_10-k.html",
            "date": " • Dec 20, 2019"
        }
        
    
  
    
        ,"post10": {
            "title": "Modeling Nonstationary Locally Mean-reverting Stochastic Processes",
            "content": "import pandas as pd import numpy as np from matplotlib import pyplot as plt from statsmodels.tsa.stattools import adfuller import seaborn as sns import statsmodels.api as sm from tqdm import tqdm, tnrange, tqdm_notebook import warnings warnings.filterwarnings(&quot;ignore&quot;) from IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = &quot;all&quot; %load_ext autoreload %autoreload 2 %config InlineBackend.figure_format = &#39;retina&#39; . Derivation of the Spread Dynamics . Imagine an asset $A$ that can be replicated, imperfectly, by some carefully chosen group of other assets. Let us call this group the synthetic hedge and denote it by $H$. Since the asset and its synthetic hedge share exposure to underlying processes, we can potentially eliminate a large part of the $A$&#39;s variance by subtracting off the synthetic hedge. We are left with exposure to the spread, i.e., $$Y_t= log left( frac{A_t}{A_0} right)- log left( frac{H_t}{H_0} right).$$ Now, analysis can be based on this spread. As an example, we might hypothesize that $Y_t$ is mean reverting. In order to optimally forecast $Y_t$ we have to think about how to model it. Modeling by a simple mean reverting process implies that $A$ and $H$ won&#39;t diverge too far from one another. Data of real world spreads contradicts this assumption. In fact, spreads tend to look close to geometric Brownian motion. If this were actually the case, there is no hope for forecasting future returns. However, it is possible that the spread has elements of both. . Hypothetical Dynamics (1) . Let us define a stochastic process that is common to both $H$ and $A$ $$C_t = C_{t-1} + epsilon_t,$$ where $ epsilon_t sim mathcal{N} left( mu, sigma^{2}_1 right)$. In words, $C_t$ is a geometric random walk. On top of that there is another random walk, independent of $C_t$, that is specific to $A$. Let us denote this process by $S_t$ and define it by $$S_t = S_{t-1} + omega_t,$$ where $ omega_t sim mathcal{N} left(0, sigma^{2}_ omega right)$. This process can be thought of as returns attributable to asset specific factors such as innovation, leadership, or lawsuits for a stock as an example. Next we define a mean reverting process, $$ R_t = phi R_{t-1} + u_t,$$ where $u_t sim mathcal{N} left(0, sigma^{2}_2 right).$ $R_t$ is an autoregressive process of order 1. This process can be thought of as temporary dislocations due to noise traders, order imbalances or a failure of timely information dissemination. . Let&#39;s imagine that $A_t$ develops as $$A_t = exp big(C_t + S_t + R_t big).$$ . $H_t$ is a function of the common process only and hence modeled by $$H_t = exp big(C_t big)$$ . Suppose that $ sigma^{2}_1 &gt;&gt; sigma^{2}_2$. In that case, there is too little signal in $A_t$ to observe it directly. Through hedging we can create exposure to the spread, though. To extract the spread we buy $A_t$ and short $H_t$. The log-return of this configuration from now $(t=0)$ to time $t$ is $$ log frac{A_t}{A_0} - log frac{H_t}{H_0} = (C_t + S_t + R_t) - (C_0 + S_0 + R_0) - (C_t-C_0) = R_t + S_t - R_0 - S_0.$$ I.e. the spread is a random walk around which an AR(1) process fluctuates. Since the random walk dominates asymptotically, this process is nonstationary. In other words, spread is modeled as a random walk observed with autocorrelated noise. These dynamics can be described by an ARIMA(1, 1, 1) model, which can be estimated via maximum likelihood estimation. . Hypothetical Dynamics (2) . One may also argue that the idiosyncratic process $S_t$ is an almost-nonstationary autoregressive process instead of a random walk. This process is modeled by a local-to-unity autoregressive root, i.e., $ phi$ is not 1 but close to it. Granger and &amp; Morris (1976) show that AR($p$) $+$ AR($q$) $=$ ARMA($p+q$, $max(p,q)$). Hence, the spread in our example would be modeled by an ARIMA(2, 0, 1). . Hypothetical Dynamics (3) . Furthermore, $H_t$ may also be a function of a specific random walk, which is defined by $$S_t^H = S_{t-1}^H + omega_t^H,$$ where $ omega_t^H sim mathcal{N} left(0, sigma^{2}_{ omega^H} right)$ and $ omega_t^H$ is independent of $ omega_t$. In addition $H_t$ may have a mean-reverting element, $R_t^H$, independent of all other processes considered so far. Then the spread is a random walk observed with ARMA(2,1) noise. Since any random walk with ARMA(p,q) noise is an ARIMA process, this spread can be modeled by an ARIMA model. . Simulation . def generate_garch_11_ts(n, sigma_sq_0, mu, alpha, beta, omega): &quot;&quot;&quot; generate GARCH log returns &quot;&quot;&quot; nu = np.random.normal(0,1,n) r = np.zeros(n) epsilon = np.zeros(n) sigma_sq = np.zeros(n) sigma_sq[0] = sigma_sq_0 if min(alpha,beta)&lt;0: raise ValueError(&#39;alpha, beta need to be non-negative&#39;) if omega &lt;=0: raise ValueError(&#39;omega needs to be positive&#39;) if alpha+beta&gt;=1: print(&#39;alpha+beta&gt;=1, variance not defined --&gt; time series will not be weakly stationary&#39;) for i in range(n): if i &gt;0: sigma_sq[i] = omega + alpha * epsilon[i-1]**2 + beta * sigma_sq[i-1] epsilon[i] = (sigma_sq[i]**0.5) * nu[i] r[i] = mu + epsilon[i] return r . def ornstein_uhlenbeck(theta, sigma, size, mu=0, x0=0): x = np.zeros(size) x[0] = x0 for i in range(1,size): dx = theta*(mu-x[i-1]) + sigma*np.random.normal(0,1) x[i] = dx + x[i-1] return x . Simulating an Asset and its Synthetic Hedge According to Hypothetical Dynamics (1) . epsilon = np.random.normal(0.0001,0.02,1000) # Gaussian specific innovations omega = np.random.normal(0, 0.01, epsilon.shape[0]) # Student t specific innovations # omega = 0.01*np.random.standard_t(2, epsilon.shape[0]) # GARCH specific innovations # omega = generate_garch_11_ts(epsilon.shape[0], sigma_sq_0=0, mu=0, alpha=0.1, beta=0.89, omega=0.0001) H = np.exp(np.cumsum(epsilon)) R = ornstein_uhlenbeck(0.5, 0.02, epsilon.shape[0]+1, mu=0, x0=0) S = omega.cumsum() # all equivalent calculations # A = np.exp(np.cumsum(epsilon + omega + np.diff(R))) # A = np.exp(np.cumsum(epsilon) + S + R[1:]) # A = np.exp(np.log(H) + S + R[1:]) A = H*np.exp(S + R[1:]) prices = pd.DataFrame(data={&#39;H&#39;:H, &#39;A&#39;:A}) prices.index = pd.date_range(start=&#39;1/1/2000&#39;, periods=len(prices), freq=&#39;B&#39;) plt.plot(prices) plt.show() . [&lt;matplotlib.lines.Line2D at 0x1c1ddbf550&gt;, &lt;matplotlib.lines.Line2D at 0x1c1e0504a8&gt;] . Y = np.log(prices.A) - np.log(prices.A.iloc[0]) - (np.log(prices.H) - np.log(prices.H.iloc[0])) Y.plot() plt.title(&#39;SPREAD&#39;) plt.show() Y[&#39;2000&#39;].plot() plt.title(&#39;SPREAD&#39;) plt.show() adfuller(Y) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c1e8f37f0&gt; . Text(0.5, 1.0, &#39;SPREAD&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c1e88fb38&gt; . Text(0.5, 1.0, &#39;SPREAD&#39;) . (-1.4982954917413052, 0.5343543656261089, 9, 990, {&#39;1%&#39;: -3.436972562223603, &#39;5%&#39;: -2.864463856182476, &#39;10%&#39;: -2.5683268054280175}, -4544.9966903021395) . Simulating an Asset and its Synthetic Hedge According to Hypothetical Dynamics (3) . n_obs = 2000 # Common Gaussain returns epsilon = np.random.normal(0.0,0.006, n_obs) # Gaussian specific innovations omega = np.random.normal(0, 0.01, epsilon.shape[0]) omega_H = np.random.normal(0, 0.01, epsilon.shape[0]) S = omega.cumsum() S_H = omega_H.cumsum() # Mean reverting process R = ornstein_uhlenbeck(0.1, 0.03, epsilon.shape[0]+1, mu=0, x0=0) R_H = ornstein_uhlenbeck(0.3, 0.01, epsilon.shape[0]+1, mu=0, x0=0) H = np.exp(np.cumsum(epsilon + omega_H + np.diff(R_H))) A = np.exp(np.cumsum(epsilon + omega + np.diff(R))) prices = pd.DataFrame(data={&#39;H&#39;:H, &#39;A&#39;:A}) prices.index = pd.date_range(start=&#39;1/1/2000&#39;, periods=len(prices), freq=&#39;B&#39;) plt.plot(prices) plt.show() . [&lt;matplotlib.lines.Line2D at 0x1c1edd8320&gt;, &lt;matplotlib.lines.Line2D at 0x1c1ee22c18&gt;] . Y = np.log(prices.A) - np.log(prices.A.iloc[0]) - (np.log(prices.H) - np.log(prices.H.iloc[0])) Y.plot() plt.title(&#39;SPREAD&#39;) plt.show() Y[&#39;2000&#39;].plot() plt.title(&#39;SPREAD&#39;) plt.show() adfuller(Y) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c1dd705c0&gt; . Text(0.5, 1.0, &#39;SPREAD&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c1e185a90&gt; . Text(0.5, 1.0, &#39;SPREAD&#39;) . (-3.2720065115288004, 0.01617710650081564, 13, 1986, {&#39;1%&#39;: -3.4336469649065298, &#39;5%&#39;: -2.862996415949189, &#39;10%&#39;: -2.5675453345421984}, -7685.910848870561) . SARIMAX Model Selection . x = Y train_pct = 0.5 train = int(train_pct*x.shape[0]) # introduce NaNs to observe behavior # x.loc[np.random.randint(1,len(x[:train]), len(x[:train]))] = np.nan # x.loc[np.random.randint(len(x[:train])+1, len(x), len(x[train:]))] = np.nan p_list = [] q_list = [] d_list = [] is_list = [] oos_list = [] is_aic_list = [] is_bic_list = [] params_list = [] for p in range(1, 2): for q in range(0, 3): for d in (1,): if q==p==0: continue p_list.append(p) q_list.append(q) d_list.append(d) order=(p, d, q) print(order) # IS fit mod = sm.tsa.statespace.SARIMAX(x[:train], order=order, # seasonal_order=(0, 0, 0, 0), # enforce_stationarity=False, # enforce_invertibility=False ) fit_results = mod.fit() print(fit_results.summary()) params_list.append(fit_results.params) _ = fit_results.plot_diagnostics(figsize=(15, 12)) plt.show() # IS prediction pred = fit_results.get_prediction(dynamic=False) trad_ret = (pred.predicted_mean-x.shift(1))*x.diff() trad_ret.cumsum().plot() plt.title(&#39;IS&#39;) plt.show() ir = trad_ret.mean()/(trad_ret.std()/np.sqrt(250)) is_list.append(ir) is_bic_list.append(fit_results.bic) is_aic_list.append(fit_results.aic) print(&#39;IR&#39;,ir) # OOS prediction mod = sm.tsa.statespace.SARIMAX(x[train:], order=order, # seasonal_order=(0, 0, 0, 0), # enforce_stationarity=False, # enforce_invertibility=False ) oos_results = mod.filter(fit_results.params)# this is needed to ingest data but doesn&#39;t change my test data pred = oos_results.get_prediction(dynamic=False,start=100) pred_ci = pred.conf_int() trad_ret = (pred.predicted_mean-x.shift(1))*x.diff() trad_ret.cumsum().plot() plt.title(&#39;OOS&#39;) plt.show() ir = trad_ret.mean()/(trad_ret.std()/np.sqrt(250)) oos_list.append(ir) print(&#39;IR&#39;,ir) (pred.predicted_mean.diff()-x.diff()).plot() plt.title(&#39;log-return prediction error&#39;) plt.show() # Null model: predict next returns as opposite signed last return trad_ret = (-x.diff().shift(1))*x.diff() trad_ret.cumsum().plot() plt.title(&#39;NULL OOS&#39;) plt.show() ir = trad_ret.mean()/(trad_ret.std()/np.sqrt(250)) print(&#39;IR&#39;,ir) df = pd.DataFrame({&#39;p&#39;:p_list, &#39;d&#39;:d_list, &#39;q&#39;:q_list, &#39;is&#39;:is_list, &#39;oos&#39;: oos_list, &#39;is_aic&#39;: is_aic_list, &#39;is_bic&#39;: is_bic_list, &#39;params&#39;:params_list }) df_sorted = df.sort_values(by=&#39;is_aic&#39;) df_sorted . (1, 1, 0) Statespace Model Results ============================================================================== Dep. Variable: y No. Observations: 1000 Model: SARIMAX(1, 1, 0) Log Likelihood 1896.681 Date: Wed, 04 Dec 2019 AIC -3789.362 Time: 20:58:47 BIC -3779.549 Sample: 01-03-2000 HQIC -3785.632 - 10-31-2003 Covariance Type: opg ============================================================================== coef std err z P&gt;|z| [0.025 0.975] ar.L1 -0.0759 0.032 -2.371 0.018 -0.139 -0.013 sigma2 0.0013 5.75e-05 22.822 0.000 0.001 0.001 =================================================================================== Ljung-Box (Q): 64.10 Jarque-Bera (JB): 1.29 Prob(Q): 0.01 Prob(JB): 0.53 Heteroskedasticity (H): 0.98 Skew: -0.08 Prob(H) (two-sided): 0.89 Kurtosis: 3.09 =================================================================================== Warnings: [1] Covariance matrix calculated using the outer product of gradients (complex-step). . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c2117b438&gt; . Text(0.5, 1.0, &#39;IS&#39;) . IR 1.2087901260674203 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c217fc978&gt; . Text(0.5, 1.0, &#39;OOS&#39;) . IR 0.8832811305644565 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c1ea5f9e8&gt; . Text(0.5, 1.0, &#39;log-return prediction error&#39;) . (1, 1, 1) Statespace Model Results ============================================================================== Dep. Variable: y No. Observations: 1000 Model: SARIMAX(1, 1, 1) Log Likelihood 1909.725 Date: Wed, 04 Dec 2019 AIC -3813.449 Time: 20:58:50 BIC -3798.729 Sample: 01-03-2000 HQIC -3807.854 - 10-31-2003 Covariance Type: opg ============================================================================== coef std err z P&gt;|z| [0.025 0.975] ar.L1 0.8567 0.034 25.546 0.000 0.791 0.922 ma.L1 -0.9497 0.020 -46.461 0.000 -0.990 -0.910 sigma2 0.0013 5.51e-05 23.215 0.000 0.001 0.001 =================================================================================== Ljung-Box (Q): 48.93 Jarque-Bera (JB): 3.04 Prob(Q): 0.16 Prob(JB): 0.22 Heteroskedasticity (H): 0.97 Skew: -0.09 Prob(H) (two-sided): 0.77 Kurtosis: 3.20 =================================================================================== Warnings: [1] Covariance matrix calculated using the outer product of gradients (complex-step). . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c1ee6f860&gt; . Text(0.5, 1.0, &#39;IS&#39;) . IR 2.5498711963729357 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c220fd908&gt; . Text(0.5, 1.0, &#39;OOS&#39;) . IR 2.461818296872518 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c233fc128&gt; . Text(0.5, 1.0, &#39;log-return prediction error&#39;) . (1, 1, 2) Statespace Model Results ============================================================================== Dep. Variable: y No. Observations: 1000 Model: SARIMAX(1, 1, 2) Log Likelihood 1909.836 Date: Wed, 04 Dec 2019 AIC -3811.672 Time: 20:58:54 BIC -3792.045 Sample: 01-03-2000 HQIC -3804.212 - 10-31-2003 Covariance Type: opg ============================================================================== coef std err z P&gt;|z| [0.025 0.975] ar.L1 0.8654 0.038 22.549 0.000 0.790 0.941 ma.L1 -0.9708 0.051 -19.045 0.000 -1.071 -0.871 ma.L2 0.0175 0.038 0.456 0.648 -0.058 0.093 sigma2 0.0013 5.51e-05 23.186 0.000 0.001 0.001 =================================================================================== Ljung-Box (Q): 48.96 Jarque-Bera (JB): 2.98 Prob(Q): 0.16 Prob(JB): 0.23 Heteroskedasticity (H): 0.97 Skew: -0.09 Prob(H) (two-sided): 0.78 Kurtosis: 3.20 =================================================================================== Warnings: [1] Covariance matrix calculated using the outer product of gradients (complex-step). . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c236a28d0&gt; . Text(0.5, 1.0, &#39;IS&#39;) . IR 2.553346430105229 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c1ed6b940&gt; . Text(0.5, 1.0, &#39;OOS&#39;) . IR 2.465331272615362 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c236e8a58&gt; . Text(0.5, 1.0, &#39;log-return prediction error&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c1e8c5080&gt; . Text(0.5, 1.0, &#39;NULL OOS&#39;) . IR 1.0447678647997458 . p d q is oos is_aic is_bic params . 1 1 | 1 | 1 | 2.549871 | 2.461818 | -3813.449449 | -3798.729185 | ar.L1 0.856690 ma.L1 -0.949701 sigma2 ... | . 2 1 | 1 | 2 | 2.553346 | 2.465331 | -3811.672253 | -3792.045234 | ar.L1 0.865388 ma.L1 -0.970781 ma.L2 ... | . 0 1 | 1 | 0 | 1.208790 | 0.883281 | -3789.362084 | -3779.548575 | ar.L1 -0.075854 sigma2 0.001313 dtype: f... | . Adaptive Out-of-sample Forecasting . x = Y x.index = range(len(x)) train_pct = 0.5 train = int(train_pct*x.shape[0]) ACTION_LAG = 0 N_PERIODS = 1 order = (1,1,1) ADAPTIVE = False ####################################################################################### #########################################TRAIN######################################### ####################################################################################### modl = sm.tsa.statespace.SARIMAX(x[:train], order=order) modl_fit = modl.fit() print(modl_fit.summary()) ####################################################################################### #########################################TEST######################################### ####################################################################################### x_test = x[train:] x_train = x[:train] pred = None for update_index in tqdm_notebook(range(0, len(x_test)-1)): next_x = x_test.iloc[update_index: update_index+1] x_train = x_train.append(next_x) modl = sm.tsa.statespace.SARIMAX(x_train, order=order) if ADAPTIVE: # this trains modl further each time a new data point arrives using prior parameters as # starting values # TODO how to initilize properly, choose &#39;learning rate&#39;? modl_fit = modl.fit(start_params=modl_fit.params , maxiter=5) else: # this filters the series without training further modl_fit = modl.filter(modl_fit.params) _pred_multi = modl_fit.forecast(N_PERIODS) # select the forecast furthest out in the future and put it in prediction series at # the timestamp of the next period _pred = pd.Series(data=_pred_multi.iloc[-1], index=[x_test.index[update_index+1]]) if pred is None: pred = _pred else: pred = pred.append(_pred) trad_ret = (pred-x.shift(1))*x.diff().shift(-ACTION_LAG) trad_ret.cumsum().plot() plt.title(&#39;OOS&#39;) plt.show() ir = trad_ret.mean()/(trad_ret.std()/np.sqrt(250)) print(&#39;OOS ir&#39;, ir) . Statespace Model Results ============================================================================== Dep. Variable: y No. Observations: 1000 Model: SARIMAX(1, 1, 1) Log Likelihood 1909.725 Date: Wed, 04 Dec 2019 AIC -3813.449 Time: 20:59:21 BIC -3798.729 Sample: 0 HQIC -3807.854 - 1000 Covariance Type: opg ============================================================================== coef std err z P&gt;|z| [0.025 0.975] ar.L1 0.8567 0.034 25.546 0.000 0.791 0.922 ma.L1 -0.9497 0.020 -46.461 0.000 -0.990 -0.910 sigma2 0.0013 5.51e-05 23.215 0.000 0.001 0.001 =================================================================================== Ljung-Box (Q): 48.93 Jarque-Bera (JB): 3.04 Prob(Q): 0.16 Prob(JB): 0.22 Heteroskedasticity (H): 0.97 Skew: -0.09 Prob(H) (two-sided): 0.77 Kurtosis: 3.20 =================================================================================== Warnings: [1] Covariance matrix calculated using the outer product of gradients (complex-step). . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c21191128&gt; . Text(0.5, 1.0, &#39;OOS&#39;) . OOS ir 2.3429464523910712 . Pitfalls of Irregularly Observed Multivariate Time Series . Regardless of the order that will eventually be used, we have to think carefully about which prices are observable and on which prices we can act. A bar is composed of several ticks observed at increasing time stamps. The last tick within the bar is observable (at the very least) at the closing time of the bar. We cannot assume a fill at this price in backtesting though. This fact is especially important when considering multiple instruments since the last tick of a bar may appear at different times for different instruments. If one were assuming fills at the close price, one would incur look-ahead bias or one would magically go back in time and trade at a past price. It is helpful to visualize these prices by a timeline as is done in Figure 1. . At $t=0$ we observe the closing price of the zeroth bar $A_0$ which is the traded price $p_0^A$ some point in time before -- or directly at -- $t_0$. The same holds for $H_0$, where the last traded price $p_0^H$ occurred slightly after $p_0^A$. $Y_0$ can be computed from $A_0$ and $H_0$ and predictions for $Y_1$ and beyond based on it. There is some lag due to data transmission and computation time. So, the first price we may assume a fill at -- before slippage -- is $p_1^A$ and $p_1^H$. Alternatively, if one only has the bar data without the ticks, one may use $A_1$ and $H_1$ assuming that the close prices occurred not within the computation lag. Then, a one-step forecast doesn&#39;t make sense anymore. In any case, do not forward (or backward) fill prices. This is also a problem for estimation. If prices are forward filled, parameters will be biased in favor of mean-reversion. If there is no traded price for the asset and the hedge at (almost) the same time, record a NaN and don&#39;t forward fill. In the state space form, ARIMA can handle the NaNs and no imputation is necessary. Since, there will always be some discrepancy between traded prices of the asset and its hedge, a small bias is unavoidable without having continuous bid/ask quotes. If the holding period is sufficiently long, the hope is that this bias is negligible. . Proof by Simulation that ffill() Price Intoduces Bias . n_obs = 10000 # Common Gaussain returns epsilon = np.random.normal(0.0,0.01, n_obs) # Gaussian specific innovations omega = np.random.normal(0, 0.01, epsilon.shape[0]) omega_H = np.random.normal(0, 0.01, epsilon.shape[0]) S = omega.cumsum() S_H = omega_H.cumsum() # Mean reverting process ################################################# # Setting the mean reversion speed to zero implies # a random walk and thus profits shouldn&#39;t be possible R = ornstein_uhlenbeck(0.0, 0.0, epsilon.shape[0]+1, mu=0, x0=0) R_H = ornstein_uhlenbeck(0.0, 0.0, epsilon.shape[0]+1, mu=0, x0=0) ################################################# H = np.exp(np.cumsum(epsilon + omega_H + np.diff(R_H))) A = np.exp(np.cumsum(epsilon + omega + np.diff(R))) na_prices = pd.DataFrame(data={&#39;H&#39;:H, &#39;A&#39;:A}) na_prices.loc[np.random.randint(1,len(na_prices), int(0.9*len(na_prices))), &#39;A&#39;] = np.nan na_prices.index = pd.date_range(start=&#39;1/1/2000&#39;, periods=len(na_prices), freq=&#39;B&#39;) plt.plot(na_prices) plt.title(&#39;Prices (raw)&#39;) plt.show() #############CULPRIT############################# ################################################# na_prices.ffill(inplace=True) plt.plot(na_prices) plt.title(&#39;Prices (ffill)&#39;) plt.show() ################################################# ################################################# Y = np.log(na_prices.A) - np.log(na_prices.A.iloc[0]) - (np.log(na_prices.H) - np.log(na_prices.H.iloc[0])) x = Y train_pct = 0.5 train = int(train_pct*x.shape[0]) order = (1, 1, 1) # IS fit mod = sm.tsa.statespace.SARIMAX(x[:train], order=order) fit_results = mod.fit() print(fit_results.summary()) _ = fit_results.plot_diagnostics(figsize=(15, 12)) plt.show() # IS prediction pred = fit_results.get_prediction(dynamic=False) trad_ret = (pred.predicted_mean-x.shift(1))*x.diff() trad_ret.cumsum().plot() plt.title(&#39;IS&#39;) plt.show() ir = trad_ret.mean()/(trad_ret.std()/np.sqrt(250)) print(&#39;IR&#39;,ir) # OOS prediction mod = sm.tsa.statespace.SARIMAX(x[train:], order=order) oos_results = mod.filter(fit_results.params)# this is needed to ingest data but doesn&#39;t change my test data pred = oos_results.get_prediction(dynamic=False,start=100) pred_ci = pred.conf_int() trad_ret = (pred.predicted_mean-x.shift(1))*x.diff() trad_ret.cumsum().plot() plt.title(&#39;OOS&#39;) plt.show() ir = trad_ret.mean()/(trad_ret.std()/np.sqrt(250)) print(&#39;IR&#39;,ir) (pred.predicted_mean.diff()-x.diff()).plot() plt.title(&#39;log-return prediction error&#39;) plt.show() . [&lt;matplotlib.lines.Line2D at 0x1c1e864828&gt;, &lt;matplotlib.lines.Line2D at 0x1c238eb8d0&gt;] . Text(0.5, 1.0, &#39;Prices (raw)&#39;) . [&lt;matplotlib.lines.Line2D at 0x1c1e8462e8&gt;, &lt;matplotlib.lines.Line2D at 0x1c1ed85550&gt;] . Text(0.5, 1.0, &#39;Prices (ffill)&#39;) . Statespace Model Results ============================================================================== Dep. Variable: y No. Observations: 5000 Model: SARIMAX(1, 1, 1) Log Likelihood 13003.408 Date: Wed, 04 Dec 2019 AIC -26000.816 Time: 21:00:50 BIC -25981.265 Sample: 01-03-2000 HQIC -25993.964 - 03-01-2019 Covariance Type: opg ============================================================================== coef std err z P&gt;|z| [0.025 0.975] ar.L1 0.5226 0.100 5.211 0.000 0.326 0.719 ma.L1 -0.6070 0.093 -6.522 0.000 -0.789 -0.425 sigma2 0.0003 4.82e-06 66.777 0.000 0.000 0.000 =================================================================================== Ljung-Box (Q): 33.45 Jarque-Bera (JB): 754.20 Prob(Q): 0.76 Prob(JB): 0.00 Heteroskedasticity (H): 0.99 Skew: -0.03 Prob(H) (two-sided): 0.87 Kurtosis: 4.90 =================================================================================== Warnings: [1] Covariance matrix calculated using the outer product of gradients (complex-step). . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c1ecbdc18&gt; . Text(0.5, 1.0, &#39;IS&#39;) . IR 1.6302357948054453 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c1edf2860&gt; . Text(0.5, 1.0, &#39;OOS&#39;) . IR 1.700099668801834 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c216982e8&gt; . Text(0.5, 1.0, &#39;log-return prediction error&#39;) . The positve profits, where no profits should be possible, show that the ffill introduced a bias. . Parametric Tests of Local Mean Reversion . The autocorrelation function (ACF) of an ARMA(1,1) process is given by $$ rho( tau)= frac{ gamma( tau)}{ gamma(0)}= phi^{ tau-1} frac{( phi+ theta)(1+ phi theta)}{1+2 theta phi+ theta^{2}}, quad tau geq 1.$$ . def arma11_acf(tau, phi, theta): return phi**(tau-1)*((phi+theta)*(1+phi*theta)/(1+2*theta*phi+theta**2)) df = pd.DataFrame(index=range(1,21)) for phi in (-0.8,0.8,0.9): for theta in (-0.89, -0.99): df[f&#39;$ phi$={phi} &#39; +r&#39;$ theta$=&#39; +str(theta)] = df.index.to_series().apply(arma11_acf, args=(phi,theta)) df.plot() plt.title(&#39;ACF ARMA(1,1)&#39;) plt.show() df . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c20c39978&gt; . Text(0.5, 1.0, &#39;ACF ARMA(1,1)&#39;) . $ phi$=-0.8 $ theta$=-0.89 $ phi$=-0.8 $ theta$=-0.99 $ phi$=0.8 $ theta$=-0.89 $ phi$=0.8 $ theta$=-0.99 $ phi$=0.9 $ theta$=-0.89 $ phi$=0.9 $ theta$=-0.99 . 1 -0.899624 | -0.899997 | -0.070416 | -0.099773 | 0.010468 | -0.049520 | . 2 0.719699 | 0.719998 | -0.056333 | -0.079818 | 0.009421 | -0.044568 | . 3 -0.575759 | -0.575998 | -0.045066 | -0.063855 | 0.008479 | -0.040112 | . 4 0.460607 | 0.460799 | -0.036053 | -0.051084 | 0.007631 | -0.036100 | . 5 -0.368486 | -0.368639 | -0.028842 | -0.040867 | 0.006868 | -0.032490 | . 6 0.294789 | 0.294911 | -0.023074 | -0.032694 | 0.006181 | -0.029241 | . 7 -0.235831 | -0.235929 | -0.018459 | -0.026155 | 0.005563 | -0.026317 | . 8 0.188665 | 0.188743 | -0.014767 | -0.020924 | 0.005007 | -0.023685 | . 9 -0.150932 | -0.150994 | -0.011814 | -0.016739 | 0.004506 | -0.021317 | . 10 0.120745 | 0.120796 | -0.009451 | -0.013391 | 0.004056 | -0.019185 | . 11 -0.096596 | -0.096636 | -0.007561 | -0.010713 | 0.003650 | -0.017267 | . 12 0.077277 | 0.077309 | -0.006049 | -0.008570 | 0.003285 | -0.015540 | . 13 -0.061822 | -0.061847 | -0.004839 | -0.006856 | 0.002957 | -0.013986 | . 14 0.049457 | 0.049478 | -0.003871 | -0.005485 | 0.002661 | -0.012587 | . 15 -0.039566 | -0.039582 | -0.003097 | -0.004388 | 0.002395 | -0.011329 | . 16 0.031653 | 0.031666 | -0.002478 | -0.003510 | 0.002155 | -0.010196 | . 17 -0.025322 | -0.025333 | -0.001982 | -0.002808 | 0.001940 | -0.009176 | . 18 0.020258 | 0.020266 | -0.001586 | -0.002247 | 0.001746 | -0.008259 | . 19 -0.016206 | -0.016213 | -0.001268 | -0.001797 | 0.001571 | -0.007433 | . 20 0.012965 | 0.012970 | -0.001015 | -0.001438 | 0.001414 | -0.006689 | . Likelihood ratio test . The thinking behind this test is that if the likelihood of the data is significantly greater under the ARIMA(1,1,1) than under a random walk ARIMA(0,1,0) we can conclude that there is local mean reversion. . from scipy.stats.distributions import chi2 def get_lrt_pvalue(specific_rw_distribution=&#39;normal&#39;): p_list = [] for i in tqdm_notebook(range(100)): # Common Gaussian shocks epsilon = np.random.normal(0.0001,0.02,1000) # Gaussian or GARCH specific shocks if specific_rw_distribution == &#39;normal&#39;: omega = np.random.normal(0, 0.01, epsilon.shape[0]) elif specific_rw_distribution == &#39;garch&#39;: omega = generate_garch_11_ts(epsilon.shape[0], 0.001, 0, 0.4, 0.49, 0.00001) else: raise ValueError(&#39;only `normal` or `garch` specific_rw_distribution supported.&#39;) # Mean reverting &#39;mispricing&#39; R = ornstein_uhlenbeck(0.1, 0.02, epsilon.shape[0]+1, mu=0, x0=0) S = omega.cumsum() C = epsilon.cumsum() H = np.exp(C) A = H*np.exp(S + R[1:]) prices = pd.DataFrame(data={&#39;H&#39;:H, &#39;A&#39;:A}) prices.index = pd.date_range(start=&#39;1/1/2000&#39;, periods=len(prices), freq=&#39;B&#39;) y = (np.log(prices.A) - np.log(prices.A.iloc[0]) - (np.log(prices.H) - np.log(prices.H.iloc[0]))) mod = sm.tsa.statespace.SARIMAX(y, order=(1,1,1)) fit_results = mod.fit() null_mod = sm.tsa.statespace.SARIMAX(y, order=(0,1,0)) null_fit_results = null_mod.fit() lr = 2*(fit_results.llf-null_fit_results.llf) p = chi2.sf(lr, 2) p_list.append(p) return p_list . p_list = get_lrt_pvalue(specific_rw_distribution=&#39;normal&#39;) p_series = pd.Series(p_list) print(&#39; null rejected:&#39;, (p_series&lt;0.05).sum()/len(p_series)) . null rejected: 0.63 . p_list = get_lrt_pvalue(specific_rw_distribution=&#39;garch&#39;) p_series = pd.Series(p_list) print(&#39; null rejected:&#39;, (p_series&lt;0.05).sum()/len(p_series)) . null rejected: 0.55 . from scipy.stats.distributions import chi2 def get_lrt_pvalue_rw(specific_rw_distribution=&#39;normal&#39;): p_list = [] for i in tqdm_notebook(range(100)): # Gaussian or GARCH specific shocks if specific_rw_distribution == &#39;normal&#39;: RW = pd.Series(np.random.normal(0.0001,0.02,1000)).cumsum() elif specific_rw_distribution == &#39;garch&#39;: RW = pd.Series(generate_garch_11_ts(1000, 0.0001, 0, 0.4, 0.49, 0.00001)).cumsum() else: raise ValueError(&#39;only `normal` or `garch` specific_rw_distribution supported.&#39;) mod = sm.tsa.statespace.SARIMAX(RW, order=(1,1,1)) fit_results = mod.fit() null_mod = sm.tsa.statespace.SARIMAX(RW, order=(0,1,0)) null_fit_results = null_mod.fit() lr = 2*(fit_results.llf-null_fit_results.llf) p = chi2.sf(lr, 2) p_list.append(p) return p_list . p_list = get_lrt_pvalue_rw(specific_rw_distribution=&#39;normal&#39;) p_series = pd.Series(p_list) print(&#39; null rejected:&#39;, (p_series&lt;0.05).sum()/len(p_series)) . null rejected: 0.01 . p_list = get_lrt_pvalue_rw(specific_rw_distribution=&#39;garch&#39;) p_series = pd.Series(p_list) print(&#39;null rejected:&#39;, (p_series&lt;0.05).sum()/len(p_series)) . null rejected: 0.23 . The likelihood ratio test works nicely if returns follow a Gaussian distribution. For heavy tailed distributions the null is overrejected significantly. . ARMA coefficient test with heteroskedasticity robust standard errors . t_ar_list = [] t_ar_ma_list = [] for i in tqdm_notebook(range(1000)): n_obs = 1000 #Specific shocks omega = generate_garch_11_ts(n_obs, 0.001, 0, 0.4, 0.49, 0.00001) S = omega.cumsum() # Mean reverting &#39;mispricing&#39; R = ornstein_uhlenbeck(0.1, 0.02, n_obs, mu=0, x0=0) y = pd.Series(S + R) mod = sm.tsa.statespace.SARIMAX(y, order=(1,1,1)) fit_results = mod.fit() t_ar = fit_results.params[&#39;ar.L1&#39;]/np.sqrt(fit_results.cov_params_robust[&#39;ar.L1&#39;][&#39;ar.L1&#39;]) t_ar_ma = (fit_results.params[&#39;ar.L1&#39;]+fit_results.params[&#39;ma.L1&#39;])/np.sqrt( fit_results.cov_params_robust[&#39;ar.L1&#39;][&#39;ar.L1&#39;] + fit_results.cov_params_robust[&#39;ma.L1&#39;][&#39;ma.L1&#39;] + 2*fit_results.cov_params_robust[&#39;ar.L1&#39;][&#39;ma.L1&#39;]) t_ar_list.append(t_ar) t_ar_ma_list.append(t_ar_ma) . df = pd.DataFrame({&#39;t_ar&#39;:t_ar_list, &#39;t_ar_ma&#39;:t_ar_ma_list}) print(&#39;null rejected:&#39;, len(df[((df.t_ar_ma&lt;-1.645))&amp;((df.t_ar&gt;1.645))])/(i+1)) print(&#39;null rejected:&#39;, len(df[((df.t_ar&gt;1.645))])/(i+1)) print(&#39;null rejected:&#39;, len(df[((df.t_ar_ma&lt;-1.645))])/(i+1)) print(&#39;null rejected:&#39;, len(df[((df.t_ar_ma&lt;-1.645))&amp;((df.t_ar&gt;0))])/(i+1)) . null rejected: 0.479 null rejected: 0.509 null rejected: 0.634 null rejected: 0.589 . t_ar_list = [] t_ar_ma_list = [] for i in tqdm_notebook(range(1000)): RW = pd.Series(generate_garch_11_ts(1000, 0.0001, 0, 0.4, 0.49, 0.00001)).cumsum() mod = sm.tsa.statespace.SARIMAX(RW, order=(1,1,1)) fit_results = mod.fit() t_ar = fit_results.params[&#39;ar.L1&#39;]/np.sqrt(fit_results.cov_params_robust[&#39;ar.L1&#39;][&#39;ar.L1&#39;]) t_ar_ma = (fit_results.params[&#39;ar.L1&#39;]+fit_results.params[&#39;ma.L1&#39;])/np.sqrt( fit_results.cov_params_robust[&#39;ar.L1&#39;][&#39;ar.L1&#39;] + fit_results.cov_params_robust[&#39;ma.L1&#39;][&#39;ma.L1&#39;] + 2*fit_results.cov_params_robust[&#39;ar.L1&#39;][&#39;ma.L1&#39;]) t_ar_list.append(t_ar) t_ar_ma_list.append(t_ar_ma) . df = pd.DataFrame({&#39;t_ar&#39;:t_ar_list, &#39;t_ar_ma&#39;:t_ar_ma_list}) print(&#39;null rejected:&#39;, len(df[((df.t_ar_ma&lt;-1.645))&amp;((df.t_ar&gt;1.645))])/(i+1)) print(&#39;null rejected:&#39;, len(df[((df.t_ar&gt;1.645))])/(i+1)) print(&#39;null rejected:&#39;, len(df[((df.t_ar_ma&lt;-1.645))])/(i+1)) print(&#39;null rejected:&#39;, len(df[((df.t_ar_ma&lt;-1.645))&amp;((df.t_ar&gt;0))])/(i+1)) . null rejected: 0.005 null rejected: 0.073 null rejected: 0.06 null rejected: 0.024 . Testing for Autocorrelations by AR(1) coefficient test with heteroskedasticity robust s.e. . To test whether there is a mean-reverting mispricing present, or the spread is better described by a pure random walk, we can use an autocorrelation test. Since the spread will almost certainly be heavy tailed. A Ljung-Box test, in its unmodified form, does not test our hypothesis. We could either modify it or simply test for joint significance of AR($p$) coefficients with heteroskedasticity robust standard errors. . t_ar_list = [] t_ar_ma_list = [] for i in tqdm_notebook(range(100)): # Common Gaussian shocks epsilon = np.random.normal(0.0001,0.02,1000) omega = generate_garch_11_ts(epsilon.shape[0], 0.001, 0, 0.4, 0.49, 0.00001) # Mean reverting &#39;mispricing&#39; R = ornstein_uhlenbeck(0.1, 0.02, epsilon.shape[0]+1, mu=0, x0=0) S = omega.cumsum() C = epsilon.cumsum() H = np.exp(C) A = H*np.exp(S + R[1:]) prices = pd.DataFrame(data={&#39;H&#39;:H, &#39;A&#39;:A}) prices.index = pd.date_range(start=&#39;1/1/2000&#39;, periods=len(prices), freq=&#39;B&#39;) y = (np.log(prices.A) - np.log(prices.A.iloc[0]) - (np.log(prices.H) - np.log(prices.H.iloc[0]))) mod = sm.tsa.statespace.SARIMAX(y, order=(1,1,0)) fit_results = mod.fit() t_ar = fit_results.params[&#39;ar.L1&#39;]/np.sqrt(fit_results.cov_params_robust[&#39;ar.L1&#39;][&#39;ar.L1&#39;]) t_ar_list.append(t_ar) df = pd.DataFrame({&#39;t_ar&#39;:t_ar_list}) print(&#39;null rejected:&#39;, len(df[df.t_ar&lt;-1.645])/(i+1)) . null rejected: 0.29 . t_ar_list = [] for i in tqdm_notebook(range(100)): RW = pd.Series(generate_garch_11_ts(1000, 0.0001, 0, 0.4, 0.49, 0.00001)).cumsum() mod = sm.tsa.statespace.SARIMAX(RW, order=(1,1,0)) fit_results = mod.fit() t_ar = fit_results.params[&#39;ar.L1&#39;]/np.sqrt(fit_results.cov_params_robust[&#39;ar.L1&#39;][&#39;ar.L1&#39;]) t_ar_list.append(t_ar) df = pd.DataFrame({&#39;t_ar&#39;:t_ar_list}) print(&#39;null rejected:&#39;, len(df[df.t_ar&lt;-1.645])/(i+1)) . null rejected: 0.07 . Testing for Autocorrelations by AR($p$) coefficient test with heteroskedasticity robust s.e. . from statsmodels.regression.linear_model import OLS f_list = [] for i in tqdm_notebook(range(1000)): n_obs = 1000 #Specific shocks omega = generate_garch_11_ts(n_obs, 0.001, 0, 0.4, 0.49, 0.00001) S = omega.cumsum() # Mean reverting &#39;mispricing&#39; R = ornstein_uhlenbeck(0.1, 0.02, n_obs, mu=0, x0=0) y = pd.Series(S + R) diff = y.diff() X = pd.DataFrame() for l in range(0,10): X[f&#39;lag_{l}&#39;] = diff.shift(l) X = X.dropna() reg = OLS(X.iloc[:,0], X.iloc[:,1:]) fit = reg.fit() # fit.summary() f_list.append(fit.f_test(np.eye(l),fit.cov_HC3).pvalue) df = pd.DataFrame({&#39;pvalue&#39;:f_list}) print(&#39;null rejected:&#39;, len(df[df.pvalue&lt;0.05])/(i+1)) . null rejected: 0.578 . from statsmodels.regression.linear_model import OLS f_list = [] for i in tqdm_notebook(range(1000)): RW = pd.Series(generate_garch_11_ts(1000, 0.0001, 0, 0.4, 0.49, 0.00001)).cumsum() diff = RW.diff() X = pd.DataFrame() for l in range(0,10): X[f&#39;lag_{l}&#39;] = diff.shift(l) X = X.dropna() reg = OLS(X.iloc[:,0], X.iloc[:,1:]) fit = reg.fit() # fit.summary() f_list.append(fit.f_test(np.eye(l),fit.cov_HC3).pvalue) df = pd.DataFrame({&#39;pvalue&#39;:f_list}) print(&#39;null rejected:&#39;, len(df[df.pvalue&lt;0.05])/(i+1)) . null rejected: 0.073 . Optimal Trading under Transaction costs . The nice thing about the ARIMA model is that a multi-period forecasts are easy to obtain. Hence, one can compute the expected return from now until some point in the future and compare the maximum cumulative return to the expected transaction costs. The decision to enter a position can then be based on the sign and magnitude of the difference. . n_obs = 50000 # Common Gaussain returns epsilon = np.random.normal(0.0,0.01, n_obs) # Gaussian specific innovations omega = np.random.normal(0, 0.001, epsilon.shape[0]) omega_H = np.random.normal(0, 0.001, epsilon.shape[0]) S = omega.cumsum() S_H = omega_H.cumsum() # Mean reverting process R = ornstein_uhlenbeck(0.01, 0.01, epsilon.shape[0]+1, mu=0, x0=0) R_H = ornstein_uhlenbeck(0.1, 0.01, epsilon.shape[0]+1, mu=0, x0=0) H = np.exp(np.cumsum(epsilon + omega_H + np.diff(R_H))) A = np.exp(np.cumsum(epsilon + omega + np.diff(R))) prices = pd.DataFrame(data={&#39;H&#39;:H, &#39;A&#39;:A}) prices.index = pd.date_range(start=&#39;1/1/2000&#39;, periods=len(prices), freq=&#39;B&#39;) Y = np.log(prices.A) - np.log(prices.A.iloc[0]) - (np.log(prices.H) - np.log(prices.H.iloc[0])) x = Y order = (1, 1, 1) mod = sm.tsa.statespace.SARIMAX(x, order=order) modl_fit = mod.fit() modl_fit.forecast(100).plot() plt.title(&#39;Multi-period spread forecast&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c1e846518&gt; . Text(0.5, 1.0, &#39;Multi-period spread forecast&#39;) .",
            "url": "https://jpwoeltjen.github.io/researchBlog/efficiency/spreads/time%20series/stat-arb/2019/12/04/NonstationarySpreadsModeling.html",
            "relUrl": "/efficiency/spreads/time%20series/stat-arb/2019/12/04/NonstationarySpreadsModeling.html",
            "date": " • Dec 4, 2019"
        }
        
    
  
    
        ,"post11": {
            "title": "Notes on Quantitative Risk Management",
            "content": "import pandas as pd import scipy.stats import numpy as np import pylab import scipy.stats as stats from matplotlib import pyplot as plt plt.style.use(&#39;seaborn&#39;) # %matplotlib notebook import seaborn as sns import random random.seed(1) . 1 VaR and Expected Shortfall . Definition 1.1 . Let $ alpha in (0, 1)$ be a fixed level. The Value-at-Risk (VaR) for level $ alpha in (0, 1)$ is defined as . $$VaR_ alpha(L_{n+1}) = inf { mathcal{l} in mathbb{R} : P(L_{n+1} &gt; mathcal{l}) leq 1- alpha } $$ . Definition 1.2 . Suppose that the conditional law of $ { L_{n+1}$ given $Z_0, . . . , Z_n$ is continuous. For fixed level $ alpha in (0, 1)$ the expected shortfall at level $ alpha$ is defined as . $$ES_ alpha(L_{n+1}) = mathbb{E}_n { L_{n+1} | L_{n+1}&gt;VaR_ alpha(L_{n+1}) } $$ . Let&#39;s study these risk measures with simulated log-normal returns and fat-tailed but stationary GARCH returns. . 1.1 GARCH . def generate_garch_11_ts(n, sigma_sq_0, mu, alpha, beta, omega): &quot;&quot;&quot; generate GARCH log returns &quot;&quot;&quot; nu = np.random.normal(0,1,n) r = np.zeros(n) epsilon = np.zeros(n) sigma_sq = np.zeros(n) sigma_sq[0] = sigma_sq_0 if min(alpha,beta)&lt;0: raise ValueError(&#39;alpha, beta need to be non-negative&#39;) if omega &lt;=0: raise ValueError(&#39;omega needs to be positive&#39;) if alpha+beta&gt;=1: print(&#39;alpha+beta&gt;=1, variance not defined --&gt; time series will not be weakly stationary&#39;) for i in range(n): if i &gt;0: sigma_sq[i] = omega + alpha * epsilon[i-1]**2 + beta * sigma_sq[i-1] epsilon[i] = (sigma_sq[i]**0.5) * nu[i] r[i] = mu + epsilon[i] return r . garch_returns = generate_garch_11_ts(250*15, 1e-5, 0.0, 0.5, 0.45, 1e-5) price = np.exp(np.cumsum(garch_returns)) fig, axes = plt.subplots(2,1) fig.tight_layout() for ax, y, name in zip(axes, [price,garch_returns], [&#39;GARCH Price Time Series&#39;,&#39; GARCH Returns&#39;]): ax.plot(y) ax.set(title=name) plt.show() . stats.probplot(garch_returns, dist=&quot;norm&quot;, plot=pylab) pylab.show() . sns.distplot(garch_returns, bins=200, label=&#39;GARCH-1-1 Returns&#39;) plt.legend() plt.show() . The QQ-Plot very clearly shows fat tails. For the histogram, however, one could falsely assume normality. . def VaR_log_normal(x, alpha, lookback): &quot;&quot;&quot; compute daily VaR given log-normal returns &quot;&quot;&quot; quantile = scipy.stats.norm.ppf(alpha) x = pd.Series(x) returns = (np.log(x)).diff() mu = returns.rolling(lookback).mean() sigma = returns.rolling(lookback).std() VaR = x*(1-np.exp((mu-sigma*quantile))) return VaR alpha = 0.99 lookback = 250 VaR = VaR_log_normal(price, alpha, lookback) . loss = -pd.Series(np.diff(price)) violation = (loss.values[lookback:]&gt;VaR.values[lookback:-1]).astype(int) violation . array([0, 0, 0, ..., 0, 0, 0]) . plt.plot(loss) plt.plot(VaR) plt.title(&#39;Loss vs. VaR&#39;) plt.show() . print(f&#39;Relative Frequency of Violations ({round(1-alpha,4)} expected under log-normal assumption): &#39;, violation.mean()) . Relative Frequency of Violations (0.01 expected under log-normal assumption): 0.022577879394112602 . 1.2 Log-Normal . log_normal_returns = np.random.normal(0,0.1/(250**0.5),len(garch_returns)) ln_price = np.exp(np.cumsum(log_normal_returns)) fig, axes = plt.subplots(2,1) fig.tight_layout() for ax, y, name in zip(axes, [ln_price,log_normal_returns], [&#39;LN Asset Price Time Series&#39;,&#39; Log-Normal Returns&#39;]): ax.plot(y) ax.set(title=name) plt.show() . stats.probplot((log_normal_returns), dist=&quot;norm&quot;, plot=pylab) pylab.show() . stats.probplot(np.exp(log_normal_returns), dist=&quot;norm&quot;, plot=pylab) pylab.show() . For comparison, below are platykurtic and leptokurtic distributions: . Platykurtic (negative excess kurtosis) . stats.probplot(np.random.uniform(0,1,log_normal_returns.shape), dist=&quot;norm&quot;, plot=pylab) pylab.show() . Leptokurtic (positive excess kurtosis) . stats.probplot(np.random.standard_t(3,log_normal_returns.shape), dist=&quot;norm&quot;, plot=pylab) pylab.show() . ln_VaR = VaR_log_normal(ln_price, alpha, lookback) ln_loss = -pd.Series(np.diff(ln_price)) ln_violation = (ln_loss.values[lookback:]&gt;ln_VaR.values[lookback:-1]).astype(int) . plt.plot(ln_loss) plt.plot(ln_VaR) plt.title(&#39;ln_Loss vs. ln_VaR&#39;) plt.show() . print(f&#39;Relative Frequency of Violations ({round(1-alpha,4)} expected under log-normal assumption): &#39;, ln_violation.mean()) . Relative Frequency of Violations (0.01 expected under log-normal assumption): 0.00943126607602172 . As can be seen from the above experiments the log-normal VaR estimate severely under estimates the risk if the return distribution is fat tailed. . def phi(x): return np.exp(-0.5*x**2)/np.sqrt(2*np.pi); def VaR_ES_var_covar (x, c, w, alpha): mu = np.mean(x,axis=0); sigma = np.cov(x); q = scipy.stats.norm.ppf(alpha) VaR = -(c + np.dot(np.transpose(w),np.transpose(mu))) + np.sqrt(np.dot(np.transpose(w),np.dot(sigma,w)))*q ES = -(c + np.dot(np.transpose(w),np.transpose(mu))) + np.sqrt(np.dot(np.transpose(w),np.dot(sigma,w)))/(1-alpha)*phi(q) return VaR, ES VaR, ES = VaR_ES_var_covar (garch_returns, 0, 1, alpha) print(&#39;VaR GARCH: &#39;, VaR) print(&#39;Expected Shortfall GARCH: &#39;, ES) print(&#39;Mean Violation GARCH: &#39;,np.mean(-garch_returns[garch_returns&lt;-VaR])) . VaR GARCH: 0.03146499939671988 Expected Shortfall GARCH: 0.03605991516868822 Mean Violation GARCH: 0.052666650092480304 . VaR, ES = VaR_ES_var_covar (log_normal_returns, 0, 1, alpha) print(&#39;VaR Log-Normal: &#39;, VaR) print(&#39;Expected Shortfall Log-Normal: &#39;, ES) print(&#39;Mean Violation Log-Normal: &#39;,np.mean(-log_normal_returns[log_normal_returns&lt;-VaR])) . VaR Log-Normal: 0.01472793412031715 Expected Shortfall Log-Normal: 0.016875034681794663 Mean Violation Log-Normal: 0.01717357925520597 . The Var-Covar method assumes iid Gaussian risk factor changes. It thus under-estimates ES of the GARCH model. The ES estimate of the log-normal returns is unbiased. . def VaR_ES_historic(x, alpha): x = np.sort(x) index = int(len(x)*(1-alpha))+1 VaR = x[index] ES = 1/index*np.sum(x[:index]) return -VaR,-ES . hist_VaR,hist_ES = VaR_ES_historic(garch_returns, alpha) print(&#39;VaR GARCH: &#39;, hist_VaR) print(&#39;Expected Shortfall GARCH: &#39;, hist_ES) . VaR GARCH: 0.034047583394326314 Expected Shortfall GARCH: 0.05577800317882427 . Some remarks regarding VaR and ES . Coherent risk measures . A coherent risk measure is a function ${ displaystyle varrho } $ that satisfies properties of monotonicity, sub-additivity, homogeneity, and translational invariance. . Monotonicity . ${ displaystyle mathrm {If} ;Z_{1},Z_{2} in { mathcal {L}} ; mathrm {and} ;Z_{1} leq Z_{2} ; mathrm {a.s.} , ; mathrm {then} ; varrho (Z_{1}) geq varrho (Z_{2})} $ That is, if portfolio ${ displaystyle Z_{2}} $ always has better values than portfolio ${ displaystyle Z_{1}}$ under almost all scenarios then the risk of ${ displaystyle Z_{2}} $ should be less than the risk of ${ displaystyle Z_{1}}$. . Sub-additivity . ${ displaystyle mathrm {If} ;Z_{1},Z_{2} in { mathcal {L}}, ; mathrm {then} ; varrho (Z_{1}+Z_{2}) leq varrho (Z_{1})+ varrho (Z_{2})} $ Indeed, the risk of two portfolios together cannot get any worse than adding the two risks separately: this is the diversification principle. In financial risk management, sub-additivity implies diversification is beneficial. . Positive homogeneity . ${ displaystyle mathrm {If} ; alpha geq 0 ; mathrm {and} ;Z in { mathcal {L}}, ; mathrm {then} ; varrho ( alpha Z)= alpha varrho (Z)} $ Loosely speaking, if you double your portfolio then you double your risk. In financial risk management, positive homogeneity implies the risk of a position is proportional to its size. . Translation invariance . If ${ displaystyle A}$ is a deterministic portfolio with guaranteed return ${ displaystyle a}$ and ${ displaystyle Z in { mathcal {L}}} $ then . $ { displaystyle varrho (Z+A)= varrho (Z)-a} varrho(Z + A) = varrho(Z) - a $ The portfolio ${ displaystyle A}$ is just adding cash ${ displaystyle a} $ to your portfolio ${ displaystyle Z}$. In particular, if ${ displaystyle a= varrho (Z)}$ then ${ displaystyle varrho (Z+A)=0}$. In financial risk management, translation invariance implies that the addition of a sure amount of capital reduces the risk by the same amount. . Convex risk measures . The notion of coherence has been subsequently relaxed. Indeed, the notions of Sub-additivity and Positive Homogeneity can be replaced by the notion of convexity: . Convexity . ${ displaystyle { text{If }}Z_{1},Z_{2} in { mathcal {L}}{ text{ and }} lambda in [0,1]{ text{ then }} varrho ( lambda Z_{1}+(1- lambda )Z_{2}) leq lambda varrho (Z_{1})+(1- lambda ) varrho (Z_{2})}$ . VaR is not a convex risk measure, meaning diversification with non-perfectly-correlated assets does not always lead to reduction in the risk measure, which is a serious flaw. In addition, VaR only provides information about the probability of a large loss but not its amount. ES does not suffer from these drawbacks. . 2 Extreme Value Theory . Definition 2.1 . A function $h : (0,+ infty) rightarrow (0,+ infty)$ with the limit . $$ g(t)= lim _{{t to infty }}{ frac {h(tx)}{h(x)}} = x^ rho $$ . for every $x &gt; 0$, is called regularly varying in $ infty$ with index $ rho in mathbb{R}$ (written $h in RV_ rho$) . For $ rho = 0$, and hence $g(t)=1$ we say that $h$ is slowly varying in $ infty$. . A random variable $X$ with cdf $F$ is called regularly varying if $ bar F in RV_{- alpha}$ for some $ alpha geq 0$, where $ bar F(x) = 1 - F(x)$. . To estimate the parameter $ alpha$ of a regularly varying random variable $X$, we use the Hill estimator. . Hill&#39;s tail-index estimator . $$ displaystyle hat alpha _{(k(n),n)}^{ text{Hill}}= left({ frac {1}{k(n)}} sum _{i=n-k(n)+1}^{n} ln(X_{(i,n)})- ln(X_{(n-k(n)+1,n)}) right)^{-1}$$ . Choose k graphically by looking at the Hill plot. If we are lucky, the plot looks approximately constant after some initial oscillations. This is the part where k and hence the estimate should be chosen. . def Hill_Estimator (x, k): y = np.sort(x)[::-1] return k / np.sum( np.log(y[:k-1]) - np.log(y[k]) ) def Hill_Plot(x): y=x[x&gt;0] n = len(y) a = [] for k in range(1,n): alpha_hat = Hill_Estimator(y,k) a.append(alpha_hat) plt.plot(range(1,n), a) plt.show() def VaR_ES_Hill(x, p, k): n = len(x) alpha = Hill_Estimator(x,k) y = np.sort(x)[::-1] VaR= (n/k*(1-p))**(-1/alpha)*y[k] ES=(1-1/alpha)**(-1)*VaR return VaR,ES . # n=1000 x = garch_returns[:1000]#np.random.standard_t(10, n)**2 Hill_Plot(x) . /Users/jan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in double_scalars This is separate from the ipykernel package so we can avoid doing imports until . k = 50 VaR, ES = VaR_ES_Hill(x,0.95,k) alpha = Hill_Estimator(x,k) print(&#39;Alpha_HILL: &#39;, alpha) print(&#39;VaR HILL: &#39;, VaR) print(&#39;ES HILL: &#39;, ES) . Alpha_HILL: 2.2348170736040154 VaR HILL: 0.019049928996944472 ES HILL: 0.034477257792572595 . Peaks Over Treshold (POT) . Let $X$ be a random variable with cdf $F$ . Define its excess distribution function over threshold $u$ as . $$ displaystyle F_{u}(y)=P(X-u leq y|X&gt;u)={ frac {F(u+y)-F(u)}{1-F(u)}}$$ . for $0 leq y leq x_{F}-u$ . . def mean_exess_fct(x, u): if u&gt;=max(x): raise ValueError(&quot; u must be smaller than max(x) &quot;) e = np.mean(x[x&gt;u]-u) return e def mean_exess_plot(x): x = pd.Series(x) x = x.sort_values(ascending=False) e = x[1:].apply(lambda u: mean_exess_fct(x, u)) plt.plot(x[1:],e,&#39;.&#39;) plt.xlabel(r&#39;$u$&#39;) plt.ylabel(r&#39;$e(u)$&#39;) plt.title(&#39;Mean Exess Plot&#39;) plt.show() . print(&#39;log-normal returns&#39;) mean_exess_plot(-log_normal_returns) print(&#39;GARCH returns&#39;) mean_exess_plot(-garch_returns) . log-normal returns . GARCH returns . If the mean exess plot behaves approximately linear with positive slope for some u, it is reasonable to assume that the tails are generalized pareto distributed and u should be chosen where this is the case. Here, for the GARCH returns, this would be around 0.025. Since this rule is nowhere true for log-normal returns, it is not reasonable to assume tails to have generalized pareto distribution. . def ml_estimator_pareto(u, x): y = x[x&gt;u]-u N_u = len(y) def log_likelihood(x): #negative log likelihood of generalized pareto distribution return N_u*np.log(x[1])+(1/x[0]+1)*np.sum(np.log(1+x[0]/x[1]*y)) #first guess x0=[1,2] #bounds bnds = [(0.000001,5),(0.000001,5)] res = scipy.optimize.minimize(log_likelihood, x0, bounds=bnds) if not res.success: raise ValueError(&#39;optimization unsuccessful&#39;) gamma = res.x[0] beta = res.x[1] return gamma, beta, N_u . def var_pot(p,gamma, beta, N_u, n, u): return u + beta/gamma *( (n/N_u*(1-p))**(-gamma) - 1 ) def es_pot(q, beta, gamma, u): return q + (beta + gamma*(q - u))/(1-gamma) . from scipy.stats import genpareto u = 0.025 n = len(garch_returns) gamma, beta, N_u = ml_estimator_pareto(u, -garch_returns) p = 0.99 print(f&#39;================= ML ESTIMATION RESULTS =============== n Gamma: {gamma} n Beta: {beta} n&#39;) fig, ax = plt.subplots(1, 1) x = np.linspace(genpareto.ppf(0.01, gamma, beta), genpareto.ppf(0.99, gamma,beta), 100) ax.plot(x, genpareto.pdf(x, gamma,beta), &#39;b-&#39;, lw=1, alpha=0.6, label=f&#39;genpareto pdf, gamma={round(gamma,3)}, beta={round(beta,3)}&#39;) ax.plot(x, genpareto.cdf(x, gamma,beta), &#39;r-&#39;, lw=1, alpha=0.6, label=f&#39;genpareto cdf, gamma={round(gamma,3)}, beta={round(beta,3)}&#39;) ax.set(title = &#39;Generalized Pareto Distribution&#39;) ax.legend() plt.show() . ================= ML ESTIMATION RESULTS =============== Gamma: 0.42139052785832587 Beta: 0.00964087399792817 . VaR = var_pot(p,gamma, beta, N_u, n, u) ES = es_pot(VaR, beta, gamma, u) print(f&#39;================= POT ESTIMATES =============== n VaR: {VaR} n ES: {ES} n&#39;) . ================= POT ESTIMATES =============== VaR: 0.03442025133801539 ES: 0.05794298875784114 . Copulas . To model dependence we use copulas. In essence, these are multivariate distribution functions that transform the margins to be uniformly distributed with support [0,1]. . By Sklar’s theorem, any multivariate joint distribution can be written in terms of univariate marginal distribution functions and a copula which describes the dependence structure between the two variables. . For example, a Gaussian copula $ C_{R}^{ text{Gauss}}(u)= Phi _{R} left( Phi ^{-1}(u_{1}), dots , Phi ^{-1}(u_{d}) right) $ is a multivariate Gaussian distribution function that transforms the marginals into uniform [0,1]. Its density is ${ displaystyle c_{R}^{ text{Gauss}}(u)={ frac {1}{ sqrt { det {R}}}} exp left(-{ frac {1}{2}}{ begin{pmatrix} Phi ^{-1}(u_{1}) vdots Phi ^{-1}(u_{d}) end{pmatrix}}^{T} cdot left(R^{-1}-I right) cdot { begin{pmatrix} Phi ^{-1}(u_{1}) vdots Phi ^{-1}(u_{d}) end{pmatrix}} right)}$. To use them we model multivariate Gaussian rvs with positive-definite covariance matrix. We then transform the marginal realizations to standard uniform rvs by applying the CDF. After that, we transform the uniform marginals into the desired distribution function by applying the inverse CDF. . multi_variate_gauss = stats.multivariate_normal(mean=[0, 0], cov=[[1., 0.7], [0.7, 1.]]) x = multi_variate_gauss.rvs(5000) x_uniform = stats.norm().cdf(x) h = sns.jointplot(x_uniform[:, 0], x_uniform[:, 1], kind=&#39;hex&#39;, stat_func=None) h.set_axis_labels(&#39;Y1&#39;, &#39;Y2&#39;); . x1_transform = stats.genpareto(c=0.25).ppf(x_uniform[:, 0]) x2_transform = stats.beta(a=10, b=2).ppf(x_uniform[:, 1]) sns.jointplot(x1_transform, x2_transform, kind=&#39;kde&#39;, xlim=(0, 5), ylim=(.5, 1.0), stat_func=None); . Compare this to the jointplot of independent rvs. . x1_transform = stats.genpareto(c=0.25).ppf(stats.uniform().rvs(5000)) x2_transform = stats.beta(a=10, b=2).ppf(stats.uniform().rvs(5000)) sns.jointplot(x1_transform, x2_transform, kind=&#39;kde&#39;, xlim=(0, 5), ylim=(.5, 1.0), stat_func=None); . Tail Dependence . The lower tail dependence is defined as $${ displaystyle lambda _{ ell }= lim _{q rightarrow 0} operatorname {P} (X_{2} leq F_{2}^{ leftarrow }(q) mid X_{1} leq F_{1}^{ leftarrow }(q)).}$$ where ${ displaystyle F^{ leftarrow }(q)= inf {x in mathbb {R} :F(x) geq q }} { displaystyle F^{ leftarrow }(q)= inf {x in mathbb {R} :F(x) geq q }}$, that is, the inverse of the cumulative probability distribution function for $q$. . The upper tail dependence is defined analogously as . $${ displaystyle lambda _{u}= lim _{q rightarrow 1} operatorname {P} (X_{2}&gt;F_{2}^{ leftarrow }(q) mid X_{1}&gt;F_{1}^{ leftarrow }(q)).}$$ . Gaussian copulas are easy to understand but lack the flexibility to model tail dependence and radial asymmetry, which is very important for risk management. For that we need to consider the class of Archimedean copulas ${ displaystyle C(u_{1}, dots ,u_{d}; theta )= psi ^{[-1]} left( psi (u_{1}; theta )+ cdots + psi (u_{d}; theta ); theta right)}$ like the Gumbel copula or the Clayton Copula. . from numpy.random import gamma def clayton_copula(n, delta, d=2): x = gamma(1/delta,1, size=n) v = stats.uniform().rvs([n,d]) u = (1-np.log(v)/x[:,None])**(-1/delta) return u n = 10000 delta = 6 u = clayton_copula(n, delta) h = sns.jointplot(u[:,0], u[:,1], kind=&#39;hex&#39;, stat_func=None) h.set_axis_labels(&#39;Y1&#39;, &#39;Y2&#39;); . x1_transform = stats.genpareto(c=0.25).ppf(u[:,0]) x2_transform = stats.beta(a=10, b=2).ppf(u[:,1]) sns.jointplot(x1_transform, x2_transform, kind=&#39;kde&#39;, xlim=(0, 5), ylim=(.5, 1.0), stat_func=None); . x = stats.uniform().rvs([n,3]) . kt = scipy.stats.kendalltau(x1_transform,x2_transform) print(&quot;Kendall&#39;s Tau: &quot;, kt) . Kendall&#39;s Tau: KendalltauResult(correlation=0.7569578157815782, pvalue=0.0) . sr = scipy.stats.spearmanr(x1_transform,x2_transform) print(&quot;Spearman&#39;s Rho: &quot;, sr) . Spearman&#39;s Rho: SpearmanrResult(correlation=0.9149789224377892, pvalue=0.0) . pc = scipy.stats.pearsonr(x1_transform,x2_transform) print(&#39;Pearson Corr: &#39;, pc) . Pearson Corr: (0.5324997734331705, 0.0) . lamda_l = 2**(-1/delta) print(&#39;Tail Dependence Coefficient: &#39;,lamda_l) . Tail Dependence Coefficient: 0.8908987181403393 .",
            "url": "https://jpwoeltjen.github.io/researchBlog/extreme%20value%20theory/risk/copulas/2019/03/25/QuantitativeRiskManagementNotes.html",
            "relUrl": "/extreme%20value%20theory/risk/copulas/2019/03/25/QuantitativeRiskManagementNotes.html",
            "date": " • Mar 25, 2019"
        }
        
    
  
    
        ,"post12": {
            "title": "Notes on Computational Finance",
            "content": "Consider a market containing two primary assets: a risk-free bond with price $B_t=e^{rt}$, a stock $S_t$ of geometric Brownian motion type $S(t)$ = $S(0) exp( mu t + sigma W(t))$, with parameters $ mu in mathbb{R}, sigma &gt; 0 $ and standard Brownian motion $W$. . A derivative or option is an asset whose payoff depends on the underlying. The payoff $X$ at time $T$ may be a function $f(S(T))$ of the underlying at time $T$ as e.g. $X = (S(T)-K)^{+}$ for a simple European call or it could be a more complex function of the whole past of $S$. In complete markets such options can be replicated perfectly. This means that there exists a self-financing portfolio $ phi = ( phi_0, phi_1)$ whose value at time $T$ equals $V_ phi(T) = X$. Absence of arbitrage implies that $V_ phi(t)$ is the only reasonable option price of the option at time $t$. It can be computed as conditional expectation . $$V_ phi(t) = B(t) mathbb{E}^ mathbb{Q} big[X/B(T) big| mathcal{F}_t big]$$ . of the discounted payoff under the unique equivalent martingale measure $ mathbb{Q}$, i.e. the unique probability measure $ mathbb{Q} sim mathbb{P}$ such that $S/B$ is a $ mathbb{Q}$-martingale. . American options are specified by an entire exercise process $X = (X(t))_{t in[0,T]}$ as e.g. $X(t) = (K-S(t))^{+}$ for an American put. In the complete case, the only reasonable price is the B-fold of the Snell envelope of $X/B$ relative to $ mathbb{Q}$. The $ mathbb{Q}$-Snell envelope is the smallest $ mathbb{Q}$-supermartingale dominating $X/B$. Again, $ mathbb{Q}$ denotes the unique equivalent martingale measure from above. One can write this fair price also as . $$V(t) = sup_{ tau} mathbb{E}^ mathbb{Q} big[X( tau)/B( tau) big| mathcal{F}_t big]$$ . where the supremum extends over all stopping times stopping between $t$ and $T$. One such stopping time is the first time $t$ such that $V (t) = X(t)$, i.e. the market price of the option equals the exercise price. . A Martingale is a sequence of random variables (i.e., a stochastic process) for which, at a particular time in the realized sequence, the expectation of the next value in the sequence is equal to the present observed value even given knowledge of all prior observed values. Supermartingale: $ mathbb{E}$ equal or less than current value. | The Snell envelope is the smallest supermartingale dominating a stochastic process. | Supremum: the smallest quantity that is greater than or equal to each of a given set or subset of quantities. | $ mathcal{F}_t$ is the sigma-field at time $t$, i.e. all the available information at time $t $ | . Binomial Trees . In the Cox-Ross-Rubinstein model (CRR) we consider equidistant times $0 = t_0,t_1,...,t_M = T$ with $t_i = i Delta t$ and hence $ Delta t = T/M.$ The bond moves according to $B(t_i) = e^{rt_i}$ . The stock goes up by a factor $u$ resp. down by a factor $d$ in each period, i.e., . $$ S(t_i) = begin{cases} S(t_{i-1})u&amp; text{with probability } p S(t_{i-1})d&amp; text{with probability } p-1 end{cases} $$, where $d &lt; e^{r Delta t} &lt; u$. More precisely, $p$ and $1-p$ denote the conditional probabilities of going up and down, repectively, given the past. Option prices in a properly chosen sequence of CRR models converge to the corresponding Black-Scholes notions if the number of time step tends to infinity. . For option pricing real-world transition probabilities do not matter. Instead we need to consider martingale probabilities, i.e. probabilities such that . $$ mathbb{E}^ mathbb{Q} big[S(t_i)/B(t_i) big| mathcal{F}_{t_{i-1}} big] = S(t_{i-1})/B(t_{i-1})$$. . If we denote the $Q$-transition probabilities by $q$ and $1-q$, respectively, the left-hand side equals $q frac{S(t_{i-1})u}{B(t_{i-1})e^{r Delta t}} + (1-q) frac{ S(t_{i-1})d}{B(t_{i-1})e^{r Delta t}} $ which equals the right-hand side iff . $$ frac{qu+(1-q)d}{e^{r Delta t}} = 1 $$ . or . $$ q = frac{e^{r Delta t}-d}{u-d}$$ . As previously mentioned a properly parameterized CRR model approximates BS as $M$ gets large. If we match the first two moments of the CRR and BS model we obtain . $$ begin{align} u = beta + sqrt{ beta^2 - 1 } d = beta - sqrt{ beta^2 - 1 } q = frac{ exp(r Delta t ) - d}{u-d} beta = frac{1}{2}( exp{(-r Delta t )} + exp{((r + sigma^2) Delta t)}) end{align}$$ European Options . Consider a European option with payoff $g(S(T))$ for some function $g$ as e.g. $g(x) = (x-K)^{+}$ (call) or $g(x) = (K-x)^{+}$ (put). We denote the fair price of the option at time $t_i$ by $V(S(t_i), t_i)$. The $Q$-martingale property for discounted option prices reads as . $$ mathbb{E}^ mathbb{Q} big[V (S(t_i), t_i)/B(t_i) big| mathcal{F}_{t_{i-1}} big] = V (S(t_{i-1}), t_{i-1})/B(t_{i-1})$$ . or . $$q frac{ V(S(t_{i-1})u,t_i)}{B(t_{i-1})e^{r Delta t}} + (1-q) frac{ V(S(t_{i-1})d,t_i)}{B(t_{i-1})e^{r Delta t}} = frac{ V(S(t_{i-1}),t_i)}{B(t_{i-1})}$$ . or . $$V (S(t_{i-1}), t_{i-1}) = e^{-r Delta t} (qV(S(t_{i-1})u, t_i) + (1-q)V(S(t_{i-1})d, t_i))$$ . We can compute option prices starting from $V (S(T ), T ) = g(S(T ))$ and moving backwards in time. . Since the model is complete, the payoff $g(S(T))$ can be replicated perfectly by a self- financing portfolio $ phi = ( phi_0, phi_1)$. Its value equals $ phi_0(t_i)B(t_i) + phi_1(t_i)S(t_i) = V (S(t_i), t_i)$. . Due to self-financeability . $$ phi_0(t_{i-1})B(t_{i-1}) + phi_1(t_{i-1})S(t_{i-1}) = phi_0(t_i)B(t_{i-1}) + phi_1(t_i)S(t_{i-1})$$ . leads to . $$ phi_0(t_i)B(t_{i-1}) + phi_1(t_i)S(t_{i-1}) = V (S(t_{i-1}), t_{i-1})$$ . $$ phi_0(t_i)B(t_{i-1}) + phi_1(t_i)S(t_{i-1}u) = V (S(t_{i-1}u), t_{i-1})$$ . $$ phi_0(t_i)B(t_{i-1}) + phi_1(t_i)S(t_{i-1}d) = V (S(t_{i-1}d), t_{i-1})$$ . Hence, . $$ phi_1(t_i) = frac{V (S(t_{i-1})u, t_i) - V (S(t_{i-1})d, t_i)}{ S(t_{i-1})(u-d)}$$ . $$ phi_0(t_i) = frac{V (S(t_{i-1}),t_{i-1})- phi_1(t_i)S(t_{i-1})}{B(t_{i-1})}$$ . American Options . American options have exercise process $g(S(t_i))$ at time $t_i$. . $$V(S(t_{i-1}), t_{i-1}) = max{ {g(S(t_{i-1})), mathbb{E}^ mathbb{Q} big[V (S(t_i), t_i)/B(t_i) big| mathcal{F}_{t_{i-1}} big] }}$$ . This leads to . $$V (S(t_{i-1}), t_{i-1}) = max{ {g(S(t_{i-1})),e^{-r Delta t} (qV(S(t_{i-1})u, t_i) + (1-q)V(S(t_{i-1})d, t_i)) }}$$ . The first optimal stopping time is . $$ tau_f = inf{ {t_i : V (S(t_i), t_i) = g(S(t_i)) }}$$ . and the last optimal stopping time is . $$ tau_s = inf{ {t_i : i = M text{ or } g(S(t_i)) &gt; e^{-r Delta t} (qV (S(t_i)u, t_{i+1}) + (1-q)V (S(t_i)d, t_{i+1})) }}$$ . import numpy as np def crr_bs_approx(S_0,r,sigma,T,M,K,EU,Type): dt=T/M #Set u,d,q such that first and second moments match BS beta = 0.5*(np.exp(-r*dt)+np.exp((r+(sigma**2))*dt)) u = beta + ((beta**2)-1)**0.5 d = 1/u#beta - ((beta**2)-1)**0.5 q = (np.exp(r*dt)-d)/(u-d) if Type == &#39;call&#39;: def g(St,K): return max(St - K, 0) elif Type == &#39;put&#39;: def g(St,K): return max(K - St, 0) else: raise Exception(&quot;Specify valid Type (&#39;put&#39;/&#39;call&#39;)&quot;) g = np.vectorize(g ,otypes=[np.float]) S = np.zeros((M+1,M+1)) S[0,0] = S_0 V = np.zeros((M+1,M+1)) for i in range(1,M+1): for j in range(i+1): S[j,i] = S_0*(u**j)*(d**(i-j)) V[:,-1] = g(S[:,-1],K) if EU == 1: for i in range(M-1,-1,-1): for j in range(i+1): V[j,i] = np.exp(-r*dt)*( q*V[j+1,i+1] + (1-q)*V[j,i+1]) elif EU == 0: for i in range(M-1,-1,-1): for j in range(i+1): V[j,i] = max(g(S[j,i],K) ,np.exp(-r*dt)*( q*V[j+1,i+1] + (1-q)*V[j,i+1])) else: raise Exception(&quot;Specify valid EU state (0/1)&quot;) return V[0,0] sigma=0.3 r=0.03 S_0=100 T=1 M=2000 K=100 EU=1 Type = &#39;call&#39; V = crr_bs_approx(S_0,r,sigma,T,M,K,EU,Type) print(f&quot;Fair value of option: {round(V,3)}&quot;) . Fair value of option: 13.282 . To check whether the approximation is any good compute BS call and compare: . from scipy.stats import norm import numpy as np def eu_call_bs(S_t,r,sigma,T,K,t): d_1 = (np.log(S_t/K)+(r+sigma**2/2)*(T-t))/(sigma*(T-t)**0.5) d_2 = d_1 - sigma*(T-t)**0.5 phi_1 = norm.cdf(d_1) cdf_d2 = norm.cdf(d_2) c = S_t * phi_1 - K*np.exp(-r*(T-t)) *cdf_d2 phi_0 = -K*np.exp(-r*T)*cdf_d2 return c, phi_0, phi_1 sigma=0.3 r=0.03 S_t=100 T=1 K=100 t = 0 c, phi_0, phi_1 = eu_call_bs(S_t,r,sigma,T,K,t) print(f&quot;The fair value of the option is {round(c,3)}, the hedging position in the the stock is {round(phi_1,3)}, the hedging position in the the bond is {round(phi_0,3)}.&quot;) . The fair value of the option is 13.283, the hedging position in the the stock is 0.599, the hedging position in the the bond is -46.587. . Monte Carlo . Once we are considering very complicated payoff structures many numerical methods relying on e.g. solving PDE’s or using integral transforms are not available anymore. In this case we can employ Monte Carlo methods, basically simulating many possible futures and taking the sample moments as an approximation of the real world population moments. For this we need a random number generator that can sample from the appropriate probability distribution. The needed law might not be implemented in the software package so we may need to implement a random number generator that takes only uniformly distributed random numbers as input. This can be done via . Inversion | Acceptance/rejection method | . import numpy as np import scipy.interpolate as interpolate from matplotlib import pyplot as plt def inverse_transform_sampling(data, n_bins=100, n_samples=1000): hist, bin_edges = np.histogram(data, bins=n_bins, density=True) cum_values = np.zeros(bin_edges.shape) cum_values[1:] = np.cumsum(hist*np.diff(bin_edges)) inv_cdf = interpolate.interp1d(cum_values, bin_edges) r = np.random.rand(n_samples) return inv_cdf(r) st_normal = np.random.normal(0,1,10000) mm_normal = np.append(st_normal, (np.random.normal(5,1,1000), np.random.normal(-10,1,1000))) c = inverse_transform_sampling(mm_normal,n_bins=100, n_samples=10000) plt.hist(mm_normal, alpha=0.5, bins=200, normed=True, color=&#39;b&#39;) plt.hist(c, alpha=0.5, bins=200, normed=True, color=&#39;r&#39;) plt.show() . import numpy as np from matplotlib import pyplot as plt from scipy.special import beta as beta # Density of the Beta distribution def Beta_density(x, alpha1, alpha2): return 1/beta(alpha1,alpha2) * (x)**(alpha1-1) * (1-x)**(alpha2-1) * (x&gt;=0) * (x&lt;=1) def Sample_Beta_AR(alpha1, alpha2, N): # Mode of the beta density x_max = (alpha1-1) / (alpha1+alpha2-2) # Constant C for the acceptance/rejection method. C is the optimal height of the rectangle in which # we through our imaginary darts. C = Beta_density(x_max, alpha1, alpha2) # Generate one sample by the acceptance/rejection method def SingleSample(): success = False while not success: U = np.random.rand(2,1) success = ( C*U[1] &lt;= Beta_density(U[0], alpha1, alpha2) ) return U[0] X = np.zeros(N) for n in range(N): X[n] = SingleSample() return X alpha1 = 2 alpha2 = 3 N = 200000 b_ar = Sample_Beta_AR (alpha1, alpha2, N) plt.hist(b_ar,alpha=0.5, bins=500, normed=False, color=&#39;b&#39;) # Compare with numpy&#39;s version of beta distribution b = np.random.beta(alpha1, alpha2, N) plt.hist(b,alpha=0.5, bins=500, normed=False, color=&#39;r&#39;) plt.show() . import numpy as np def mc_eu(S_0,K,r,sigma,T,N,g): def f(x): return np.exp(-r*T)*g(S_0*np.exp( (r-0.5*sigma**2)*T + (sigma*T**0.5*x) ), K) X = np.random.normal(0,1,N) Y = f(X) v_0 = np.mean(Y) epsilon = 1.96 * np.sqrt(np.var(Y)/N) return v_0, epsilon sigma=0.3 r=0.03 S_0=100 T=1 N=10000 K=100 g = lambda S, K: np.maximum((S - K),0) V,e = mc_eu(S_0,K,r,sigma,T,N,g) print(f&quot;Fair value of option: {round(V,3)}, n95% confidence interval: [ {round(V - e,3)} , {round(V+e,3)} ]&quot;) . Fair value of option: 13.462, 95% confidence interval: [ 13.036 , 13.889 ] . Variance reduction . The Monte Carlo estimator converges at the rate $ frac{1}{ sqrt{ N }}$ since its standard deviation is $ sigma( hat{V_N}) = frac{ sigma(f(X))}{ sqrt{N}}$. Since we don&#39;t have unlimited compute power this rate of convergence might be to slow for our purposes. We may can do better by trying to reduce $ sigma(f(X))$ through some tricks, e.g., by means of antithetic variables or control variates. Another problem arises if there are outcomes with very low probability that have a large effect on the option value, i.e., Black Swans. In this case, we may not get any samples from the part of the distribution that matter most and our estimate will be way off. To correct for that use importance sampling. This can be useful e.g. in pricing deep out of the money puts: . from scipy.stats import norm import numpy as np # Importance Sampling def mc_eu_is(S_0, K, r, sigma, T, N, g, mu): def f(x): return np.exp(-r*T-x*mu+0.5*mu**2)*g(S_0*np.exp( (r-0.5*sigma**2)*T + (sigma*T**0.5*x) ), K) X = np.random.normal(mu,1,N) Y = f(X) v_0 = np.mean(Y) # Compute radius of 95% confidence interval. epsilon = 1.96 * np.sqrt(np.var(Y)/N) return v_0, epsilon #Antithetic Variables def mc_eu_av (S_0, K, r, sigma, T, N, g): def f(x): return np.exp(-r*T) * g(S_0*np.exp((r-0.5*sigma**2)*T + sigma*np.sqrt(T)*x), K) X = np.random.normal(0,1,N) # Compute Monte-Carlo estimator using antithetic variables taking advantage of the symmetry of the gaussian. # Antithetic variables only increase the bang for the buck if random vars are negatively correlated. # Since X2 = -X1 this is clearly the case here. Y = (f(X)+f(-X)) / 2 v_0 = np.mean(Y) # Compute radius of 95% confidence interval. epsilon = 1.96 * np.sqrt(np.var(Y)/N) return v_0, epsilon #Closed Form BS def eu_put_bs(S_t, r, sigma, T, K, t): d_1 = (np.log(S_t/K)+(r+sigma**2/2)*(T-t))/(sigma*(T-t)**0.5) d_2 = d_1 - sigma*(T-t)**0.5 p = K*np.exp(-r*(T-t)) *norm.cdf(-d_2) - S_t *norm.cdf(-d_1) return p sigma=0.3 r=0.03 S_0=S_t=200 T=1 N=10000 K=100 g = lambda S, K: np.maximum((K-S),0) t = 0 mu = ( np.log(K/S_0) - (r-1/2*sigma**2)*T ) / ( sigma*np.sqrt(T) ) V,e = mc_eu_is(S_0,K,r,sigma,T,N,g,mu) print(f&quot;Fair value of OOM put with importance sampling: {round(V,3)}, 95% confidence interval: [ {round(V - e,3)} , {round(V+e,3)} ]&quot;) V,e = mc_eu_av(S_0,K,r,sigma,T,N,g) print(f&quot;Fair value of OOM put with antithetic variables: {round(V,3)}, 95% confidence interval: [ {round(V - e,3)} , {round(V+e,3)} ]&quot;) V,e = mc_eu(S_0,K,r,sigma,T,N,g) print(f&quot;Fair value of OOM put standard MC : {round(V,3)}, 95% confidence interval: [ {round(V - e,3)} , {round(V+e,3)} ]&quot;) V = eu_put_bs(S_t,r,sigma,T,K,t) print(f&quot;Fair value of OOM put BS: {round(V,3)}&quot;) . Fair value of OOM put with importance sampling: 0.109, 95% confidence interval: [ 0.106 , 0.111 ] Fair value of OOM put with antithetic variables: 0.109, 95% confidence interval: [ 0.09 , 0.127 ] Fair value of OOM put standard MC : 0.126, 95% confidence interval: [ 0.098 , 0.155 ] Fair value of OOM put BS: 0.109 . As can be seen above, the importance sampling technique estimates a much tighter confidence interval for the OOM put. The antithetic variables approach reduces the variance a bit. . Using control variables to reduce the variance of MC-estimators . Let&#39;s price a self-quanto call in the Black-Scholes model via the Monte-Carlo approach while reducing the variance with a European call as control variable. . from scipy.stats import norm import numpy as np def eu_call_bs(S_t,r,sigma,T,K,t): d_1 = (np.log(S_t/K)+(r+sigma**2/2)*(T-t))/(sigma*(T-t)**0.5) d_2 = d_1 - sigma*(T-t)**0.5 phi_1 = norm.cdf(d_1) cdf_d2 = norm.cdf(d_2) c = S_t * phi_1 - K*np.exp(-r*(T-t)) *cdf_d2 phi_0 = -K*np.exp(-r*T)*cdf_d2 return c, phi_0, phi_1 # function for computing the price of a self quanto call in the BS-model using Monte-Carlo with control variate (ST-K)^+ def EuOption_BS_MC_CV (S0, r, sigma, T, K, M): # Determine beta = Cov(ST*(ST-K)^+,(ST-K)^+) / Var((ST-K)^+) by Monte Carlo simulation. X = np.random.normal(0,1,M) ST = S0*np.exp( (r-0.5*sigma**2)*T + sigma*np.sqrt(T)*X ) VT = ST*np.maximum((ST-K),0) CT = np.maximum((ST-K),0) C0,_,_ = eu_call_bs(S0,r,sigma,T,K,t) #initial price of the call used as control variate,also used here to # help determine the empirical covariance instead of Covar = np.mean( (CT-np.mean(CT)) * (VT-np.mean(VT)) ) Covar = np.mean( (CT-C0*np.exp(r*T)) * (VT-np.mean(VT)) ) beta = Covar / np.var(CT) # Compute Monte Carlo estimator using the initial Call option price as control variate. X = np.random.normal(0,1,M) ST_hat = S0*np.exp( (r-0.5*sigma**2)*T + sigma*np.sqrt(T)*X ) Y = ST_hat*np.maximum((ST_hat-K),0)-beta*np.maximum((ST_hat-K),0) V0 = np.exp(-r*T)*np.mean(Y) + beta*C0 # Compute radius of 95% confidence interval epsilon = 1.96 * np.sqrt(np.var(Y)/M) return V0, epsilon, beta def EuOption_BS_MC (S0, r, sigma, T, K, M): X = np.random.normal(0,1,M) ST_hat = S0*np.exp( (r-0.5*sigma**2)*T + sigma*np.sqrt(T)*X ) Y = ST_hat*np.maximum((ST_hat-K),0) V0 = np.exp(-r*T)*np.mean(Y) # Compute radius of 95% confidence interval epsilon = 1.96 * np.sqrt(np.var(Y)/M) return V0, epsilon sigma=0.3 r=0.03 S0=100 T=1 K=100 t = 0 M=100000 V0, epsilon, beta = EuOption_BS_MC_CV (S0, r, sigma, T, K, M) print(f&quot;Control Variate n Option Price: {V0} n Epsilon: {epsilon} n Beta: {beta}&quot;) print(&quot; n&quot;) V0, epsilon= EuOption_BS_MC (S0, r, sigma, T, K, M) print(f&quot;Standard MC n Option Price: {V0} n Epsilon: {epsilon}&quot;) . Control Variate Option Price: 1999.7236047923905 Epsilon: 5.604362549148449 Beta: 179.43568394156813 Standard MC Option Price: 2015.4528087679719 Epsilon: 25.915084371493748 . Finite difference method . In addition to the fair value of the option we need to know how to hedge it. Hedging strategies often involve the derivative of the option with respect to variables such as the stock price. The finite difference approach gives us an approximation of the derivative. This works by simply nudging the variable $+h/2$ and $-h/2$ and observing how the function changes relative to $h$. More formally, for a sufficiently smooth function $z$ we have the Taylor expansion . $$ z( theta + h/2) = z( theta) + z&#39; ( theta) h/2 + z&#39;&#39; ( theta) h^2/8 + z&#39;&#39;&#39; ( theta) h^3/48 + O(h^4)$$ . and . $$ z( theta - h/2) = z( theta) - z&#39; ( theta) h/2 + z&#39;&#39; ( theta) h^2/8 - z&#39;&#39;&#39; ( theta) h^3/48 + O(h^4)$$ . where $O(h^4)$ stands for an expression such that $O(h^4)/h^4$ is bounded in a neighbourhood of $h = 0$. . This yields . $$z&#39;( theta) = frac{z( theta+h/2)-z( theta-h/2)}{h} + O(h^2)$$ . $$ = mathbb{E} big[ frac{Z( theta+h/2)-Z( theta-h/2)}{h} big] + O(h^2)$$ . The quantities $ mathbb{E} big[ Z( theta + h/2) big]$ and $ mathbb{E} big[ Z( theta - h/2) big]$ can be computed by Monte Carlo simulation as usual. Choosing a small $h$ reduces the bias due to the $O(h^2)$ term. . import numpy as np def mc_eu(S_0,K,r,sigma,T,N,g): x = np.random.normal(0,1,N) y = np.exp(-r*T)*g((S_0)*np.exp( (r-0.5*sigma**2)*T + (sigma*T**0.5*x) ), K) a = np.exp(-r*T)*g((S_0+h/2)*np.exp( (r-0.5*sigma**2)*T + (sigma*T**0.5*x) ), K) b = np.exp(-r*T)*g((S_0-h/2)*np.exp( (r-0.5*sigma**2)*T + (sigma*T**0.5*x) ), K) d = (a-b)/h delta = np.mean(d) V_0 = np.mean(y) return V_0, delta sigma=0.3 r=0.03 S_0=100 T=1 N=10**7 K=100 g = lambda S, K: np.maximum((S - K),0) h = 1 V_0, delta = mc_eu(S_0,K,r,sigma,T,N,g) print(f&quot;Option Value: {round(V_0,3)} n Hedging Position Stock {round(delta,3)} n Hedging Position Bond {round((V_0-delta*S_0),3)}&quot;) . Option Value: 13.285 Hedging Position Stock 0.599 Hedging Position Bond -46.576 . import numpy as np from scipy.misc import derivative def EuCallHedge_BS_MC_IP (St, r, sigma, g, T, t, N, K): X = np.random.normal(0,1,N) g_del = np.zeros(X.shape) # Compute derivative of g(St*exp( (r-0.5*sigma^2)*(T-t) + sigma*sqrt(T-t)*X)) w.r.t to St # This equeals g&#39;(St*exp( (r-0.5*sigma^2)*(T-t) + sigma*sqrt(T-t)*X ))* exp( (r-0.5*sigma^2)*(T-t) + # sigma*sqrt(T-t)*X by chain rule) for i in range(N): g_del[i] = derivative(g, St*np.exp( (r-0.5*sigma**2)*(T-t) + sigma*np.sqrt(T-t)*X[i]),args=(K,) ) Y = g_del * np.exp( -0.5*sigma**2*(T-t) + sigma*np.sqrt(T-t)*X) delta = np.mean(Y) # Compute 95% confidence interval epsilon = 1.96 * np.sqrt(np.var(Y)/N) c1=delta-epsilon c2=delta+epsilon return delta, c1, c2 sigma=0.3 r=0.03 S_t=100 t=0 T=1 N=10**5 K=100 g = lambda S, K: np.maximum((S - K),0) delta, c1, c2 = EuCallHedge_BS_MC_IP (St, r, sigma, g, T, t, N, K) print(f&quot;Delta via Infinitesimal Perturbation n Delta: {delta} n 95% Confidence Interval: [{c1},{c2}]&quot;) . Delta via Infinitesimal Perturbation Delta: 0.5999187977035505 95% Confidence Interval: [0.5959336800017455,0.6039039154053555] . Simulation of stochastic integrals . In some applications we need to simulate the whole path $(X (t))_{t in[0,T ]}$ of a stochastic process rather than just its terminal value $X(T)$. This happens e.g. if we want to compute prices of path-dependent claims as e.g. lookback, barrier, or Asian options. But even if we are interested only in$ X (T )$, it may be necessary to simulate the past as well because we do not know the law of $ X(T)$ in closed form. In this section we focus on diffusion-type processes, i.e., solution to stochastic differential equations of the form $$dX (t) = a(X (t), t)dt + b(X (t), t)dW (t)$$ . with some Wiener process $W$, some starting value $X(0)$, and given deterministic functions $a$, $b$. It is obviously impossible to simulate infinitely many numbers on a real computer. The goal is therefore to generate random paths of $X$ on some equidistant time grid $0 = t_0 &lt; t_1 &lt; ··· &lt; t_m = T$ with $t_i = i Delta t$ and $ Delta t = T/m$, i.e., $X(t_0),...,X(t_m)$. . Euler method . If $a$ and $b$ are sufficiently smooth and the grid is sufficiently dense $$X(t_i) approx X(t_{i-1})+ int_{t_{i-1}}^{t_i} a(X(t_{i-1}),t_{i-1})dt + int_{t_{i-1}}^{t_i} b(X(t_{i-1}),t_{i-1})dW(t)$$ . $$= X(t_{i-1}) + a(X(t_{i-1}), t_{i-1}) Delta t + b(X(t_{i-1}), t_{i-1}) Delta W_i$$ . with $W_i = W_{t_{i}} - W_{t_{i-1}}$ is a good approximation for . $$X(t_i) = X(t_{i-1})+ int_{t_{i-1}}^{t_i} a(X(t),t)dt + int_{t_{i-1}}^{t_i} b(X(t),t)dW(t)$$ . Let&#39;s simulate geometric Brownian motion where $a(X(t),t) = mu * X(t) $ and $b(X(t),t) = sigma * X(t) $ . import numpy as np from matplotlib import pyplot as plt import seaborn def geom_brownian_motion(m,T,a,b,X_0): dt = T/m r = np.random.normal(0,1,m) dW = r * np.sqrt(dt) X = np.zeros(m) t = np.zeros(m) t[0] = 0 X[0] = X_0 for i in range(1,m): t[i] = t[i-1]+dt X[i] = X[i-1] + a(X[i-1],t[i-1])*dt + b(X[i-1],t[i-1])*dW[i] return X, t m=1000 T=1 mu = 0.1 sigma = 0.3 a = lambda X, t: mu*X b = lambda X, t:sigma*X X_0 = 1 # Let&#39;s simulate n alternative future histories. n = 100 X = np.zeros((n,m)) for i in range(n): X[i],t = geom_brownian_motion(m,T,a,b,X_0) plt.plot(t,X[i],alpha=0.5) plt.title(&quot;Simulated geometric Brownian motion paths&quot;) plt.xlabel(&quot;time&quot;) plt.ylabel(&quot;Price&quot;) plt.show() X_mean = np.mean(X[:,-1]) print(f&quot;The mean stock price at time T is {X_mean}&quot;) . The mean stock price at time T is 1.1030173035417743 . And let us show by simulation that this is equivalent to $X(t) = X(0) exp(( mu - sigma^2/2)t + sigma W (t))$ with $t = T$ . . X = np.zeros(n) r = np.random.normal(0,1,m) X = X_0 *np.exp((mu-sigma**2/2)*T + sigma*r*T**0.5) X_mean = np.mean(X) print(f&quot;The mean stock price at time T is {X_mean}&quot;) . The mean stock price at time T is 1.1003810409996253 . MC Heston model . The Heston model extends the Black-Scholes model in that the variance of returns now is variable. To use MC methods we have to simulate the whole sample paths. Let&#39;s first simulate paths using the Euler method and then use straight forward MC methods to prive the option. . import numpy as np def SimPaths_Ito_Euler(X0 ,a ,b ,T ,m, N): Delta_t = T/(m-1) Delta_W = np.random.normal(0, np.sqrt(Delta_t),(N, m)) X = np.zeros(Delta_W.shape) X[:,0] = X0 * np.ones(N) for i in range(m-1): X[:,i+1]=X[:,i]+a(X[:,i],i*Delta_t)*Delta_t+b(X[:,i],i*Delta_t)*Delta_W[:,i] return X def Heston_EuOption_MC(S0, r, gamma, T, g, K): N, m = gamma.shape[0], gamma.shape[1] Delta_t = T/(m-1) X = np.random.normal(0,1,N) drift = np.dot((np.ones((N, m))*r - gamma * 0.5) , np.ones(m)) * Delta_t std = np.sqrt( np.dot(gamma , np.ones(m)) * Delta_t) ST = S0*np.exp(drift + std * X) V0 = np.exp(-r*T) * np.mean(g(ST,K)) return V0 K = 100 N=100000 m=100 S0 = 100 r = 0.03 nu0 = 0.3**2 kappa = 0.3**2 Lambda = 2.5 sigma_tilde = 0.2 T = 1 g = lambda S, K: np.maximum((S - K),0) # Function for the drift of the variance process in the heston model a = lambda x, t: kappa-Lambda*x # Function for the standard deviation of the variance process in the heston model b = lambda x, t: np.sqrt(x)*sigma_tilde gamma = SimPaths_Ito_Euler(nu0 ,a ,b ,T ,m, N) V0 = Heston_EuOption_MC(S0, r, gamma, T, g, K) print(f&quot;Option Value: {V0}&quot;) . Option Value: 10.851623171194033 . import numpy as np def UpOutPut_BS_MC_Richardson (S0, r, sigma, T, K, B, M, m): # Time step on the fine grid. delta_t = T/(2*m) S_fine = S0 * np.ones(M) S_coarse = S0 * np.ones(M) no_barrier_hit_fine = np.ones(M) no_barrier_hit_coarse = np.ones(M) # Loop over points on coarse grid. for k in range(m): # Simulate two increments of Brownian motion on the fine grid. delta_W_1 = np.random.normal(0,np.sqrt(delta_t),M) delta_W_2 = np.random.normal(0,np.sqrt(delta_t),M) # 1st Euler step on fine grid. S_fine = S_fine + r*S_fine*delta_t + sigma*S_fine*delta_W_1 no_barrier_hit_fine = no_barrier_hit_fine * (S_fine&lt;B) # 2nd Euler step on fine grid. S_fine = S_fine + r*S_fine*delta_t + sigma*S_fine*delta_W_2 no_barrier_hit_fine = no_barrier_hit_fine * (S_fine&lt;B) # Euler step on coarse grid. S_coarse = S_coarse + r*S_coarse*2*delta_t + sigma*S_coarse*(delta_W_1+delta_W_2) no_barrier_hit_coarse = no_barrier_hit_coarse * (S_coarse&lt;B) # Compute (discounted) payoffs for paths on fine and coarse grids. VT_fine = no_barrier_hit_fine * (np.exp(-r*T)*np.maximum( K - S_fine , 0)) VT_coarse = no_barrier_hit_coarse * (np.exp(-r*T)*np.maximum( K - S_coarse , 0)) # Compute Monte Carlo estimate. VT = 2*VT_fine - VT_coarse V0 = np.mean(VT) # Compute radius of 95% confidence interval epsilon = 1.96 * np.sqrt(np.var(VT)/M) return V0, epsilon S0 = 100 r = 0.05 sigma = 0.2 T = 1 K = 100 B = 110 M = 100000 m = 250 V0, epsilon = UpOutPut_BS_MC_Richardson (S0, r, sigma, T, K, B, M, m) print(f&quot;Option Value: {V0} n Radius of 95% Confidence Interval: {epsilon}&quot;) . Option Value: 4.28812361180373 Radius of 95% Confidence Interval: 0.05227428329961295 . Ito calculus . If we move from dicrete time to continuous time, we need a proper framework for working with continuous stochastic processes. We are working in a probability space ($ Omega$, $ mathcal{F}$ , $P$ ) consisting of the set $ Omega$ of possible outcomes, a $ sigma$-field $ mathcal{F}$ on $ Omega$ and a probability measure $P$ on $ mathcal{F}$. Standard calculus works for functions with finite variation, continuous stochastic functions, however, can have infinite variation, i.e., they can&#39;t be approximated by a smooth function no matter how local one gets. Many adapted stochastic proccesses can be decomposed into $X = X(0)+M+A$ with a martingale $M$ and a predictable process of finite variation $A$ according to Doop-Meyer. Our primary need from stochastic calculus is the stochastic intragal which represents financial gains. If $H$ is piecewise constant $H(t) = sum_{i=1}^{n}{V_i mathcal{1}_{[t_{i-1},t_i)}(t)}$ then the integral $$H bullet X := int H(t)dX(t)$$ is defined as . $$H bullet X(t):= int_{0}^{t} H(s)dX(s):= sum_{i=1}^n V_{i}(X(t_i)-X(t_{i-1}))$$ If $H(t$) stands for the number of shares in the portfolio at time $t$, $d$ for the assets, and $X(t)$ denotes the stock price at time $t$, then simple bookkeeping yields that $ int_0^t H(s)dX(s)$ stands for the financial gains of the portfolio from time 0 to $t$. . This can also be written as . $$dY (s) = H(s)dX(s)$$ . The integration by parts rule $$d(X(t)Y (t)) = X(t-)dY (t) + Y (t-)dX(t) + d[X, Y ](t)$$ turns out to be useful, where $[X, Y ](t)$ denotes the covariation process, which is the limit of expressions . $$ sum_{k=1}^n big(X( frac{k}{n} t)-X( frac{k-1}{n} t) big) big(Y( frac{k}{n} t)-(Y( frac{k-1}{n} t) big)$$ . if we let $n$ tend to infinity. In other words, $[X, Y ]$ sums up products of increments of $ X$ and $ Y$ over infinitesimal time intervals. For $X = Y$ it is called quadratic variation. This can also be written as $dX(t)dY (t) = d[X, Y ](t)$. It vanishes if both either $X$ or $Y$ is continuous and either of them is of finite variation. Moreover, $[X, Y ]$ is itself of finite variation. . The most import formula from stochastic calculus is Ito&#39;s formula $$f(X(t)) =f(X(0))+ int_0^t f&#39;(X(s))dX(s)+ frac{1}{2} int_0^t f&#39;&#39;(X(s))d[X,X](s)$$ . or in differential notation . $$df(X(t)) = f&#39;(X(t))dX(t) + frac{1}{2} f&#39;&#39;(X)d[X, X](t)$$ . One reoccuring proccess that involves the yet unknown proccess on both sides is the stochastic exponential . $$dZ(t) = Z(t)dX(t) text{ , } Z(0) = 1$$ . It has the unique solution . $$ mathcal{E}(X)(t)= exp big(X(t)-X(0)-2[X,X](t) big)$$ . The stochastic exponential of an Ito process is . $$ mathcal{E}(X)(t)= exp bigg( int_0^t big( mu(s) - 0.5 sigma^2(s) big) ds + int_0^t sigma(s) dW(s) bigg)$$ . Brownian motion can be written as $X(t) = mu t + sigma W(t)$ with drift rate $ mu$ , diffusion coefficient $ sigma$, and Wiener proccess $W$. The quadratic variation of Brownian motion equals $[X, X](t) = sigma^2t$ and in particular $[W, W](t) = t$ for a Wiener process $W$ . . $dX(t) = mu X(t)dt + sigma X(t)dW(t)$, $X(0) = x$ can be rephrased as $dX(t) = X(t)dY (t)$, $X(0) = x$ with $Y (t) = mu t + sigma W (t)$. By the stochastic exponential $X(t)=x mathcal{E} (Y)(t)=x exp big(( mu-0.5 sigma^2) t + sigma W(t) big)$. This process $X$ is called geometric Brownian motion. . Black-Scholes model . We are in a complete market with a bond $$B(t) = e^{rt}$$ and a stock $$S(t) = S(0) exp big( mu t + sigma W(t) big)$$ . In order to compute option prices we need the law of the stock price under the unique EMM $ mathbb{Q}$. The discounted price process $ hat{S}(t) = S(0) exp(( mu - r)t + sigma W (t))$ solves the stochastic differential equation $d hat S(t)= hat S(t) big( ( mu - r + 0.5 sigma^2) dt + sigma dW(t) big)$ because of the stochastic exponential. . Now consider the measure $ mathbb{Q} sim mathbb{P}$ with density process . $$dZ(t) = -Z(t) frac{ mu - r + 0.5 sigma^2}{ sigma}dW(t)$$ . with $$Z(0) = 1 $$ . Girsanov&#8217;s theorem . If the density process $Z$ of $ mathbb{Q} sim mathbb{P}$ reads as $dZ(t) = Z(t) sigma (t)dW(t)$ the Wiener process $W$ can be written as $dW (t) = dW^ mathcal{Q}(t) + sigma(t)dt$ . Hence, $$dW(t) = dW^ mathcal{Q}(t) - frac{ mu - r + 0.5 sigma^2}{ sigma}dt $$ with some $ mathcal{Q}$-Wiener process $W^ mathcal{Q}$ . . This yields $ d hat{S}(t) = hat{S}(t) sigma d W^ mathcal{Q}(t)$, which implies that $ hat S$ is a $ mathcal{Q}$-martingale because the drift part in the Ito process decomposition relative to $ mathcal{Q}$ vanishes. Hence we have found the unique EMM $ mathcal{Q}$. . Using $S = hat S B $ and the stochastic exponential we obtain $$ S(t)=S(0) exp big( (r - 0.5 sigma^2) t + sigma W^ mathcal{Q}(t) big)$$ . . The option value is of an European option with payoff $X = f(S(T))$ is $V (0) = B(0) mathbb{E}^ mathbb{Q} big[(f(S(T))/B(T)) big] $ . . $W^ mathcal{Q}(T)$ has the same law as $ sqrt{T}Y$ for a standard normal random variable $Y$. . Because $ mathbb{E}[g(Y)] = frac{1}{ sqrt{2 pi}} int_{- infty}^{+ infty} g(x) exp(-0.5x^2) dx$ . for any function g . $$V(0) = frac{1}{ sqrt{2 pi}} int_{- infty}^{+ infty} exp(-{rT}) f(S(0) exp big( (r - 0.5 sigma^2) T + sigma sqrt{T} x big) ) exp(-0.5x^2) dx$$ . The option value at time $t$ $$V (t) =B(t) mathbb{E}^ mathbb{Q} big[f(S(T))/B(T) big| mathcal{F}_t big]$$ . can be computed by . $$V(t) = frac{1}{ sqrt{2 pi}} int_{- infty}^{+ infty} exp(-{r(T-t)}) f(S(t) exp big( (r - 0.5 sigma^2) (T-t) + sigma sqrt{T-t} x big) ) exp(-0.5x^2) dx$$ . if we use the representation $S(T)=S(t) exp big((r - 2 (T - t)+ sigma (W^ mathbb{Q} (T) - W^ mathbb{Q} (t)) big)$ and observe that $W^ mathbb{Q} (T) - W^ mathbb{Q}(t)$ has the same law as $ sqrt{T-t}Y$ for a standard normal random variable $Y$. . PDE approach . Instead of pricing by integration one could also solve the PDE to get the option value. . With $ hat V(t) = hat v(t, S(t))$ and $dS(t) = S(t)rdt + S(t) sigma W^ mathbb{Q}(t)$ Ito&#39;s formula yields . $$ hat{dV(t)} = hat{dv(t, S(t))}$$ $$= big( partial_1 hat{v(t, S(t))} + partial_2 hat{v(t, S(t))}rS(t) + 0.5 partial_{22} hat{v(t, S(t))} sigma^2 S (t)^2 big)dt + partial_2 hat{v(t, S(t))} sigma S(t) dW^ mathbb{Q}(t) $$ . Since $ hat V$ is a $ mathbb{Q}$-martingale, it drift part has to vanish regardless of $t$ and the present value of $S(t)$. This is only possible if $ hat v$ satisfies the partial differential equation (PDE) . $$ partial_1 hat{v(t, x)} + partial_2 hat{v(t,x)}rx + 0.5 partial_{22} hat{v(t, x)} sigma^2 x^2 = 0 $$ . or $$ frac{ partial}{ partial t} hat{v(t, x)} = -rx frac{ partial}{ partial x } hat{v(t,x)} - 0.5 sigma^2 x^2 frac{ partial}{ partial x^2} hat{v(t, x)} $$ . Since $v(t,x) = hat{v(t,x)}e^{rt}$, this leads to the Black-Scholes PDE . $$ frac{ partial}{ partial t} {v(t, x)} = rv(t,x)-rx frac{ partial}{ partial x } {v(t,x)} - 0.5 sigma^2 x^2 frac{ partial}{ partial x^2} {v(t, x)} $$ . This is complemented by the final value $v(T,x) = f(x)$. . Solving this system of equations gives the fair value of an European option. . The option can be replicated by a self-financing strategy $ phi = ( phi_0, phi_1)$ that satisfies $d hat V phi(t) = phi_1(t)d hat S(t) = phi_1(t) hat S (t) sigma dW^ mathbb{Q}(t)$. . Comparing this with the result from Ito&#39;s formula yields $$ phi_1(t) hat S(t) sigma = partial_2 hat v(t, S(t)) sigma S(t)$$ or . $$ phi_1(t) =S(t)/ hat S(t) partial_2 hat v(t, S(t)) $$ $$ = e^{rt} partial_2 hat v(t, S(t)) $$ $$ = partial_2 v(t, S(t)) $$ . the partial derivative of the option price relative to the stock price yields the number of shares of stock in the replicating portfolio. This quantity is called Delta. . The remaining funds are invested in the bond. . $$ phi_0(t) = frac{ v(t, S(t)) - phi_1 S(t)}{B(t)} $$ . from scipy.stats import norm import numpy as np from matplotlib import pyplot as plt import seaborn def eu_call_bs(S_t,r,sigma,T,K,t): d_1 = (np.log(S_t/K)+(r+sigma**2/2)*(T-t))/(sigma*(T-t)**0.5) d_2 = d_1 - sigma*(T-t)**0.5 phi_1 = norm.cdf(d_1) cdf_d2 = norm.cdf(d_2) c = S_t * phi_1 - K*np.exp(-r*(T-t)) *cdf_d2 phi_0 = -K*np.exp(-r*T)*cdf_d2 return c, phi_0, phi_1 def BS_EuCall_FiDi_Explicit (r, sigma, a, b, m, nu_max, T, K): w = np.zeros(m + 1) # Time and space discretization step sizes delta_t = 0.5*sigma**2*T / nu_max delta_x = (b-a) / m # Check for stability of explicit scheme Lambda = delta_t / delta_x**2 if Lambda &gt;= 1/2: raise ValueError(&quot;Finite difference scheme unstable.&quot;) # Determine grid in time and space t_tilde = np.array(range(nu_max+1)) * delta_t x_tilde = a + np.array(range(m+1)) * delta_x # Compute auxiliary variables q = 2*r/sigma**2 q_minus = 0.5 * (q-1) q_plus = 0.5 * (q+1) # Boundary condition for t_tilde = 0, corresponding to payoff # of the option at maturity. w = np.maximum( np.exp(q_plus*x_tilde)-np.exp(q_minus*x_tilde), 0 ) # Boundary condition for x = b. Stays the same for all time points w[0] = 0 # Iterate through time layers. for nu in range(nu_max): w[1:-1] = Lambda*(w[0:-2]) + (1-2*Lambda)*w[1:-1] + Lambda*(w[2:]) # Boundary condition for x = b. w[-1] = np.exp(q_plus*b + q_plus**2*t_tilde[nu+1])-np.exp( q_minus*b + q_minus**2*t_tilde[nu+1] ) # retransformation of heat equation V0 =( K * w * np.exp(-q_minus*x_tilde - 0.5* sigma**2*T*(q_minus**2 + q))) # vector of initial stock prices S = K*np.exp(x_tilde) return V0, S r = 0.05 sigma = 0.2 a = -0.7 b = 0.4 m = 100 nu_max = 2000 T = 1 K = 100 t=0 V0,S = BS_EuCall_FiDi_Explicit (r, sigma, a, b, m, nu_max, T, K) plt.plot(S,V0,label=&#39;FiDi - Explicit&#39;) plt.title(&quot;European Call vs. Stock Price - Explicit Finite Differences Scheme&quot;) plt.xlabel(&quot;Stock Price&quot;) plt.ylabel(&quot;Call Price&quot;) C, _,_ = eu_call_bs(S,r,sigma,T,K,t) plt.plot(S,C,&#39;.&#39;,label=&#39;BS closed form&#39;) plt.legend() plt.show() . from scipy.stats import norm import numpy as np from matplotlib import pyplot as plt import seaborn def eu_call_bs(S_t,r,sigma,T,K,t): d_1 = (np.log(S_t/K)+(r+sigma**2/2)*(T-t))/(sigma*(T-t)**0.5) d_2 = d_1 - sigma*(T-t)**0.5 phi_1 = norm.cdf(d_1) cdf_d2 = norm.cdf(d_2) c = S_t * phi_1 - K*np.exp(-r*(T-t)) *cdf_d2 phi_0 = -K*np.exp(-r*T)*cdf_d2 return c, phi_0, phi_1 def BS_EuCall_FiDi_CrankNicholson(r, sigma, a, b, m, nu_max, T, K): w = np.zeros(m + 1) # Time and space discretization step sizes delta_t = 0.5*sigma**2*T / nu_max delta_x = (b-a) / m # Determine grid in time and space t_tilde = np.array(range(nu_max+1)) * delta_t x_tilde = a + np.array(range(m+1)) * delta_x # Compute auxiliary variables q = 2*r/sigma**2 qminus = 0.5 * (q-1) qplus = 0.5 * (q+1) Lambda = delta_t / delta_x**2 # tridiagonal matrix for the implicit scheme A_alpha = (1+Lambda)*np.ones(m-1) A_beta = -0.5*Lambda*np.ones(m-2) # defining the tridiagonal matrix A in the linear equation system Ax=b A = np.diag(np.ones(m-1)*(1+Lambda),0) + np.diag(np.ones(m-2)*-0.5*Lambda,-1) + np.diag(np.ones(m-2)*-0.5*Lambda,1) # Boundary condition for t_tilde = 0, corresponding to payoff # of the call option at maturity. w = np.maximum( np.exp(qplus*x_tilde) - np.exp(qminus*x_tilde), 0 ) # Boundary condition for x = a. w[0] = 0 # Boundary condition for x = b. w[-1] = np.exp( qplus*b + qplus**2*t_tilde[0] ) - np.exp(qminus*b + qminus**2*t_tilde[0]) # Iterate through time layers. for nu in range(nu_max): # Explicit part of Crank-Nicholson w[1:-1] = 0.5*Lambda*w[:-2] + (1-Lambda)*w[1:-1] + 0.5*Lambda*w[2:] # Boundary condition for x = b. w[-1] = np.exp( qplus*b + qplus**2*t_tilde[nu+1] ) - np.exp(qminus*b + qminus**2*t_tilde[nu+1]) # Modification for w_nu,m and implicit part of Crank-Nicholson w[-2] = w[-2] + 0.5*Lambda*w[-1] # Solving the linear equation system Ax=b for x w[1:-1] = np.linalg.solve(A, w[1:-1].transpose()) # retransformation of heat equation V0 =( K * w * np.exp(-qminus*x_tilde - 0.5* sigma**2*T*(qminus**2 + q))) # vector of initial stock prices S = K*np.exp(x_tilde) #phi_1(0) = derivative of V(0,S(0)) w.r.t. S(0), can be approximated by (V(0,S(0)+h)-V(0,S(0)))/h #here h = difference between two initial stock price values (usually h should be small, but it works okay ) phi1 = np.diff(V0) / np.diff(S) return V0, S, phi1 r = 0.05 sigma = 0.2 a = -0.7 b = 0.4 m = 100 nu_max = 2000 T = 1 K = 100 V0, S, phi1 = BS_EuCall_FiDi_CrankNicholson(r, sigma, a, b, m, nu_max, T, K) plt.plot(S[1:],phi1,label=&#39;FiDi - Crank-Nicholson&#39;) plt.title(&quot;Delta vs. Stock Price - Crank-Nicholson Scheme&quot;) plt.xlabel(&quot;Stock Price&quot;) plt.ylabel(&quot;Delta&quot;) _, _,phi1_cf= eu_call_bs(S,r,sigma,T,K,0) plt.plot(S[1:],phi1_cf[:-1],&#39;.&#39;,label=&#39;BS closed form&#39;) plt.legend() plt.show() . Valueing American puts by solving the Black Scholes PDE using finite differences . from scipy.stats import norm import numpy as np from matplotlib import pyplot as plt import seaborn def brennon_schwartz(A_alpha, A_beta, A_gamma, b_solve, g_discr): n = len(A_alpha) x = np.zeros(A_alpha.shape) alpha_hat = np.zeros(A_alpha.shape) b_hat = np.zeros(b_solve.shape) # initial values of alpha_hat and b_hat alpha_hat[-1] = A_alpha[-1] b_hat[-1]= b_solve[-1] # backward recursion for values of alpha_hat and b_hat for i in range(n-2, -1, -1): alpha_hat[i] = A_alpha[i] - A_beta[i]*A_gamma[i+1]/alpha_hat[i+1] b_hat[i] = b_solve[i] - A_beta[i]*b_hat[i+1]/alpha_hat[i+1] # first value of output x x[0] = max(b_hat[0]/alpha_hat[0], g_discr[0]) # forward recursion for values of output x for i in range(1,n): x[i] = np.maximum((b_hat[i]-A_gamma[i]*x[i-1])/alpha_hat[i],g_discr[i]) return x def BS_AmPut_FiDi_CN (r, sigma, a, b, m, nu_max, T, K): w = np.zeros(m + 1) # Time and space discretization step sizes delta_t = 0.5*sigma**2*T / nu_max delta_x = (b-a) / m # Determine grid in time and space. t_tilde = np.array(range(nu_max+1)) * delta_t x_tilde = a + np.array(range(m+1)) * delta_x # Compute auxiliary variables. q = 2*r/sigma**2 qminus = 0.5 * (q-1) qplus = 0.5 * (q+1) Lambda = delta_t / delta_x**2 # Function for the transformed payoff of the american put def g(t_tilde, x_tilde): return np.exp(qplus**2*t_tilde)*np.maximum(np.exp(x_tilde*qminus)-np.exp(x_tilde*qplus),0) # tridiagonal matrix for the implicit scheme, used in the Brennon-Schwartz algorithm A_alpha = (1+Lambda)*np.ones(m-1) A_beta = -0.5*Lambda*np.ones(m-1) A_beta[-1]= 0 A_gamma = -0.5*Lambda*np.ones(m-1) A_gamma[0] = 0 # Boundary condition for t_tilde = 0, corresponding to payoff of the call option at maturity. w = g(t_tilde[0], x_tilde) b_solve = np.zeros(m-1) # Iterate through time layers. for nu in range(nu_max): g_discr = g(t_tilde[nu],x_tilde) # Boundary condition for x = b. w[-1] = g_discr[-1] # boundary condition for x = a. w[0] = g_discr[0] # Creating the vector b b_solve[1:-1] = w[2:-2] + 0.5*Lambda*(w[1:-3] - 2*w[2:-2] + w[3:-1]) # First and last value for b b_solve[0] = w[1]+ 0.5*Lambda*(w[2] - 2*w[1] + g_discr[0] + g(t_tilde[nu+1],a)) b_solve[-1] = w[-2] + 0.5*Lambda*(g_discr[-1] - 2*w[-2] + w[-3] + g(t_tilde[nu+1],b)) # Using the Brennon Schwartz algorithm w[1:-1] = brennon_schwartz(A_alpha, A_beta, A_gamma, b_solve, g_discr[1:-1]) # retransformation of heat equation V0 =( K * w * np.exp(-qminus*x_tilde - 0.5* sigma**2*T*(qminus**2 + q))) # vector of initial stock prices S = K*np.exp(x_tilde) return V0, S def crr_bs_approx(S_0,r,sigma,T,M,K,EU,Type): dt=T/M #Set u,d,q such that first and second moments match BS beta = 0.5*(np.exp(-r*dt)+np.exp((r+(sigma**2))*dt)) u = beta + ((beta**2)-1)**0.5 d = 1/u#beta - ((beta**2)-1)**0.5 q = (np.exp(r*dt)-d)/(u-d) if Type == &#39;call&#39;: def g(St,K): return max(St - K, 0) elif Type == &#39;put&#39;: def g(St,K): return max(K - St, 0) else: raise Exception(&quot;Specify valid Type (&#39;put&#39;/&#39;call&#39;)&quot;) g = np.vectorize(g ,otypes=[np.float]) S = np.zeros((M+1,M+1)) S[0,0] = S_0 V = np.zeros((M+1,M+1)) for i in range(1,M+1): for j in range(i+1): S[j,i] = S_0*(u**j)*(d**(i-j)) V[:,-1] = g(S[:,-1],K) if EU == 1: for i in range(M-1,-1,-1): for j in range(i+1): V[j,i] = np.exp(-r*dt)*( q*V[j+1,i+1] + (1-q)*V[j,i+1]) elif EU == 0: for i in range(M-1,-1,-1): for j in range(i+1): V[j,i] = max(g(S[j,i],K) ,np.exp(-r*dt)*( q*V[j+1,i+1] + (1-q)*V[j,i+1])) else: raise Exception(&quot;Specify valid EU state (0/1)&quot;) return V[0,0] r = 0.05 sigma = 0.2 a = -0.7 b = 0.4 m = 100 nu_max = 2000 T = 1 K = 100 V0,S = BS_AmPut_FiDi_CN (r, sigma, a, b, m, nu_max, T, K) plt.plot(S,V0,label=&#39;FiDi&#39;) plt.title(&quot;Put vs. Stock Price&quot;) plt.xlabel(&quot;Stock Price&quot;) plt.ylabel(&quot;Put Value&quot;) V0_crr = np.zeros(S.shape) for i in range(len(S)): V0_crr[i] = crr_bs_approx(S[i],r,sigma,T,200,K,0,&#39;put&#39;) plt.plot(S,V0_crr,&#39;.&#39;, label=&#39;CRR&#39;) plt.legend() plt.show() . Hedging Error . Since the option price depends on $ sigma$, which is not known and must be estimated, an interesting question to ask is what happens if one puts in a &#39;wrong&#39; $ sigma^*$. . Consider the delta hedged portfolio . $$ Pi_t = V_t - Delta_t S_t - frac{(V_t - Delta_t S_t)}{B_t} B_t $$ . $$d Pi_t = dV_t - Delta_t dS_t - (V_t - Delta_t S_t) r dt $$ $ tag{1}$ . By Ito&#39;s lemma . $$ begin{align} dV_t &amp;= frac{ partial V}{ partial t} dt + frac{ partial V}{ partial t} dS_t + frac{1}{2} frac{ partial^2 V}{ partial S^2} d[S_t,S_t] &amp;= theta_t dt + Delta_t dS_t + frac{1}{2} Gamma_t d[S_t,S_t] end{align}$$ $ tag{2}$ . Plugging (2) into (1) yields (3) . $$ d Pi_t = ( theta_t + rS_t Delta_t - rV_t)dt - frac{1}{2} Gamma d[S_t,S_t] $$ $ tag{3}$ . In the Black-Scholes model . $$ begin{align} theta_t + r S_t Delta_t + frac{1}{2} Gamma S_t^2 sigma^2 - r V_t = 0 theta_t + r S_t Delta_t - r V_t = - frac{1}{2} Gamma S_t^2 sigma^2 end{align} $$$ tag{4}$ . Plugging (4) into (3) . $$ begin{align} d Pi_t &amp;= - frac{1}{2} Gamma S_t^2 sigma^2 dt - frac{1}{2} Gamma d[S_t,S_t] &amp;= frac{1}{2} Gamma S_t^2 left( frac{d[S_t,S_t]}{(S_t)^2 dt} - sigma^2 right) dt &amp;= frac{1}{2} Gamma S_t^2 left( sigma^{2*} - sigma^2 right) dt end{align}$$Where the last equality follows from the fact that the quadratic variation of geometric Brownian motion $ d[S_t,S_t] = sigma^{2*} S_t^2 dt$ . The total $PnL_t$ due to the wrong $ sigma^{*}$ is given by the integral . $$PnL_t = int_0^T e^{-r(T-t)} frac{1}{2} Gamma S_t^2 left( sigma^{2*} - sigma^2 right) dt$$ . Options pricing via Laplace Transform . Often the probability density function of the stock price are not known but characteristic functions are available instead. In these cases we can use Laplace transforms to price options. We consider a model with bond and stock of the form . $$ begin{align} B(t) = e^{rt} S(t) = e^{X(t)} end{align} $$ . If the payoff of the claim under consideration is of the form $f(X(T))$, the initial fair option price equals . $$ begin{align} V(t) = B(0) mathbb{E}^ mathbb{Q} big[[f(X(T))/B(T) big] = e^{-rT} mathbb{E}^ mathbb{Q} big[[f(X(T)) big] end{align} $$ . For very simple payoffs, the expectation can be calculated explicitly in many models, namely for $f(x) = e^{zx} $ with some constant $z$. For more complicated ones we consider payoffs of the form . $$ f(x)=e^{zx} =e^{ Re(z)x(cos( Im(z)x)+ i sin( Im(z)x))}$$ . where $z in mathbb{C}$ and $i = sqrt{-1} $. . The corresponding option price equals . $$V(0) = e^{-rT} mathbb{E}^ mathbb{Q}(e^{zX(T))}) = e^{-rT} chi(-i z)$$ . with $ chi(u) = mathbb{E}^ mathbb{Q}(e^{i u X(T)})$. . $ chi(u)$ is called (extended) characteristic function of $ X (T )$ in $u$ and it is known in closed form for many processes $X$. . The key idea is to write an arbitrary, more complicated payoff (e.g. a European call) in the form . $$ f(x) = int rho(z) e^{xz} dz$$ . with some function $ rho(z)$. Such a representation can be viewed as a generalized linear combination of “simple” payoffs $e^{zx}$. . import math from scipy.integrate import quad import numpy as np def bs_eu_call(S0, r, sigma, T, K, R): # Laplace transform of f(x) = (e^x - K)^+ def f_tilde(z): return K**(1-z) / (z*(z-1)) # Characteristic function of log(S(T)) in the Black-Scholes model def chi(u): return np.exp( 1j*u*(np.log(S0)+r*T) - (1j*u+u**2)*sigma**2/2*T ) # Integrand for the Laplace transform method def integrand(u): return np.exp(-r*T)/math.pi * ( f_tilde(R+1j*u)*chi(u-1j*R) ).real # option price V0 , err = quad(integrand, 0, 50) return V0 S0 = 100 r = 0.03 sigma = 0.3 T = 1 K = 100 R = 1.1 V0_BS = bs_eu_call(S0, r, sigma, T, K, R) print(f&quot;Option price via Laplace Transform in BS framework: {round(V0_BS,3)}&quot;) . Option price via Laplace Transform in BS framework: 13.283 . import math from scipy.integrate import quad import numpy as np def heston_eu_call_hedge_laplace(St, r, nut, kappa, Lambda, sigma_tilde, t, T, K, K_tilde, R, R_tilde): # Laplace transform of the function f(x) = (e^x - K)^+, def f_tilde_K(z): return K**(1-z) / (z*(z-1)) # Laplace transform of the function f(x) = (e^x - K_tilde)^+, def f_tilde_K_tilde(z): return K_tilde**(1-z) / (z*(z-1)) # Characteristic function of log(S(T)) at timepoint t in the Heston model, def chi (u): d = np.sqrt(Lambda**2+sigma_tilde**2*(1j*u+u**2)) n = math.cosh(d*(T-t)/2) + Lambda*math.sinh(d*(T-t)/2)/d z1 = np.exp(Lambda*(T-t)/2) z2 = (1j*u+u**2)*math.sinh(d*(T-t)/2)/d return np.exp( 1j*u*(np.log(St)+r*(T-t))) * (z1/n)**(2*kappa/sigma_tilde**2) * np.exp(-nut*z2/n) # derivative of Characteristic function of log(S(T)) w.r.t. S(t) def delx_chi(u): return 1j*u/St * chi(u) # derivative of Characteristic function of log(S(T)) w.r.t. gamma(t) def delnu_chi(u): d = np.sqrt(Lambda**2+sigma_tilde**2*(1j*u+u**2)) n = math.cosh(d*(T-t)/2) + Lambda*math.sinh(d*(T-t)/2)/d z2 = (1j*u+u**2)*math.sinh(d*(T-t)/2)/d return -z2/n * chi(u); # general Integrand for the Laplace transform method, arguments are (besides &quot;u&quot;) # the laplace transform we want to use (for different calls or puts) and characteristic funtion or derivative def laplace_integrand (u, f_tilde, del_func): return np.exp(-r*T)/math.pi * ( f_tilde(R+1j*u)*del_func(u-1j*R) ).real # price of the option and liquidly traded Call Vt, _ = quad(laplace_integrand, 0, 50 ,args=(f_tilde_K_tilde,chi)) Ct, _ = quad(laplace_integrand, 0, 50 , args=(f_tilde_K, chi)) # derivates of the option price and liquidly traded Call with respect to the variance process (gamma_t) del3v,_ = quad(laplace_integrand, 0, 50 ,args=(f_tilde_K_tilde, delnu_chi)) del3c,_ = quad(laplace_integrand, 0, 50 ,args=(f_tilde_K, delnu_chi)) # derivates of the option price and liquidly traded Call with respect to S_t del2v, _ = quad(laplace_integrand, 0, 50, args=(f_tilde_K_tilde, delx_chi)) del2c, _ = quad(laplace_integrand, 0, 50, args=(f_tilde_K, delx_chi)) # hedge phi2 = del3v/del3c phi1 = del2v - phi2*del2c phi0 = (Vt - phi1*St - phi2*Ct)*np.exp(-r*t) return Vt, phi0, phi1, phi2 St = 100 r = 0.03 nut = 0.3**2 kappa = 0.3**2 Lambda = 2.5 sigma_tilde = 0.2 T = 1 K = 95 K_tilde = 100 R = 1.1 R_tilde = 1.1 t=0 Vt, phi0, phi1, phi2 = heston_eu_call_hedge_laplace(St, r, nut, kappa, Lambda, sigma_tilde, t, T, K, K_tilde, R, R_tilde) print(f&quot; Option Price: {Vt} n Hedge Position Call: {phi2} n Hedge Position Stock: {phi1} n Hedge Position Bond: {phi0} n&quot;) . Option Price: 9.755867884013995 Hedge Position Call: 1.0634215306663264 Hedge Position Stock: -0.12638679472545133 Hedge Position Bond: 9.309745759697751 . /Users/jan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part /Users/jan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:23: ComplexWarning: Casting complex values to real discards the imaginary part /Users/jan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:35: ComplexWarning: Casting complex values to real discards the imaginary part /Users/jan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:36: ComplexWarning: Casting complex values to real discards the imaginary part . from IPython.display import display import numpy as np from matplotlib import pyplot as plt import pandas as pd import seaborn def BS_EuCall_FFT(S0, r, sigma, T, K, R, N, M): &#39;&#39;&#39; The fast Fourier transform method allows us to compute option prices for many strikes in an efficient manner. &#39;&#39;&#39; Delta=M/N kappa_1=np.log(S0)-(N/2)*2*np.pi/M kappa=kappa_1+np.array(range(N))*2*np.pi/M def g(u): # Laplace transform of the function f_0(x) = (e^x - e^0)^+, def f_tilde_0(z): return 1 / (z*(z-1)) # Characteristic function of log(S(T)) in the Black-Scholes model, def chi (u): return np.exp( 1j*u*(np.log(S0)+r*T) - (1j*u+u**2)*sigma**2/2*T ) y=np.ones(len(u),dtype=complex)# Important: You need to tell numpy the dtype=complex for i in range(len(u)): y[i]=f_tilde_0(R+1j*u[i])*chi(u[i]-1j*R) return y print(y) x=g((np.array(range(1,N+1))-0.5)*Delta)*Delta*np.exp(-1j*(np.array(range(N)))*Delta*kappa_1) #Computing the discrete fourier transform of x x_hat=np.fft.fft(x) #Computing the prices of options with values in kappa V_kappa=1/np.pi*np.exp(-r*T+(1-R)*kappa)*((x_hat*np.exp(-1j/2*Delta*kappa)).real) #Computing the prices of option with value in K using linear interpolation in log-strikes V0=np.interp(np.log(K),kappa, V_kappa ) return V0 S0 = 100 r = 0.05 sigma = 0.2 T = 1 K = np.array(range(80,131)) R = 1.1 N = 2**11 M = 50 #time the function %timeit V=BS_EuCall_FFT(S0, r, sigma, T, K, R, N, M) option_prices = pd.DataFrame() option_prices[&#39;Strike_Price&#39;] = K option_prices[&#39;Option_Price&#39;] = V display(option_prices) plt.plot(K,V, &#39;.&#39;) plt.title(&quot;Call vs. Strike Price -- fast Fourier transform&quot;) plt.xlabel(&quot;Strike Price&quot;) plt.ylabel(&quot;Call Value&quot;) plt.show() . 100 loops, best of 3: 17 ms per loop . Strike_Price Option_Price . 0 80 | 24.606408 | . 1 81 | 23.766824 | . 2 82 | 22.937541 | . 3 83 | 22.118311 | . 4 84 | 21.308892 | . 5 85 | 20.509052 | . 6 86 | 19.718568 | . 7 87 | 18.937222 | . 8 88 | 18.164805 | . 9 89 | 17.468357 | . 10 90 | 16.795490 | . 11 91 | 16.130059 | . 12 92 | 15.471901 | . 13 93 | 14.820857 | . 14 94 | 14.176777 | . 15 95 | 13.539513 | . 16 96 | 12.908922 | . 17 97 | 12.284865 | . 18 98 | 11.667209 | . 19 99 | 11.055824 | . 20 100 | 10.450584 | . 21 101 | 10.013594 | . 22 102 | 9.580910 | . 23 103 | 9.152447 | . 24 104 | 8.728124 | . 25 105 | 8.307862 | . 26 106 | 7.891583 | . 27 107 | 7.479213 | . 28 108 | 7.070679 | . 29 109 | 6.665910 | . 30 110 | 6.264838 | . 31 111 | 5.867396 | . 32 112 | 5.473518 | . 33 113 | 5.083141 | . 34 114 | 4.798694 | . 35 115 | 4.581994 | . 36 116 | 4.367170 | . 37 117 | 4.154190 | . 38 118 | 3.943022 | . 39 119 | 3.733637 | . 40 120 | 3.526004 | . 41 121 | 3.320094 | . 42 122 | 3.115878 | . 43 123 | 2.913330 | . 44 124 | 2.712422 | . 45 125 | 2.513128 | . 46 126 | 2.315421 | . 47 127 | 2.119278 | . 48 128 | 1.924673 | . 49 129 | 1.779225 | . 50 130 | 1.698612 | .",
            "url": "https://jpwoeltjen.github.io/researchBlog/derivatives/valuation/monte%20carlo/stochastic%20calculus/2019/01/03/ComputationalFinanceNotes.html",
            "relUrl": "/derivatives/valuation/monte%20carlo/stochastic%20calculus/2019/01/03/ComputationalFinanceNotes.html",
            "date": " • Jan 3, 2019"
        }
        
    
  
    
        ,"post13": {
            "title": "The Mandelbrot Set",
            "content": "import numpy as np import matplotlib.pyplot as plt %config InlineBackend.figure_format = &#39;retina&#39; import numba from numba import prange . @numba.njit def get_iter(c:complex, thresh:int =4, max_steps:int =25) -&gt; int: &quot;&quot;&quot; This is the main logic. Iterate Z_(0) = c Z_(n) = (Z_(n-1))^2 + c where c is a complex number and check if Z is bounded or diverges. &quot;&quot;&quot; z=c i=1 while i&lt;max_steps and (z*z.conjugate()).real&lt;thresh: z = z * z + c i += 1 return i @numba.njit def mapper(x, y, n): &quot;&quot;&quot;The following numbers are chosen such that the image is centered around the interesting parts.&quot;&quot;&quot; mx = 2.48 / (n-1) my = 2.26 / (n-1) return (mx*x - 2, my*y - 1.13) @numba.njit(parallel=True) def plotter(n, thresh, max_steps=25): img=np.full((n,n), 255) for x in prange(n): for y in range(n): it = get_iter(complex(*mapper(x, y, n)), thresh=thresh, max_steps=max_steps) img[y][x] = 255 - it return img . n = 20000 img = plotter(n, thresh=4, max_steps=200) . # plt.imshow(img, cmap=&quot;twilight_shifted&quot;) plt.imshow(img, cmap=&quot;gray&quot;) plt.axis(&quot;off&quot;) plt.tight_layout() plt.savefig(&#39;mandelbrot.png&#39;, dpi=3000, facecolor=&#39;w&#39;, edgecolor=&#39;w&#39;, orientation=&#39;portrait&#39;, papertype=None, format=None, transparent=False, bbox_inches=None, pad_inches=0., frameon=None, metadata=None) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: The frameon kwarg was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use facecolor instead. .",
            "url": "https://jpwoeltjen.github.io/researchBlog/2019/01/01/Mandelbrot.html",
            "relUrl": "/2019/01/01/Mandelbrot.html",
            "date": " • Jan 1, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This blog is written by Jan P. Woeltjen. .",
          "url": "https://jpwoeltjen.github.io/researchBlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jpwoeltjen.github.io/researchBlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}